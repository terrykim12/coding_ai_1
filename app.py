from contextlib import asynccontextmanager
from fastapi import FastAPI

@asynccontextmanager
async def lifespan(app: FastAPI):
    # TODO: add startup logic if needed
    yield
    # TODO: add shutdown logic if needed

#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Single entry that re-exports the FastAPI app from server.app
"""

from server.app import app  # noqa: F401
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Qwen3-8B Local Coding AI - FastAPI ì„œë²„ (CUDA ìµœì í™”)
"""

import os
import logging
import uuid
import json
from typing import List, Dict, Any, Optional
from fastapi import FastAPI, HTTPException, BackgroundTasks
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel, Field
from starlette.concurrency import run_in_threadpool
import anyio
import torch

import re, os
from pathlib import Path
from .model import Model
from .context import build_context
from .patch_apply import apply_patch_json
from .test_runner import run_pytest, get_test_summary
from .debug_runtime import DebugRuntime
from .path_resolver import PathResolver

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# FastAPI ì•± ìƒì„±
app = FastAPI(
    title="Qwen3-8B Local Coding AI",
    description="ë¡œì»¬ì—ì„œ ì‹¤í–‰ë˜ëŠ” AI ì½”ë”© ì–´ì‹œìŠ¤í„´íŠ¸",
    version="1.0.0"
)

# CORS ì„¤ì •
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ì „ì—­ ë³€ìˆ˜
WORKSPACE_ROOT = str(Path(__file__).resolve().parents[1])  # repo ë£¨íŠ¸ë¡œ ì¡°ì •
RESOLVER = PathResolver(WORKSPACE_ROOT)
model: Optional[Model] = None
debug_runtime: Optional[DebugRuntime] = None

def _pre_match(text: str, pat: str, use_regex: bool) -> bool:
    """ì•ˆì „í•œ ë§¤ì¹­: ê¸°ë³¸ì€ ë¦¬í„°ëŸ´. regex ìš”ì²­ ì‹œ ì»´íŒŒì¼ ì‹¤íŒ¨í•˜ë©´ ë¦¬í„°ëŸ´ fallback."""
    if not use_regex:
        return pat in text
    try:
        return re.search(pat, text, re.DOTALL | re.MULTILINE) is not None
    except re.error:
        return pat in text

def _tighten_loc_for_func(edit: dict, func_name: str):
    """í•¨ìˆ˜ ë¸”ë¡ ì •ê·œì‹ìœ¼ë¡œ loc ìë™ ë³´ì •"""
    pat = rf"^def {re.escape(func_name)}\([^\)]*\):[\s\S]*?(?=^\S)"
    loc = edit.get("loc", {})
    if loc.get("type") != "regex":
        edit["loc"] = {"type": "regex", "pattern": pat}
    else:
        edit["loc"]["pattern"] = pat

def _sanitize_preconditions(patch: dict) -> tuple[dict, list[dict]]:
    """
    ìƒˆ ì½”ë“œì—ë§Œ ìˆëŠ” íŒ¨í„´ì´ must_containì— ìˆìœ¼ë©´ must_not_containë¡œ ì´ë™.
    pre.regexê°€ ì—†ìœ¼ë©´ ê¸°ë³¸ ë¦¬í„°ëŸ´ ë§¤ì¹­.
    """
    changes = []
    for e in patch.get("edits", []):
        path = e.get("path"); code = e.get("code", "")
        pre  = e.get("pre", {}) or {}
        mc   = list(pre.get("must_contain", []) or [])
        mnc  = list(pre.get("must_not_contain", []) or [])
        use_regex = bool(pre.get("regex", False))

        abs_path = RESOLVER.resolve(path) if path else None
        try:
            src = open(abs_path, "r", encoding="utf-8").read() if abs_path else ""
        except Exception:
            src = ""

        moved = []
        for pat in list(mc):
            in_src = _pre_match(src, pat, use_regex)
            in_new = _pre_match(code, pat, use_regex)
            # íŒŒì¼ì—ëŠ” ì—†ê³  ìƒˆ ì½”ë“œì—ëŠ” ìˆìœ¼ë©´ â†’ must_not_containìœ¼ë¡œ ì´ë™
            if (not in_src) and in_new:
                mc.remove(pat)
                if pat not in mnc:
                    mnc.append(pat)
                moved.append(pat)

        if moved:
            pre["must_contain"] = mc
            pre["must_not_contain"] = mnc
            e["pre"] = pre
            changes.append({"path": path, "moved": moved})
            
            # í•¨ìˆ˜ ë¸”ë¡ ì•µì»¤ ìë™-ì¡°ì´ê¸°
            for pat in moved:
                m = re.search(r"def\s+([A-Za-z_][A-Za-z0-9_]*)\s*\(", pat)
                if m:
                    _tighten_loc_for_func(e, m.group(1))
                    
    return patch, changes

# Pydantic ëª¨ë¸ë“¤
class PlanRequest(BaseModel):
    intent: str = Field(..., description="ì‚¬ìš©ì ì˜ë„")
    paths: List[str] = Field(..., description="ë¶„ì„í•  íŒŒì¼ ê²½ë¡œë“¤")
    code_paste: str = Field("", description="ì½”ë“œ ìŠ¤ë‹ˆí«")

class PlanResponse(BaseModel):
    plan_id: str
    plan: Dict[str, Any]
    raw_response: str

class PatchRequest(BaseModel):
    plan: Dict[str, Any] = Field(..., description="ìˆ˜ì • ê³„íš")

class PatchResponse(BaseModel):
    patch_id: str
    patch: Dict[str, Any]
    raw_response: str

class ApplyRequest(BaseModel):
    patch: Dict[str, Any] = Field(..., description="ì ìš©í•  íŒ¨ì¹˜")
    allowed_paths: List[str] = Field(..., description="í—ˆìš©ëœ íŒŒì¼ ê²½ë¡œë“¤")
    dry_run: bool = Field(False, description="ì‹¤ì œ ì ìš© ì—¬ë¶€")

class ApplyResponse(BaseModel):
    applied: List[str]
    skipped: List[Dict[str, Any]] = Field(default_factory=list, description="ê±´ë„ˆë›´ íŒŒì¼ë“¤")
    failed: List[Dict[str, Any]] = Field(default_factory=list, description="ì‹¤íŒ¨í•œ íŒŒì¼ë“¤")
    dry_run: bool
    details: List[Dict[str, Any]] = Field(default_factory=list, description="ìƒì„¸ ì •ë³´")

class TestRequest(BaseModel):
    paths: Optional[List[str]] = Field(None, description="í…ŒìŠ¤íŠ¸í•  ê²½ë¡œë“¤")
    coverage: bool = Field(False, description="ì»¤ë²„ë¦¬ì§€ í¬í•¨ ì—¬ë¶€")

class TestResponse(BaseModel):
    summary: Dict[str, Any]
    output: str

class WorkflowRequest(BaseModel):
    intent: str = Field(..., description="ì‚¬ìš©ì ì˜ë„")
    paths: List[str] = Field(..., description="ë¶„ì„í•  íŒŒì¼ ê²½ë¡œë“¤")
    code_paste: str = Field("", description="ì½”ë“œ ìŠ¤ë‹ˆí«")
    auto_apply: bool = Field(False, description="ìë™ ì ìš© ì—¬ë¶€")
    auto_test: bool = Field(False, description="ìë™ í…ŒìŠ¤íŠ¸ ì—¬ë¶€")

class WorkflowResponse(BaseModel):
    workflow_id: str
    plan: Dict[str, Any]
    patch: Dict[str, Any]
    applied: bool
    test_results: Optional[Dict[str, Any]]

class DebugRequest(BaseModel):
    command: str = Field(..., description="ë””ë²„ê·¸ ëª…ë ¹ì–´")
    script_path: Optional[str] = Field(None, description="ìŠ¤í¬ë¦½íŠ¸ ê²½ë¡œ")
    port: Optional[int] = Field(None, description="ë””ë²„ê·¸ í¬íŠ¸")

class DebugResponse(BaseModel):
    success: bool
    message: str
    debug_port: Optional[int]
    processes: List[Dict[str, Any]]
async def startup_event():
    """ì„œë²„ ì‹œì‘ ì‹œ ëª¨ë¸ ë¡œë”©"""
    global model, debug_runtime
    
    try:
        logger.info("ğŸš€ Qwen3-8B Local Coding AI ì„œë²„ ì‹œì‘...")
        
        # CUDA í™˜ê²½ í™•ì¸
        if torch.cuda.is_available():
            logger.info(f"âœ… CUDA ì‚¬ìš© ê°€ëŠ¥: {torch.cuda.get_device_name()}")
            logger.info(f"   GPU ë©”ëª¨ë¦¬: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f}GB")
            logger.info(f"   CUDA ë²„ì „: {torch.version.cuda}")
        else:
            logger.warning("âš ï¸ CUDA ì‚¬ìš© ë¶ˆê°€, CPU ëª¨ë“œë¡œ ì‹¤í–‰")
        
        # ëª¨ë¸ ë¡œë”©
        model = Model()
        logger.info("âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ")
        
        # ë””ë²„ê·¸ ëŸ°íƒ€ì„ ì´ˆê¸°í™”
        debug_runtime = DebugRuntime()
        logger.info("âœ… ë””ë²„ê·¸ ëŸ°íƒ€ì„ ì´ˆê¸°í™” ì™„ë£Œ")
        
        logger.info("ğŸ‰ ì„œë²„ ì‹œì‘ ì™„ë£Œ!")
        
    except Exception as e:
        logger.error(f"âŒ ì„œë²„ ì‹œì‘ ì‹¤íŒ¨: {e}")
        raise
async def shutdown_event():
    """ì„œë²„ ì¢…ë£Œ ì‹œ ì •ë¦¬"""
    global model, debug_runtime
    
    try:
        if model:
            del model
        if debug_runtime:
            debug_runtime.cleanup_all()
        
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        
        logger.info("âœ… ì„œë²„ ì •ë¦¬ ì™„ë£Œ")
        
    except Exception as e:
        logger.error(f"âŒ ì„œë²„ ì •ë¦¬ ì‹¤íŒ¨: {e}")

@app.get("/health")
async def health_check():
    """ì„œë²„ ìƒíƒœ í™•ì¸ (CUDA ì •ë³´ í¬í•¨)"""
    if not model:
        raise HTTPException(status_code=503, detail="ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•ŠìŒ")
    
    # ê¸°ë³¸ ìƒíƒœ
    status = {
        "status": "healthy",
        "model_loaded": model.is_loaded(),
        "debug_processes": len(debug_runtime.list_processes()) if debug_runtime else 0
    }
    
    # CUDA ì •ë³´ ì¶”ê°€
    if torch.cuda.is_available():
        status.update({
            "cuda_available": True,
            "gpu_name": torch.cuda.get_device_name(),
            "gpu_memory_total": f"{torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f}GB",
            "gpu_memory_allocated": f"{torch.cuda.memory_allocated(0) / 1024**3:.1f}GB",
            "gpu_memory_cached": f"{torch.cuda.memory_reserved(0) / 1024**3:.1f}GB",
            "cuda_version": torch.version.cuda
        })
    else:
        status["cuda_available"] = False
    
    # ëª¨ë¸ ë””ë°”ì´ìŠ¤ ì •ë³´
    if model:
        device_info = model.get_device_info()
        status.update(device_info)
    
    return status

@app.post("/plan", response_model=PlanResponse)
async def create_plan(request: PlanRequest):
    """ì½”ë“œ ìˆ˜ì • ê³„íš ìƒì„±"""
    if not model:
        raise HTTPException(status_code=503, detail="ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•ŠìŒ")
    
    try:
        # ì»¨í…ìŠ¤íŠ¸ ë¹Œë“œë„ ë¬´ê±°ìš°ë©´ ìŠ¤ë ˆë“œë¡œ...
        context = await run_in_threadpool(build_context, request.paths)
        
        # ì¶”ë¡ ì€ ë°˜ë“œì‹œ ìŠ¤ë ˆë“œ + íƒ€ì„ì•„ì›ƒ
        with anyio.fail_after(90):   # 90ì´ˆ íƒ€ì„ì•„ì›ƒ (ëª¨ë¸ ì‹œê°„ì˜ˆì‚°ë³´ë‹¤ ì—¬ìœ ë¡­ê²Œ)
            plan_result = await run_in_threadpool(
                model.plan,
                context,
                request.intent,
                request.code_paste
            )
        
        # ì‘ë‹µ í˜•ì‹ ë§ì¶”ê¸°
        return PlanResponse(
            plan_id=f"plan_{uuid.uuid4().hex[:8]}",
            plan=plan_result,
            raw_response="Generated with prefix forcing"
        )
        
    except TimeoutError as te:
        logger.error(f"PLAN íƒ€ì„ì•„ì›ƒ: {te}")
        raise HTTPException(status_code=504, detail=str(te))
    except Exception as e:
        logger.error(f"PLAN ìƒì„± ì‹¤íŒ¨ ìƒì„¸: {e}")
        logger.error(f"ì…ë ¥ ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´: {len(context)}")
        logger.error(f"ì‚¬ìš©ì ì˜ë„: {request.intent}")
        logger.error(f"ì½”ë“œ ìŠ¤ë‹ˆí« ê¸¸ì´: {len(request.code_paste)}")
        raise HTTPException(status_code=500, detail=f"PLAN ìƒì„± ì‹¤íŒ¨: {str(e)}")

@app.post("/patch", response_model=PatchResponse)
async def create_patch(request: PatchRequest):
    """ì½”ë“œ íŒ¨ì¹˜ ìƒì„± (ê¸°ì¡´)"""
    if not model:
        raise HTTPException(status_code=503, detail="ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•ŠìŒ")
    
    try:
        # íŒ¨ì¹˜ ìƒì„±ë„ ìŠ¤ë ˆë“œ + íƒ€ì„ì•„ì›ƒ
        with anyio.move_on_after(90) as cs:   # 90ì´ˆ íƒ€ì„ì•„ì›ƒ (45ì´ˆ â†’ 90ì´ˆë¡œ ì¦ê°€)
            patch_result = await run_in_threadpool(model.patch, request.plan)
        
        if cs.cancel_called:
            raise TimeoutError("PATCH generation timeout")
        
        # ì‘ë‹µ í˜•ì‹ ë§ì¶”ê¸°
        return PatchResponse(
            patch_id=f"patch_{uuid.uuid4().hex[:8]}",
            patch=patch_result,
            raw_response="Generated with prefix forcing"
        )
        
    except TimeoutError as te:
        logger.error(f"PATCH íƒ€ì„ì•„ì›ƒ: {te}")
        raise HTTPException(status_code=504, detail=str(te))
    except Exception as e:
        logger.error(f"PATCH ìƒì„± ì‹¤íŒ¨ ìƒì„¸: {e}")
        logger.error(f"PLAN ë°ì´í„°: {json.dumps(request.plan, ensure_ascii=False)[:200]}...")
        raise HTTPException(status_code=500, detail=f"íŒ¨ì¹˜ ìƒì„± ì‹¤íŒ¨: {str(e)}")

@app.post("/patch_smart", response_model=PatchResponse)
async def create_patch_smart(request: PatchRequest):
    """ì½”ë“œ íŒ¨ì¹˜ ìƒì„± (ìì²´ ë³µêµ¬ ë£¨í”„ í¬í•¨)"""
    if not model:
        raise HTTPException(status_code=503, detail="ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•ŠìŒ")
    
    RETRY = 2
    plan = request.plan
    feedback = ""
    
    for attempt in range(1, RETRY + 2):
        try:
            logger.info(f"PATCH ìƒì„± ì‹œë„ {attempt}/{RETRY + 1}")
            
            with anyio.fail_after(70):
                patch = await run_in_threadpool(model.patch, plan, feedback)
                
            # --- after: patch = await run_in_threadpool(model.patch, plan, feedback)
            import json, re

            _FENCE_RE = re.compile(r"^```[a-zA-Z]*\s*|\s*```$", re.M)
            def _strip_fences(s: str) -> str:
                return _FENCE_RE.sub("", s).strip()

            def _balanced_obj(s: str) -> str | None:
                s = s.strip()
                i = s.find("{")
                if i < 0: return None
                depth = 0; j0 = None; in_str = False; esc = False
                for j, ch in enumerate(s[i:], start=i):
                    if in_str:
                        if esc: esc = False
                        elif ch == "\\": esc = True
                        elif ch == '"': in_str = False
                        continue
                    if ch == '"': in_str = True
                    elif ch == "{":
                        depth += 1
                        if j0 is None: j0 = j
                    elif ch == "}":
                        depth -= 1
                        if depth == 0 and j0 is not None:
                            return s[j0:j+1]
                return None

            def _coerce_edit_item(x):
                if isinstance(x, dict):
                    return x
                if isinstance(x, str):
                    t = _strip_fences(x).strip().strip(",")
                    obj = _balanced_obj(t)
                    if obj:
                        try:
                            return json.loads(obj)
                        except Exception:
                            pass
                return None

            # normalize edits
            if not isinstance(patch, dict) or not isinstance(patch.get("edits"), list):
                raise HTTPException(status_code=422, detail="patch missing edits[]")

            bad_idx = []
            norm = []
            for i, it in enumerate(patch["edits"]):
                ed = _coerce_edit_item(it)
                if ed is None:
                    bad_idx.append(i)
                else:
                    norm.append(ed)

            if bad_idx:
                if not norm:
                    raise HTTPException(status_code=422, detail=f"invalid edit type at indices {bad_idx[:10]}")
                # ë¶€ë¶„ë§Œ ìœ íš¨í•˜ë©´ ìœ íš¨í•œ ê²ƒë§Œ ì‚¬ìš© (ì„ íƒ)
                patch["edits"] = norm
                
        except TimeoutError as te:
            if attempt <= RETRY:
                feedback += f"\n[ERROR] generation-timeout: {te}"
                logger.warning(f"PATCH ìƒì„± íƒ€ì„ì•„ì›ƒ, ì¬ì‹œë„ {attempt}/{RETRY}")
                continue
            raise HTTPException(504, f"PATCH timeout: {te}")
        except ValueError as e:
            # ì˜ˆ: "PATCH: JSON parse failed" ë“±
            if attempt <= RETRY:
                feedback = f"[ERROR] {str(e)}"
                logger.warning(f"PATCH ValueError, ì¬ì‹œë„ {attempt}/{RETRY}: {e}")
                continue
            raise HTTPException(status_code=422, detail=str(e))
        except Exception as e:
            # ë¯¸ìƒ ì˜ˆì™¸ë„ ì ˆëŒ€ 500ìœ¼ë¡œ í„°ëœ¨ë¦¬ì§€ ë§ê³  ë©”ì‹œì§€ ì „ë‹¬
            if attempt <= RETRY:
                feedback = f"[ERROR] unexpected: {str(e)}"
                logger.warning(f"PATCH ì˜ˆì™¸, ì¬ì‹œë„ {attempt}/{RETRY}: {e}")
                continue
            raise HTTPException(status_code=422, detail=f"patch failed: {e}")
        
        # 1) ìŠ¤í‚¤ë§ˆ ì ê²€
        edits = patch.get("edits", [])
        if not edits or not isinstance(edits, list):
            if attempt <= RETRY:
                feedback = "[ERROR] empty-edits: produce at least one edit targeting the function in PLAN."
                logger.warning(f"PATCH empty edits, ì¬ì‹œë„ {attempt}/{RETRY}")
                continue
            raise HTTPException(422, "PATCH empty edits")
        
        # edits ë°°ì—´ ìœ íš¨ì„± ê²€ì‚¬
        for i, edit in enumerate(edits):
            if not isinstance(edit, dict):
                if attempt <= RETRY:
                    feedback = f"[ERROR] invalid-edit-{i}: edit must be a JSON object, got {type(edit).__name__}"
                    logger.warning(f"PATCH invalid edit type at index {i}: {type(edit).__name__}")
                    continue
                raise HTTPException(422, f"PATCH invalid edit at index {i}")
            
            if not edit.get("path"):
                if attempt <= RETRY:
                    feedback = f"[ERROR] missing-path-{i}: edit must have 'path' field"
                    logger.warning(f"PATCH missing path at index {i}")
                    continue
                raise HTTPException(422, f"PATCH missing path at index {i}")
        
        # 2) ê²½ë¡œ ìë™ ë³´ì •
        patch, remap, not_found = RESOLVER.fix_patch_paths(patch)
        
        if remap:
            logger.info(f"PATCH path remap: {remap}")  # /home/user/... -> examples/sample_py/app.py
        if not_found:
            # í›„ë³´ ìì²´ê°€ ì—†ìœ¼ë©´ ë°”ë¡œ 409ë¡œ ë°˜í™˜ (ëª¨ë¸ ì¬ì‹œë„ ì „ì— ì‚¬ìš©ì/í”Œëœ í™•ì¸)
            from fastapi.responses import JSONResponse
            return JSONResponse(
                status_code=409,
                content={"error":"file-not-found","paths":not_found}
            )
        
        # â˜… í”„ë¦¬ì»¨ë””ì…˜ ìë™ êµì •
        patch, moved = _sanitize_preconditions(patch)
        if moved:
            logger.info(f"PATCH preconditions sanitized: {moved}")
        
        # 3) dry-run ì ìš© ê²€ì‚¬
        try:
            dry = apply_patch_json(patch, dry_run=True)
            if dry["failed"]:
                if attempt <= RETRY:
                    # pre.must_contain miss ì˜¤ë¥˜ íŠ¹ë³„ ì²˜ë¦¬
                    fails = dry.get("failed", [])
                    misses = [f.get("error","") for f in fails if "pre.must_contain miss:" in f.get("error","")]
                    if misses:
                        feedback = (
                            "Use pre.must_contain ONLY for patterns that EXIST in the current file (BEFORE patch). "
                            "Move new patterns like 'def main(' to pre.must_not_contain. "
                            "Return the edits array again, minimal and anchored."
                        )
                    else:
                        feedback = "[ERROR] dry-run-failed: tighten loc with regex/ast; add pre.must_not_contain to avoid duplicates."
                    
                    logger.warning(f"PATCH dry-run ì‹¤íŒ¨, ì¬ì‹œë„ {attempt}/{RETRY}: {dry['failed']}")
                    continue
                raise HTTPException(409, f"apply failed: {dry['failed']}")
            
            # ì„±ê³µ ì‹œ ì‘ë‹µ ë°˜í™˜
            logger.info(f"PATCH ìƒì„± ì„±ê³µ (ì‹œë„ {attempt})")
            return PatchResponse(
                patch_id=f"patch_{uuid.uuid4().hex[:8]}",
                patch=patch,
                raw_response=f"Generated with autorepair (attempt {attempt})"
            )
            
        except Exception as e:
            if attempt <= RETRY:
                feedback += f"\n[ERROR] apply-check-failed: {str(e)}"
                logger.warning(f"PATCH ê²€ì¦ ì‹¤íŒ¨, ì¬ì‹œë„ {attempt}/{RETRY}: {e}")
                continue
            raise HTTPException(500, f"PATCH validation failed: {str(e)}")
    
    # ëª¨ë“  ì‹œë„ ì‹¤íŒ¨
    raise HTTPException(500, f"PATCH generation failed after {RETRY + 1} attempts. Final feedback: {feedback}")

@app.post("/apply", response_model=ApplyResponse)
async def apply_patch(request: ApplyRequest):
    """íŒ¨ì¹˜ ì ìš©"""
    try:
        # íŒ¨ì¹˜ ì ìš©
        result = apply_patch_json(
            patch=request.patch,
            allowed_paths=request.allowed_paths,
            dry_run=request.dry_run
        )
        
        return ApplyResponse(
            applied=result.get("applied", []),
            skipped=result.get("skipped", []),
            failed=result.get("failed", []),
            dry_run=request.dry_run,
            details=result.get("details", [])
        )
        
    except Exception as e:
        logger.error(f"íŒ¨ì¹˜ ì ìš© ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=f"íŒ¨ì¹˜ ì ìš© ì‹¤íŒ¨: {str(e)}")

@app.post("/test", response_model=TestResponse)
async def run_tests(request: TestRequest):
    """í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
    try:
        # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
        if request.coverage:
            test_path = (request.paths or ["."])[0] if request.paths else "."
            output = run_pytest(test_path, coverage=True)
        else:
            test_path = (request.paths or ["."])[0] if request.paths else "."
            output = run_pytest(test_path)
        
        # í…ŒìŠ¤íŠ¸ ìš”ì•½
        summary = get_test_summary(output)
        
        return TestResponse(summary=summary, output=output.output)
        
    except Exception as e:
        logger.error(f"í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=f"í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì‹¤íŒ¨: {str(e)}")

@app.post("/workflow", response_model=WorkflowResponse)
async def run_workflow(request: WorkflowRequest):
    """ì „ì²´ ì›Œí¬í”Œë¡œìš° ì‹¤í–‰"""
    if not model:
        raise HTTPException(status_code=503, detail="ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•ŠìŒ")
    
    try:
        workflow_id = str(uuid.uuid4())
        logger.info(f"ì›Œí¬í”Œë¡œìš° ì‹œì‘: {workflow_id}")
        
        # 1. ì»¨í…ìŠ¤íŠ¸ ë¹Œë“œ
        context = await run_in_threadpool(build_context, request.paths)
        
        # 2. ê³„íš ìƒì„± (íƒ€ì„ì•„ì›ƒ ì ìš©)
        with anyio.move_on_after(30) as cs:
            plan_result = await run_in_threadpool(
                model.plan,
                context,
                request.intent,
                request.code_paste
            )
        
        if cs.cancel_called:
            raise TimeoutError("PLAN generation timeout")
        
        # 3. íŒ¨ì¹˜ ìƒì„± (íƒ€ì„ì•„ì›ƒ ì ìš©)
        with anyio.move_on_after(90) as cs:   # 90ì´ˆë¡œ ì¦ê°€
            patch_result = await run_in_threadpool(model.patch, plan_result)
        
        if cs.cancel_called:
            raise TimeoutError("PATCH generation timeout")
        
        # 4. íŒ¨ì¹˜ ì ìš© (ì„ íƒì‚¬í•­)
        applied = False
        if request.auto_apply:
            apply_result = apply_patch_json(
                patch=patch_result,
                allowed_paths=request.paths,
                dry_run=False
            )
            applied = len(apply_result.get("applied", [])) > 0
        
        # 5. í…ŒìŠ¤íŠ¸ ì‹¤í–‰ (ì„ íƒì‚¬í•­)
        test_results = None
        if request.auto_test:
            try:
                test_path = request.paths[0] if request.paths else "."
                output = run_pytest(test_path)
                test_results = get_test_summary(output)
            except Exception as e:
                logger.warning(f"í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
        
        return WorkflowResponse(
            workflow_id=workflow_id,
            plan=plan_result,
            patch=patch_result,
            applied=applied,
            test_results=test_results
        )
        
    except TimeoutError as te:
        logger.error(f"ì›Œí¬í”Œë¡œìš° íƒ€ì„ì•„ì›ƒ: {te}")
        raise HTTPException(status_code=504, detail=str(te))
    except Exception as e:
        logger.error(f"ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=f"ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ ì‹¤íŒ¨: {str(e)}")

@app.post("/debug", response_model=DebugResponse)
async def debug_control(request: DebugRequest):
    """ë””ë²„ê·¸ ëŸ°íƒ€ì„ ì œì–´"""
    if not debug_runtime:
        raise HTTPException(status_code=503, detail="ë””ë²„ê·¸ ëŸ°íƒ€ì„ì´ ì´ˆê¸°í™”ë˜ì§€ ì•ŠìŒ")
    
    try:
        if request.command == "start":
            if not request.script_path:
                raise HTTPException(status_code=400, detail="ìŠ¤í¬ë¦½íŠ¸ ê²½ë¡œê°€ í•„ìš”í•©ë‹ˆë‹¤")
            
            debug_port = debug_runtime.run_with_debugpy(
                script_path=request.script_path,
                port=request.port
            )
            
            return DebugResponse(
                success=True,
                message=f"ë””ë²„ê·¸ í”„ë¡œì„¸ìŠ¤ ì‹œì‘ë¨ (í¬íŠ¸: {debug_port})",
                debug_port=debug_port,
                processes=debug_runtime.list_processes()
            )
            
        elif request.command == "stop":
            if not request.port:
                raise HTTPException(status_code=400, detail="í¬íŠ¸ ë²ˆí˜¸ê°€ í•„ìš”í•©ë‹ˆë‹¤")
            
            debug_runtime.stop_process(request.port)
            
            return DebugResponse(
                success=True,
                message=f"í¬íŠ¸ {request.port}ì˜ í”„ë¡œì„¸ìŠ¤ ì¤‘ì§€ë¨",
                debug_port=None,
                processes=debug_runtime.list_processes()
            )
            
        elif request.command == "list":
            return DebugResponse(
                success=True,
                message="ë””ë²„ê·¸ í”„ë¡œì„¸ìŠ¤ ëª©ë¡",
                debug_port=None,
                processes=debug_runtime.list_processes()
            )
            
        else:
            raise HTTPException(status_code=400, detail=f"ì•Œ ìˆ˜ ì—†ëŠ” ëª…ë ¹ì–´: {request.command}")
            
    except Exception as e:
        logger.error(f"ë””ë²„ê·¸ ì œì–´ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=f"ë””ë²„ê·¸ ì œì–´ ì‹¤íŒ¨: {str(e)}")

@app.get("/")
async def root():
    """ë£¨íŠ¸ ì—”ë“œí¬ì¸íŠ¸"""
    return {
        "message": "Qwen3-8B Local Coding AI ì„œë²„",
        "version": "1.0.0",
        "endpoints": [
            "/health - ì„œë²„ ìƒíƒœ í™•ì¸",
            "/plan - ì½”ë“œ ìˆ˜ì • ê³„íš ìƒì„±",
            "/patch - ì½”ë“œ íŒ¨ì¹˜ ìƒì„±",
            "/apply - íŒ¨ì¹˜ ì ìš©",
            "/test - í…ŒìŠ¤íŠ¸ ì‹¤í–‰",
            "/workflow - ì „ì²´ ì›Œí¬í”Œë¡œìš° ì‹¤í–‰",
            "/debug - ë””ë²„ê·¸ ëŸ°íƒ€ì„ ì œì–´"
        ]
    }

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="127.0.0.1", port=8765)


