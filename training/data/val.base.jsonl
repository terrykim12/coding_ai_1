{"instruction": "Propose a minimal JSON plan for code changes. Output PLAN JSON only.", "input": "[INTENT]\n학습 안정화(AMP+훈련 설정)\n\n[CONTEXT]\n<<<FILE examples/ai796/train_utils.py>>>\ndef get_dtype():\n    return torch.float16\n\n<<<END>>>\n<<<FILE examples/ai796/training_args.py>>>\nargs = TrainingArguments(output_dir='out', per_device_train_batch_size=1, num_train_epochs=1)\n\n<<<END>>>", "output": {"files": [{"path": "examples/ai796/train_utils.py", "reason": "dtype가 고정 float16", "strategy": "regex", "tests": ["bf16 또는 fp16 반환"]}, {"path": "examples/ai796/training_args.py", "reason": "로깅/평가/GA 설정 누락", "strategy": "regex", "tests": ["eval_strategy=='steps'"]}], "notes": "bf16 지원 시 우선"}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n학습 후 어댑터 저장 추가\n\n[CONTEXT]\n<<<FILE examples/ai336/save_after_train.py>>>\nfrom peft import PeftModel\n# ... 학습 후\n# BUG: 저장 누락\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai336/save_after_train.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["save_pretrained"]}, "code": "from peft import PeftModel\n# ... 학습 후\nmodel.save_pretrained(\"training/qlora-out/export\")\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n훈련 안정화: GA steps/로깅/평가/체크포인팅/스케줄러\n\n[CONTEXT]\n<<<FILE examples/ai203/training_args.py>>>\nfrom transformers import TrainingArguments\n\nargs = TrainingArguments(\n    output_dir=\"out\",\n    per_device_train_batch_size=1,\n    num_train_epochs=1,\n)\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai203/training_args.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["gradient_accumulation_steps"]}, "code": "from transformers import TrainingArguments\n\nargs = TrainingArguments(\n    output_dir=\"out\",\n    per_device_train_batch_size=1,\n    gradient_accumulation_steps=8,\n    num_train_epochs=1,\n    logging_steps=10,\n    eval_strategy=\"steps\",\n    eval_steps=50,\n    save_steps=50,\n    lr_scheduler_type=\"cosine\",\n    warmup_ratio=0.03,\n    fp16=False, bf16=True,\n    gradient_checkpointing=True,\n    max_grad_norm=1.0,\n)\n"}]}}
{"instruction": "Propose a minimal JSON plan for code changes. Output PLAN JSON only.", "input": "[INTENT]\n입출력/런타임 안전성 개선\n\n[CONTEXT]\n<<<FILE examples/ai759/dataloader_main.py>>>\nloader = DataLoader(...); for b in loader: pass\n\n<<<END>>>\n<<<FILE examples/ai759/ddp_main.py>>>\ntrainer()\n\n<<<END>>>", "output": {"files": [{"path": "examples/ai759/dataloader_main.py", "reason": "Windows freeze 가능", "strategy": "regex", "tests": ["__main__ 가드"]}, {"path": "examples/ai759/ddp_main.py", "reason": "DDP 가드 없음", "strategy": "regex", "tests": ["__main__"]}], "notes": "spawn 보호"}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nWindows에서 DataLoader 프리즈 방지 + pin_memory/persistent_workers\n\n[CONTEXT]\n<<<FILE examples/ai310/dataloader_main.py>>>\nimport torch, torch.utils.data as tud\n\nclass Dummy(torch.utils.data.Dataset):\n    def __len__(self): return 8\n    def __getitem__(self, i): return i\n\n# BUG: 모듈 임포트만으로 워커가 스폰되어 Windows에서 프리즈\nloader = tud.DataLoader(Dummy(), batch_size=2, num_workers=4)\nfor b in loader:\n    pass\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai310/dataloader_main.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["if __name__ == \"__main__\":"]}, "code": "import torch, torch.utils.data as tud\n\nclass Dummy(torch.utils.data.Dataset):\n    def __len__(self): return 8\n    def __getitem__(self, i): return i\n\n\ndef main():\n    loader = tud.DataLoader(Dummy(), batch_size=2, num_workers=4, pin_memory=True, persistent_workers=True)\n    for _ in loader:\n        pass\n\n\nif __name__ == \"__main__\":\n    main()\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n필수 import 추가\n\n[CONTEXT]\n<<<FILE examples/ai545/imports.py>>>\n# BUG: numpy 미사용이지만 추후 사용 예정\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai545/imports.py", "loc": {"type": "anchor", "before": "\\A"}, "action": "insert_after", "once": true, "pre": {"must_not_contain": ["import numpy as np"]}, "code": "import numpy as np\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n필수 import 추가\n\n[CONTEXT]\n<<<FILE examples/ai494/imports.py>>>\n# BUG: numpy 미사용이지만 추후 사용 예정\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai494/imports.py", "loc": {"type": "anchor", "before": "\\A"}, "action": "insert_after", "once": true, "pre": {"must_not_contain": ["import numpy as np"]}, "code": "import numpy as np\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nGPU bf16 지원 시 dtype 선택 개선\n\n[CONTEXT]\n<<<FILE examples/ai370/train_utils.py>>>\nimport torch\n\n# BUG: dtype가 고정되어 혼합정밀에 불리\ndef get_dtype():\n    return torch.float16\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai370/train_utils.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["torch.bfloat16"]}, "code": "import torch\n\n\ndef get_dtype():\n    \"\"\"GPU가 bf16 지원 시 bfloat16, 아니면 float16 반환\"\"\"\n    if torch.cuda.is_available() and torch.cuda.is_bf16_supported():\n        return torch.bfloat16\n    return torch.float16\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nAMP 적용 + bf16 우선\n\n[CONTEXT]\n<<<FILE examples/ai428/train_step.py>>>\nimport torch\n\n# BUG: autocast 미사용\ndef train_step(model, batch):\n    x, y = batch\n    out = model(x)\n    loss = out.loss if hasattr(out, 'loss') else out.mean()\n    loss.backward()\n    return loss.item()\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai428/train_step.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["autocast"]}, "code": "import torch\n\n@torch.no_grad()\ndef _bf16_supported():\n    return torch.cuda.is_available() and torch.cuda.is_bf16_supported()\n\n\ndef train_step(model, batch):\n    x, y = batch\n    amp_dtype = torch.bfloat16 if _bf16_supported() else torch.float16\n    scaler = torch.cuda.amp.GradScaler(enabled=True)\n    with torch.cuda.amp.autocast(enabled=True, dtype=amp_dtype):\n        out = model(x)\n        loss = out.loss if hasattr(out, 'loss') else out.mean()\n    scaler.scale(loss).backward()\n    scaler.step(torch.optim.Adam(model.parameters(), lr=1e-4))\n    scaler.update()\n    model.zero_grad(set_to_none=True)\n    return float(loss.detach().item())\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nAMP 적용 + bf16 우선\n\n[CONTEXT]\n<<<FILE examples/ai204/train_step.py>>>\nimport torch\n\n# BUG: autocast 미사용\ndef train_step(model, batch):\n    x, y = batch\n    out = model(x)\n    loss = out.loss if hasattr(out, 'loss') else out.mean()\n    loss.backward()\n    return loss.item()\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai204/train_step.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["autocast"]}, "code": "import torch\n\n@torch.no_grad()\ndef _bf16_supported():\n    return torch.cuda.is_available() and torch.cuda.is_bf16_supported()\n\n\ndef train_step(model, batch):\n    x, y = batch\n    amp_dtype = torch.bfloat16 if _bf16_supported() else torch.float16\n    scaler = torch.cuda.amp.GradScaler(enabled=True)\n    with torch.cuda.amp.autocast(enabled=True, dtype=amp_dtype):\n        out = model(x)\n        loss = out.loss if hasattr(out, 'loss') else out.mean()\n    scaler.scale(loss).backward()\n    scaler.step(torch.optim.Adam(model.parameters(), lr=1e-4))\n    scaler.update()\n    model.zero_grad(set_to_none=True)\n    return float(loss.detach().item())\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nLoRA target_modules 확장\n\n[CONTEXT]\n<<<FILE examples/ai388/lora_cfg.py>>>\nfrom peft import LoraConfig\n\nLC = LoraConfig(r=8, target_modules=[\n    \"q_proj\", \"v_proj\"  # BUG: 부족\n])\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai388/lora_cfg.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["k_proj", "o_proj"]}, "code": "from peft import LoraConfig\n\nLC = LoraConfig(r=8, target_modules=[\n    \"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"\n])\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nflash 미지원 시 sdpa 안전 경로\n\n[CONTEXT]\n<<<FILE examples/ai130/model_load.py>>>\nfrom transformers import AutoModelForCausalLM\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"Qwen/Qwen3-8B\",\n)\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai130/model_load.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["attn_implementation"]}, "code": "from transformers import AutoModelForCausalLM\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"Qwen/Qwen3-8B\",\n    attn_implementation=\"sdpa\",\n)\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n재현성: 시드 함수 추가\n\n[CONTEXT]\n<<<FILE examples/ai622/seed_utils.py>>>\n# BUG: 시드 미설정\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai622/seed_utils.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["set_seed("]}, "code": "import os, random, numpy as np, torch\n\ndef set_seed(seed: int = 42):\n    random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n훈련 안정화: GA steps/로깅/평가/체크포인팅/스케줄러\n\n[CONTEXT]\n<<<FILE examples/ai167/training_args.py>>>\nfrom transformers import TrainingArguments\n\nargs = TrainingArguments(\n    output_dir=\"out\",\n    per_device_train_batch_size=1,\n    num_train_epochs=1,\n)\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai167/training_args.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["gradient_accumulation_steps"]}, "code": "from transformers import TrainingArguments\n\nargs = TrainingArguments(\n    output_dir=\"out\",\n    per_device_train_batch_size=1,\n    gradient_accumulation_steps=8,\n    num_train_epochs=1,\n    logging_steps=10,\n    eval_strategy=\"steps\",\n    eval_steps=50,\n    save_steps=50,\n    lr_scheduler_type=\"cosine\",\n    warmup_ratio=0.03,\n    fp16=False, bf16=True,\n    gradient_checkpointing=True,\n    max_grad_norm=1.0,\n)\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n조기 종료 콜백 추가\n\n[CONTEXT]\n<<<FILE examples/ai483/callbacks.py>>>\nfrom transformers import TrainerCallback\n\nclass MyCB(TrainerCallback):\n    pass\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai483/callbacks.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["EarlyStoppingCallback"]}, "code": "from transformers import TrainerCallback, EarlyStoppingCallback\n\nclass MyCB(TrainerCallback):\n    pass\n\nCALLBACKS = [EarlyStoppingCallback(early_stopping_patience=2)]\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n조기 종료 콜백 추가\n\n[CONTEXT]\n<<<FILE examples/ai378/callbacks.py>>>\nfrom transformers import TrainerCallback\n\nclass MyCB(TrainerCallback):\n    pass\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai378/callbacks.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["EarlyStoppingCallback"]}, "code": "from transformers import TrainerCallback, EarlyStoppingCallback\n\nclass MyCB(TrainerCallback):\n    pass\n\nCALLBACKS = [EarlyStoppingCallback(early_stopping_patience=2)]\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nDDP/torchrun 안전 가드\n\n[CONTEXT]\n<<<FILE examples/ai40/ddp_main.py>>>\nimport torch\n\n# BUG: torchrun에서 main 가드 없음\ntrainer()\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai40/ddp_main.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["__main__"]}, "code": "import torch\n\n\ndef main():\n    trainer()\n\n\nif __name__ == \"__main__\":\n    main()\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n필수 import 추가\n\n[CONTEXT]\n<<<FILE examples/ai470/imports.py>>>\n# BUG: numpy 미사용이지만 추후 사용 예정\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai470/imports.py", "loc": {"type": "anchor", "before": "\\A"}, "action": "insert_after", "once": true, "pre": {"must_not_contain": ["import numpy as np"]}, "code": "import numpy as np\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n재현성: 시드 함수 추가\n\n[CONTEXT]\n<<<FILE examples/ai154/seed_utils.py>>>\n# BUG: 시드 미설정\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai154/seed_utils.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["set_seed("]}, "code": "import os, random, numpy as np, torch\n\ndef set_seed(seed: int = 42):\n    random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nWindows에서 DataLoader 프리즈 방지 + pin_memory/persistent_workers\n\n[CONTEXT]\n<<<FILE examples/ai66/dataloader_main.py>>>\nimport torch, torch.utils.data as tud\n\nclass Dummy(torch.utils.data.Dataset):\n    def __len__(self): return 8\n    def __getitem__(self, i): return i\n\n# BUG: 모듈 임포트만으로 워커가 스폰되어 Windows에서 프리즈\nloader = tud.DataLoader(Dummy(), batch_size=2, num_workers=4)\nfor b in loader:\n    pass\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai66/dataloader_main.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["if __name__ == \"__main__\":"]}, "code": "import torch, torch.utils.data as tud\n\nclass Dummy(torch.utils.data.Dataset):\n    def __len__(self): return 8\n    def __getitem__(self, i): return i\n\n\ndef main():\n    loader = tud.DataLoader(Dummy(), batch_size=2, num_workers=4, pin_memory=True, persistent_workers=True)\n    for _ in loader:\n        pass\n\n\nif __name__ == \"__main__\":\n    main()\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n8bit 옵티마이저 + 코사인 스케줄러\n\n[CONTEXT]\n<<<FILE examples/ai163/optim.py>>>\nimport torch\nopt = torch.optim.Adam([], lr=3e-4)\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai163/optim.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["PagedAdamW8bit"]}, "code": "from transformers import get_scheduler\nfrom bitsandbytes.optim import PagedAdamW8bit\nopt = PagedAdamW8bit([], lr=2e-4)\nsched = get_scheduler(\"cosine\", optimizer=opt, num_warmup_steps=100, num_training_steps=1000)\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nWindows에서 DataLoader 프리즈 방지 + pin_memory/persistent_workers\n\n[CONTEXT]\n<<<FILE examples/ai344/dataloader_main.py>>>\nimport torch, torch.utils.data as tud\n\nclass Dummy(torch.utils.data.Dataset):\n    def __len__(self): return 8\n    def __getitem__(self, i): return i\n\n# BUG: 모듈 임포트만으로 워커가 스폰되어 Windows에서 프리즈\nloader = tud.DataLoader(Dummy(), batch_size=2, num_workers=4)\nfor b in loader:\n    pass\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai344/dataloader_main.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["if __name__ == \"__main__\":"]}, "code": "import torch, torch.utils.data as tud\n\nclass Dummy(torch.utils.data.Dataset):\n    def __len__(self): return 8\n    def __getitem__(self, i): return i\n\n\ndef main():\n    loader = tud.DataLoader(Dummy(), batch_size=2, num_workers=4, pin_memory=True, persistent_workers=True)\n    for _ in loader:\n        pass\n\n\nif __name__ == \"__main__\":\n    main()\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n조기 종료 콜백 추가\n\n[CONTEXT]\n<<<FILE examples/ai577/callbacks.py>>>\nfrom transformers import TrainerCallback\n\nclass MyCB(TrainerCallback):\n    pass\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai577/callbacks.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["EarlyStoppingCallback"]}, "code": "from transformers import TrainerCallback, EarlyStoppingCallback\n\nclass MyCB(TrainerCallback):\n    pass\n\nCALLBACKS = [EarlyStoppingCallback(early_stopping_patience=2)]\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n조기 종료 콜백 추가\n\n[CONTEXT]\n<<<FILE examples/ai524/callbacks.py>>>\nfrom transformers import TrainerCallback\n\nclass MyCB(TrainerCallback):\n    pass\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai524/callbacks.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["EarlyStoppingCallback"]}, "code": "from transformers import TrainerCallback, EarlyStoppingCallback\n\nclass MyCB(TrainerCallback):\n    pass\n\nCALLBACKS = [EarlyStoppingCallback(early_stopping_patience=2)]\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nGPU bf16 지원 시 dtype 선택 개선\n\n[CONTEXT]\n<<<FILE examples/ai540/train_utils.py>>>\nimport torch\n\n# BUG: dtype가 고정되어 혼합정밀에 불리\ndef get_dtype():\n    return torch.float16\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai540/train_utils.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["torch.bfloat16"]}, "code": "import torch\n\n\ndef get_dtype():\n    \"\"\"GPU가 bf16 지원 시 bfloat16, 아니면 float16 반환\"\"\"\n    if torch.cuda.is_available() and torch.cuda.is_bf16_supported():\n        return torch.bfloat16\n    return torch.float16\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n재현성: 시드 함수 추가\n\n[CONTEXT]\n<<<FILE examples/ai316/seed_utils.py>>>\n# BUG: 시드 미설정\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai316/seed_utils.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["set_seed("]}, "code": "import os, random, numpy as np, torch\n\ndef set_seed(seed: int = 42):\n    random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n"}]}}
{"instruction": "Propose a minimal JSON plan for code changes. Output PLAN JSON only.", "input": "[INTENT]\n학습 안정화(AMP+훈련 설정)\n\n[CONTEXT]\n<<<FILE examples/ai768/train_utils.py>>>\ndef get_dtype():\n    return torch.float16\n\n<<<END>>>\n<<<FILE examples/ai768/training_args.py>>>\nargs = TrainingArguments(output_dir='out', per_device_train_batch_size=1, num_train_epochs=1)\n\n<<<END>>>", "output": {"files": [{"path": "examples/ai768/train_utils.py", "reason": "dtype가 고정 float16", "strategy": "regex", "tests": ["bf16 또는 fp16 반환"]}, {"path": "examples/ai768/training_args.py", "reason": "로깅/평가/GA 설정 누락", "strategy": "regex", "tests": ["eval_strategy=='steps'"]}], "notes": "bf16 지원 시 우선"}}
{"instruction": "Propose a minimal JSON plan for code changes. Output PLAN JSON only.", "input": "[INTENT]\n입출력/런타임 안전성 개선\n\n[CONTEXT]\n<<<FILE examples/ai771/dataloader_main.py>>>\nloader = DataLoader(...); for b in loader: pass\n\n<<<END>>>\n<<<FILE examples/ai771/ddp_main.py>>>\ntrainer()\n\n<<<END>>>", "output": {"files": [{"path": "examples/ai771/dataloader_main.py", "reason": "Windows freeze 가능", "strategy": "regex", "tests": ["__main__ 가드"]}, {"path": "examples/ai771/ddp_main.py", "reason": "DDP 가드 없음", "strategy": "regex", "tests": ["__main__"]}], "notes": "spawn 보호"}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n학습 후 어댑터 저장 추가\n\n[CONTEXT]\n<<<FILE examples/ai498/save_after_train.py>>>\nfrom peft import PeftModel\n# ... 학습 후\n# BUG: 저장 누락\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai498/save_after_train.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["save_pretrained"]}, "code": "from peft import PeftModel\n# ... 학습 후\nmodel.save_pretrained(\"training/qlora-out/export\")\n"}]}}
{"instruction": "Propose a minimal JSON plan for code changes. Output PLAN JSON only.", "input": "[INTENT]\n입출력/런타임 안전성 개선\n\n[CONTEXT]\n<<<FILE examples/ai779/dataloader_main.py>>>\nloader = DataLoader(...); for b in loader: pass\n\n<<<END>>>\n<<<FILE examples/ai779/ddp_main.py>>>\ntrainer()\n\n<<<END>>>", "output": {"files": [{"path": "examples/ai779/dataloader_main.py", "reason": "Windows freeze 가능", "strategy": "regex", "tests": ["__main__ 가드"]}, {"path": "examples/ai779/ddp_main.py", "reason": "DDP 가드 없음", "strategy": "regex", "tests": ["__main__"]}], "notes": "spawn 보호"}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\npad 토큰 ID 정확히 설정\n\n[CONTEXT]\n<<<FILE examples/ai215/label_cfg.py>>>\nfrom transformers import AutoTokenizer\nfrom dataclasses import dataclass\n\n@dataclass\nclass Cfg:\n    pad_id: int = -100  # BUG\n\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai215/label_cfg.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["CFG = Cfg"]}, "code": "from transformers import AutoTokenizer\nfrom dataclasses import dataclass\n\n@dataclass\nclass Cfg:\n    pad_id: int | None = None\n\n# tokenizer 로드 후 설정\ntok = AutoTokenizer.from_pretrained(\"Qwen/Qwen3-8B\", use_fast=True)\nCFG = Cfg(pad_id=tok.pad_token_id)\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n체크포인트 재개 지원\n\n[CONTEXT]\n<<<FILE examples/ai552/trainer_resume.py>>>\nfrom transformers import Trainer\ntrainer = Trainer(... )\n# BUG: 재시작 미지원\ntrainer.train()\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai552/trainer_resume.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["resume_from_checkpoint"]}, "code": "from transformers import Trainer\ntrainer = Trainer(... )\ntrainer.train(resume_from_checkpoint=True)\n"}]}}
{"instruction": "Propose a minimal JSON plan for code changes. Output PLAN JSON only.", "input": "[INTENT]\n학습 안정화(AMP+훈련 설정)\n\n[CONTEXT]\n<<<FILE examples/ai772/train_utils.py>>>\ndef get_dtype():\n    return torch.float16\n\n<<<END>>>\n<<<FILE examples/ai772/training_args.py>>>\nargs = TrainingArguments(output_dir='out', per_device_train_batch_size=1, num_train_epochs=1)\n\n<<<END>>>", "output": {"files": [{"path": "examples/ai772/train_utils.py", "reason": "dtype가 고정 float16", "strategy": "regex", "tests": ["bf16 또는 fp16 반환"]}, {"path": "examples/ai772/training_args.py", "reason": "로깅/평가/GA 설정 누락", "strategy": "regex", "tests": ["eval_strategy=='steps'"]}], "notes": "bf16 지원 시 우선"}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n조기 종료 콜백 추가\n\n[CONTEXT]\n<<<FILE examples/ai4/callbacks.py>>>\nfrom transformers import TrainerCallback\n\nclass MyCB(TrainerCallback):\n    pass\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai4/callbacks.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["EarlyStoppingCallback"]}, "code": "from transformers import TrainerCallback, EarlyStoppingCallback\n\nclass MyCB(TrainerCallback):\n    pass\n\nCALLBACKS = [EarlyStoppingCallback(early_stopping_patience=2)]\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n훈련 안정화: GA steps/로깅/평가/체크포인팅/스케줄러\n\n[CONTEXT]\n<<<FILE examples/ai94/training_args.py>>>\nfrom transformers import TrainingArguments\n\nargs = TrainingArguments(\n    output_dir=\"out\",\n    per_device_train_batch_size=1,\n    num_train_epochs=1,\n)\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai94/training_args.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["gradient_accumulation_steps"]}, "code": "from transformers import TrainingArguments\n\nargs = TrainingArguments(\n    output_dir=\"out\",\n    per_device_train_batch_size=1,\n    gradient_accumulation_steps=8,\n    num_train_epochs=1,\n    logging_steps=10,\n    eval_strategy=\"steps\",\n    eval_steps=50,\n    save_steps=50,\n    lr_scheduler_type=\"cosine\",\n    warmup_ratio=0.03,\n    fp16=False, bf16=True,\n    gradient_checkpointing=True,\n    max_grad_norm=1.0,\n)\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nLoRA target_modules 확장\n\n[CONTEXT]\n<<<FILE examples/ai255/lora_cfg.py>>>\nfrom peft import LoraConfig\n\nLC = LoraConfig(r=8, target_modules=[\n    \"q_proj\", \"v_proj\"  # BUG: 부족\n])\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai255/lora_cfg.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["k_proj", "o_proj"]}, "code": "from peft import LoraConfig\n\nLC = LoraConfig(r=8, target_modules=[\n    \"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"\n])\n"}]}}
{"instruction": "Propose a minimal JSON plan for code changes. Output PLAN JSON only.", "input": "[INTENT]\n입출력/런타임 안전성 개선\n\n[CONTEXT]\n<<<FILE examples/ai753/dataloader_main.py>>>\nloader = DataLoader(...); for b in loader: pass\n\n<<<END>>>\n<<<FILE examples/ai753/ddp_main.py>>>\ntrainer()\n\n<<<END>>>", "output": {"files": [{"path": "examples/ai753/dataloader_main.py", "reason": "Windows freeze 가능", "strategy": "regex", "tests": ["__main__ 가드"]}, {"path": "examples/ai753/ddp_main.py", "reason": "DDP 가드 없음", "strategy": "regex", "tests": ["__main__"]}], "notes": "spawn 보호"}}
{"instruction": "Propose a minimal JSON plan for code changes. Output PLAN JSON only.", "input": "[INTENT]\n학습 안정화(AMP+훈련 설정)\n\n[CONTEXT]\n<<<FILE examples/ai683/train_utils.py>>>\ndef get_dtype():\n    return torch.float16\n\n<<<END>>>\n<<<FILE examples/ai683/training_args.py>>>\nargs = TrainingArguments(output_dir='out', per_device_train_batch_size=1, num_train_epochs=1)\n\n<<<END>>>", "output": {"files": [{"path": "examples/ai683/train_utils.py", "reason": "dtype가 고정 float16", "strategy": "regex", "tests": ["bf16 또는 fp16 반환"]}, {"path": "examples/ai683/training_args.py", "reason": "로깅/평가/GA 설정 누락", "strategy": "regex", "tests": ["eval_strategy=='steps'"]}], "notes": "bf16 지원 시 우선"}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\npad 토큰 ID 정확히 설정\n\n[CONTEXT]\n<<<FILE examples/ai491/label_cfg.py>>>\nfrom transformers import AutoTokenizer\nfrom dataclasses import dataclass\n\n@dataclass\nclass Cfg:\n    pad_id: int = -100  # BUG\n\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai491/label_cfg.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["CFG = Cfg"]}, "code": "from transformers import AutoTokenizer\nfrom dataclasses import dataclass\n\n@dataclass\nclass Cfg:\n    pad_id: int | None = None\n\n# tokenizer 로드 후 설정\ntok = AutoTokenizer.from_pretrained(\"Qwen/Qwen3-8B\", use_fast=True)\nCFG = Cfg(pad_id=tok.pad_token_id)\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n재현성: 시드 함수 추가\n\n[CONTEXT]\n<<<FILE examples/ai567/seed_utils.py>>>\n# BUG: 시드 미설정\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai567/seed_utils.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["set_seed("]}, "code": "import os, random, numpy as np, torch\n\ndef set_seed(seed: int = 42):\n    random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n"}]}}
{"instruction": "Propose a minimal JSON plan for code changes. Output PLAN JSON only.", "input": "[INTENT]\n학습 안정화(AMP+훈련 설정)\n\n[CONTEXT]\n<<<FILE examples/ai797/train_utils.py>>>\ndef get_dtype():\n    return torch.float16\n\n<<<END>>>\n<<<FILE examples/ai797/training_args.py>>>\nargs = TrainingArguments(output_dir='out', per_device_train_batch_size=1, num_train_epochs=1)\n\n<<<END>>>", "output": {"files": [{"path": "examples/ai797/train_utils.py", "reason": "dtype가 고정 float16", "strategy": "regex", "tests": ["bf16 또는 fp16 반환"]}, {"path": "examples/ai797/training_args.py", "reason": "로깅/평가/GA 설정 누락", "strategy": "regex", "tests": ["eval_strategy=='steps'"]}], "notes": "bf16 지원 시 우선"}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nLoRA target_modules 확장\n\n[CONTEXT]\n<<<FILE examples/ai75/lora_cfg.py>>>\nfrom peft import LoraConfig\n\nLC = LoraConfig(r=8, target_modules=[\n    \"q_proj\", \"v_proj\"  # BUG: 부족\n])\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai75/lora_cfg.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["k_proj", "o_proj"]}, "code": "from peft import LoraConfig\n\nLC = LoraConfig(r=8, target_modules=[\n    \"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"\n])\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nLoRA target_modules 확장\n\n[CONTEXT]\n<<<FILE examples/ai97/lora_cfg.py>>>\nfrom peft import LoraConfig\n\nLC = LoraConfig(r=8, target_modules=[\n    \"q_proj\", \"v_proj\"  # BUG: 부족\n])\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai97/lora_cfg.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["k_proj", "o_proj"]}, "code": "from peft import LoraConfig\n\nLC = LoraConfig(r=8, target_modules=[\n    \"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"\n])\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n학습 후 어댑터 저장 추가\n\n[CONTEXT]\n<<<FILE examples/ai447/save_after_train.py>>>\nfrom peft import PeftModel\n# ... 학습 후\n# BUG: 저장 누락\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai447/save_after_train.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["save_pretrained"]}, "code": "from peft import PeftModel\n# ... 학습 후\nmodel.save_pretrained(\"training/qlora-out/export\")\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nLoRA target_modules 확장\n\n[CONTEXT]\n<<<FILE examples/ai272/lora_cfg.py>>>\nfrom peft import LoraConfig\n\nLC = LoraConfig(r=8, target_modules=[\n    \"q_proj\", \"v_proj\"  # BUG: 부족\n])\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai272/lora_cfg.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["k_proj", "o_proj"]}, "code": "from peft import LoraConfig\n\nLC = LoraConfig(r=8, target_modules=[\n    \"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"\n])\n"}]}}
{"instruction": "Propose a minimal JSON plan for code changes. Output PLAN JSON only.", "input": "[INTENT]\n입출력/런타임 안전성 개선\n\n[CONTEXT]\n<<<FILE examples/ai707/dataloader_main.py>>>\nloader = DataLoader(...); for b in loader: pass\n\n<<<END>>>\n<<<FILE examples/ai707/ddp_main.py>>>\ntrainer()\n\n<<<END>>>", "output": {"files": [{"path": "examples/ai707/dataloader_main.py", "reason": "Windows freeze 가능", "strategy": "regex", "tests": ["__main__ 가드"]}, {"path": "examples/ai707/ddp_main.py", "reason": "DDP 가드 없음", "strategy": "regex", "tests": ["__main__"]}], "notes": "spawn 보호"}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n재현성: 시드 함수 추가\n\n[CONTEXT]\n<<<FILE examples/ai424/seed_utils.py>>>\n# BUG: 시드 미설정\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai424/seed_utils.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["set_seed("]}, "code": "import os, random, numpy as np, torch\n\ndef set_seed(seed: int = 42):\n    random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nDDP/torchrun 안전 가드\n\n[CONTEXT]\n<<<FILE examples/ai127/ddp_main.py>>>\nimport torch\n\n# BUG: torchrun에서 main 가드 없음\ntrainer()\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai127/ddp_main.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["__main__"]}, "code": "import torch\n\n\ndef main():\n    trainer()\n\n\nif __name__ == \"__main__\":\n    main()\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n체크포인트 재개 지원\n\n[CONTEXT]\n<<<FILE examples/ai398/trainer_resume.py>>>\nfrom transformers import Trainer\ntrainer = Trainer(... )\n# BUG: 재시작 미지원\ntrainer.train()\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai398/trainer_resume.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["resume_from_checkpoint"]}, "code": "from transformers import Trainer\ntrainer = Trainer(... )\ntrainer.train(resume_from_checkpoint=True)\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n재현성: 시드 함수 추가\n\n[CONTEXT]\n<<<FILE examples/ai377/seed_utils.py>>>\n# BUG: 시드 미설정\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai377/seed_utils.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["set_seed("]}, "code": "import os, random, numpy as np, torch\n\ndef set_seed(seed: int = 42):\n    random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n"}]}}
{"instruction": "Propose a minimal JSON plan for code changes. Output PLAN JSON only.", "input": "[INTENT]\n입출력/런타임 안전성 개선\n\n[CONTEXT]\n<<<FILE examples/ai783/dataloader_main.py>>>\nloader = DataLoader(...); for b in loader: pass\n\n<<<END>>>\n<<<FILE examples/ai783/ddp_main.py>>>\ntrainer()\n\n<<<END>>>", "output": {"files": [{"path": "examples/ai783/dataloader_main.py", "reason": "Windows freeze 가능", "strategy": "regex", "tests": ["__main__ 가드"]}, {"path": "examples/ai783/ddp_main.py", "reason": "DDP 가드 없음", "strategy": "regex", "tests": ["__main__"]}], "notes": "spawn 보호"}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nGPU bf16 지원 시 dtype 선택 개선\n\n[CONTEXT]\n<<<FILE examples/ai60/train_utils.py>>>\nimport torch\n\n# BUG: dtype가 고정되어 혼합정밀에 불리\ndef get_dtype():\n    return torch.float16\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai60/train_utils.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["torch.bfloat16"]}, "code": "import torch\n\n\ndef get_dtype():\n    \"\"\"GPU가 bf16 지원 시 bfloat16, 아니면 float16 반환\"\"\"\n    if torch.cuda.is_available() and torch.cuda.is_bf16_supported():\n        return torch.bfloat16\n    return torch.float16\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n훈련 안정화: GA steps/로깅/평가/체크포인팅/스케줄러\n\n[CONTEXT]\n<<<FILE examples/ai339/training_args.py>>>\nfrom transformers import TrainingArguments\n\nargs = TrainingArguments(\n    output_dir=\"out\",\n    per_device_train_batch_size=1,\n    num_train_epochs=1,\n)\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai339/training_args.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["gradient_accumulation_steps"]}, "code": "from transformers import TrainingArguments\n\nargs = TrainingArguments(\n    output_dir=\"out\",\n    per_device_train_batch_size=1,\n    gradient_accumulation_steps=8,\n    num_train_epochs=1,\n    logging_steps=10,\n    eval_strategy=\"steps\",\n    eval_steps=50,\n    save_steps=50,\n    lr_scheduler_type=\"cosine\",\n    warmup_ratio=0.03,\n    fp16=False, bf16=True,\n    gradient_checkpointing=True,\n    max_grad_norm=1.0,\n)\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n재현성: 시드 함수 추가\n\n[CONTEXT]\n<<<FILE examples/ai52/seed_utils.py>>>\n# BUG: 시드 미설정\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai52/seed_utils.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["set_seed("]}, "code": "import os, random, numpy as np, torch\n\ndef set_seed(seed: int = 42):\n    random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nAMP 적용 + bf16 우선\n\n[CONTEXT]\n<<<FILE examples/ai277/train_step.py>>>\nimport torch\n\n# BUG: autocast 미사용\ndef train_step(model, batch):\n    x, y = batch\n    out = model(x)\n    loss = out.loss if hasattr(out, 'loss') else out.mean()\n    loss.backward()\n    return loss.item()\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai277/train_step.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["autocast"]}, "code": "import torch\n\n@torch.no_grad()\ndef _bf16_supported():\n    return torch.cuda.is_available() and torch.cuda.is_bf16_supported()\n\n\ndef train_step(model, batch):\n    x, y = batch\n    amp_dtype = torch.bfloat16 if _bf16_supported() else torch.float16\n    scaler = torch.cuda.amp.GradScaler(enabled=True)\n    with torch.cuda.amp.autocast(enabled=True, dtype=amp_dtype):\n        out = model(x)\n        loss = out.loss if hasattr(out, 'loss') else out.mean()\n    scaler.scale(loss).backward()\n    scaler.step(torch.optim.Adam(model.parameters(), lr=1e-4))\n    scaler.update()\n    model.zero_grad(set_to_none=True)\n    return float(loss.detach().item())\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nLoRA target_modules 확장\n\n[CONTEXT]\n<<<FILE examples/ai334/lora_cfg.py>>>\nfrom peft import LoraConfig\n\nLC = LoraConfig(r=8, target_modules=[\n    \"q_proj\", \"v_proj\"  # BUG: 부족\n])\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai334/lora_cfg.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["k_proj", "o_proj"]}, "code": "from peft import LoraConfig\n\nLC = LoraConfig(r=8, target_modules=[\n    \"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"\n])\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nDDP/torchrun 안전 가드\n\n[CONTEXT]\n<<<FILE examples/ai538/ddp_main.py>>>\nimport torch\n\n# BUG: torchrun에서 main 가드 없음\ntrainer()\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai538/ddp_main.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["__main__"]}, "code": "import torch\n\n\ndef main():\n    trainer()\n\n\nif __name__ == \"__main__\":\n    main()\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nAMP 적용 + bf16 우선\n\n[CONTEXT]\n<<<FILE examples/ai13/train_step.py>>>\nimport torch\n\n# BUG: autocast 미사용\ndef train_step(model, batch):\n    x, y = batch\n    out = model(x)\n    loss = out.loss if hasattr(out, 'loss') else out.mean()\n    loss.backward()\n    return loss.item()\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai13/train_step.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["autocast"]}, "code": "import torch\n\n@torch.no_grad()\ndef _bf16_supported():\n    return torch.cuda.is_available() and torch.cuda.is_bf16_supported()\n\n\ndef train_step(model, batch):\n    x, y = batch\n    amp_dtype = torch.bfloat16 if _bf16_supported() else torch.float16\n    scaler = torch.cuda.amp.GradScaler(enabled=True)\n    with torch.cuda.amp.autocast(enabled=True, dtype=amp_dtype):\n        out = model(x)\n        loss = out.loss if hasattr(out, 'loss') else out.mean()\n    scaler.scale(loss).backward()\n    scaler.step(torch.optim.Adam(model.parameters(), lr=1e-4))\n    scaler.update()\n    model.zero_grad(set_to_none=True)\n    return float(loss.detach().item())\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n학습 후 어댑터 저장 추가\n\n[CONTEXT]\n<<<FILE examples/ai341/save_after_train.py>>>\nfrom peft import PeftModel\n# ... 학습 후\n# BUG: 저장 누락\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai341/save_after_train.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["save_pretrained"]}, "code": "from peft import PeftModel\n# ... 학습 후\nmodel.save_pretrained(\"training/qlora-out/export\")\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nLoRA target_modules 확장\n\n[CONTEXT]\n<<<FILE examples/ai6/lora_cfg.py>>>\nfrom peft import LoraConfig\n\nLC = LoraConfig(r=8, target_modules=[\n    \"q_proj\", \"v_proj\"  # BUG: 부족\n])\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai6/lora_cfg.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["k_proj", "o_proj"]}, "code": "from peft import LoraConfig\n\nLC = LoraConfig(r=8, target_modules=[\n    \"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"\n])\n"}]}}
{"instruction": "Propose a minimal JSON plan for code changes. Output PLAN JSON only.", "input": "[INTENT]\n입출력/런타임 안전성 개선\n\n[CONTEXT]\n<<<FILE examples/ai722/dataloader_main.py>>>\nloader = DataLoader(...); for b in loader: pass\n\n<<<END>>>\n<<<FILE examples/ai722/ddp_main.py>>>\ntrainer()\n\n<<<END>>>", "output": {"files": [{"path": "examples/ai722/dataloader_main.py", "reason": "Windows freeze 가능", "strategy": "regex", "tests": ["__main__ 가드"]}, {"path": "examples/ai722/ddp_main.py", "reason": "DDP 가드 없음", "strategy": "regex", "tests": ["__main__"]}], "notes": "spawn 보호"}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n조기 종료 콜백 추가\n\n[CONTEXT]\n<<<FILE examples/ai115/callbacks.py>>>\nfrom transformers import TrainerCallback\n\nclass MyCB(TrainerCallback):\n    pass\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai115/callbacks.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["EarlyStoppingCallback"]}, "code": "from transformers import TrainerCallback, EarlyStoppingCallback\n\nclass MyCB(TrainerCallback):\n    pass\n\nCALLBACKS = [EarlyStoppingCallback(early_stopping_patience=2)]\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nDDP/torchrun 안전 가드\n\n[CONTEXT]\n<<<FILE examples/ai603/ddp_main.py>>>\nimport torch\n\n# BUG: torchrun에서 main 가드 없음\ntrainer()\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai603/ddp_main.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["__main__"]}, "code": "import torch\n\n\ndef main():\n    trainer()\n\n\nif __name__ == \"__main__\":\n    main()\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n조기 종료 콜백 추가\n\n[CONTEXT]\n<<<FILE examples/ai100/callbacks.py>>>\nfrom transformers import TrainerCallback\n\nclass MyCB(TrainerCallback):\n    pass\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai100/callbacks.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["EarlyStoppingCallback"]}, "code": "from transformers import TrainerCallback, EarlyStoppingCallback\n\nclass MyCB(TrainerCallback):\n    pass\n\nCALLBACKS = [EarlyStoppingCallback(early_stopping_patience=2)]\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nAMP 적용 + bf16 우선\n\n[CONTEXT]\n<<<FILE examples/ai158/train_step.py>>>\nimport torch\n\n# BUG: autocast 미사용\ndef train_step(model, batch):\n    x, y = batch\n    out = model(x)\n    loss = out.loss if hasattr(out, 'loss') else out.mean()\n    loss.backward()\n    return loss.item()\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai158/train_step.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["autocast"]}, "code": "import torch\n\n@torch.no_grad()\ndef _bf16_supported():\n    return torch.cuda.is_available() and torch.cuda.is_bf16_supported()\n\n\ndef train_step(model, batch):\n    x, y = batch\n    amp_dtype = torch.bfloat16 if _bf16_supported() else torch.float16\n    scaler = torch.cuda.amp.GradScaler(enabled=True)\n    with torch.cuda.amp.autocast(enabled=True, dtype=amp_dtype):\n        out = model(x)\n        loss = out.loss if hasattr(out, 'loss') else out.mean()\n    scaler.scale(loss).backward()\n    scaler.step(torch.optim.Adam(model.parameters(), lr=1e-4))\n    scaler.update()\n    model.zero_grad(set_to_none=True)\n    return float(loss.detach().item())\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n재현성: 시드 함수 추가\n\n[CONTEXT]\n<<<FILE examples/ai139/seed_utils.py>>>\n# BUG: 시드 미설정\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai139/seed_utils.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["set_seed("]}, "code": "import os, random, numpy as np, torch\n\ndef set_seed(seed: int = 42):\n    random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n재현성: 시드 함수 추가\n\n[CONTEXT]\n<<<FILE examples/ai551/seed_utils.py>>>\n# BUG: 시드 미설정\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai551/seed_utils.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["set_seed("]}, "code": "import os, random, numpy as np, torch\n\ndef set_seed(seed: int = 42):\n    random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n체크포인트 재개 지원\n\n[CONTEXT]\n<<<FILE examples/ai420/trainer_resume.py>>>\nfrom transformers import Trainer\ntrainer = Trainer(... )\n# BUG: 재시작 미지원\ntrainer.train()\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai420/trainer_resume.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["resume_from_checkpoint"]}, "code": "from transformers import Trainer\ntrainer = Trainer(... )\ntrainer.train(resume_from_checkpoint=True)\n"}]}}
{"instruction": "Propose a minimal JSON plan for code changes. Output PLAN JSON only.", "input": "[INTENT]\n입출력/런타임 안전성 개선\n\n[CONTEXT]\n<<<FILE examples/ai800/dataloader_main.py>>>\nloader = DataLoader(...); for b in loader: pass\n\n<<<END>>>\n<<<FILE examples/ai800/ddp_main.py>>>\ntrainer()\n\n<<<END>>>", "output": {"files": [{"path": "examples/ai800/dataloader_main.py", "reason": "Windows freeze 가능", "strategy": "regex", "tests": ["__main__ 가드"]}, {"path": "examples/ai800/ddp_main.py", "reason": "DDP 가드 없음", "strategy": "regex", "tests": ["__main__"]}], "notes": "spawn 보호"}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n8bit 옵티마이저 + 코사인 스케줄러\n\n[CONTEXT]\n<<<FILE examples/ai69/optim.py>>>\nimport torch\nopt = torch.optim.Adam([], lr=3e-4)\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai69/optim.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["PagedAdamW8bit"]}, "code": "from transformers import get_scheduler\nfrom bitsandbytes.optim import PagedAdamW8bit\nopt = PagedAdamW8bit([], lr=2e-4)\nsched = get_scheduler(\"cosine\", optimizer=opt, num_warmup_steps=100, num_training_steps=1000)\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nDDP/torchrun 안전 가드\n\n[CONTEXT]\n<<<FILE examples/ai59/ddp_main.py>>>\nimport torch\n\n# BUG: torchrun에서 main 가드 없음\ntrainer()\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai59/ddp_main.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["__main__"]}, "code": "import torch\n\n\ndef main():\n    trainer()\n\n\nif __name__ == \"__main__\":\n    main()\n"}]}}
{"instruction": "Propose a minimal JSON plan for code changes. Output PLAN JSON only.", "input": "[INTENT]\n입출력/런타임 안전성 개선\n\n[CONTEXT]\n<<<FILE examples/ai746/dataloader_main.py>>>\nloader = DataLoader(...); for b in loader: pass\n\n<<<END>>>\n<<<FILE examples/ai746/ddp_main.py>>>\ntrainer()\n\n<<<END>>>", "output": {"files": [{"path": "examples/ai746/dataloader_main.py", "reason": "Windows freeze 가능", "strategy": "regex", "tests": ["__main__ 가드"]}, {"path": "examples/ai746/ddp_main.py", "reason": "DDP 가드 없음", "strategy": "regex", "tests": ["__main__"]}], "notes": "spawn 보호"}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n조기 종료 콜백 추가\n\n[CONTEXT]\n<<<FILE examples/ai198/callbacks.py>>>\nfrom transformers import TrainerCallback\n\nclass MyCB(TrainerCallback):\n    pass\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai198/callbacks.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["EarlyStoppingCallback"]}, "code": "from transformers import TrainerCallback, EarlyStoppingCallback\n\nclass MyCB(TrainerCallback):\n    pass\n\nCALLBACKS = [EarlyStoppingCallback(early_stopping_patience=2)]\n"}]}}
{"instruction": "Propose a minimal JSON plan for code changes. Output PLAN JSON only.", "input": "[INTENT]\n학습 안정화(AMP+훈련 설정)\n\n[CONTEXT]\n<<<FILE examples/ai695/train_utils.py>>>\ndef get_dtype():\n    return torch.float16\n\n<<<END>>>\n<<<FILE examples/ai695/training_args.py>>>\nargs = TrainingArguments(output_dir='out', per_device_train_batch_size=1, num_train_epochs=1)\n\n<<<END>>>", "output": {"files": [{"path": "examples/ai695/train_utils.py", "reason": "dtype가 고정 float16", "strategy": "regex", "tests": ["bf16 또는 fp16 반환"]}, {"path": "examples/ai695/training_args.py", "reason": "로깅/평가/GA 설정 누락", "strategy": "regex", "tests": ["eval_strategy=='steps'"]}], "notes": "bf16 지원 시 우선"}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nGPU bf16 지원 시 dtype 선택 개선\n\n[CONTEXT]\n<<<FILE examples/ai535/train_utils.py>>>\nimport torch\n\n# BUG: dtype가 고정되어 혼합정밀에 불리\ndef get_dtype():\n    return torch.float16\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai535/train_utils.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["torch.bfloat16"]}, "code": "import torch\n\n\ndef get_dtype():\n    \"\"\"GPU가 bf16 지원 시 bfloat16, 아니면 float16 반환\"\"\"\n    if torch.cuda.is_available() and torch.cuda.is_bf16_supported():\n        return torch.bfloat16\n    return torch.float16\n"}]}}
{"instruction": "Propose a minimal JSON plan for code changes. Output PLAN JSON only.", "input": "[INTENT]\n입출력/런타임 안전성 개선\n\n[CONTEXT]\n<<<FILE examples/ai659/dataloader_main.py>>>\nloader = DataLoader(...); for b in loader: pass\n\n<<<END>>>\n<<<FILE examples/ai659/ddp_main.py>>>\ntrainer()\n\n<<<END>>>", "output": {"files": [{"path": "examples/ai659/dataloader_main.py", "reason": "Windows freeze 가능", "strategy": "regex", "tests": ["__main__ 가드"]}, {"path": "examples/ai659/ddp_main.py", "reason": "DDP 가드 없음", "strategy": "regex", "tests": ["__main__"]}], "notes": "spawn 보호"}}
{"instruction": "Propose a minimal JSON plan for code changes. Output PLAN JSON only.", "input": "[INTENT]\n입출력/런타임 안전성 개선\n\n[CONTEXT]\n<<<FILE examples/ai643/dataloader_main.py>>>\nloader = DataLoader(...); for b in loader: pass\n\n<<<END>>>\n<<<FILE examples/ai643/ddp_main.py>>>\ntrainer()\n\n<<<END>>>", "output": {"files": [{"path": "examples/ai643/dataloader_main.py", "reason": "Windows freeze 가능", "strategy": "regex", "tests": ["__main__ 가드"]}, {"path": "examples/ai643/ddp_main.py", "reason": "DDP 가드 없음", "strategy": "regex", "tests": ["__main__"]}], "notes": "spawn 보호"}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n8bit 옵티마이저 + 코사인 스케줄러\n\n[CONTEXT]\n<<<FILE examples/ai180/optim.py>>>\nimport torch\nopt = torch.optim.Adam([], lr=3e-4)\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai180/optim.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["PagedAdamW8bit"]}, "code": "from transformers import get_scheduler\nfrom bitsandbytes.optim import PagedAdamW8bit\nopt = PagedAdamW8bit([], lr=2e-4)\nsched = get_scheduler(\"cosine\", optimizer=opt, num_warmup_steps=100, num_training_steps=1000)\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n8bit 옵티마이저 + 코사인 스케줄러\n\n[CONTEXT]\n<<<FILE examples/ai120/optim.py>>>\nimport torch\nopt = torch.optim.Adam([], lr=3e-4)\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai120/optim.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["PagedAdamW8bit"]}, "code": "from transformers import get_scheduler\nfrom bitsandbytes.optim import PagedAdamW8bit\nopt = PagedAdamW8bit([], lr=2e-4)\nsched = get_scheduler(\"cosine\", optimizer=opt, num_warmup_steps=100, num_training_steps=1000)\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nLoRA target_modules 확장\n\n[CONTEXT]\n<<<FILE examples/ai597/lora_cfg.py>>>\nfrom peft import LoraConfig\n\nLC = LoraConfig(r=8, target_modules=[\n    \"q_proj\", \"v_proj\"  # BUG: 부족\n])\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai597/lora_cfg.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["k_proj", "o_proj"]}, "code": "from peft import LoraConfig\n\nLC = LoraConfig(r=8, target_modules=[\n    \"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"\n])\n"}]}}
