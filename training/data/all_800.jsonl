{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nflash 미지원 시 sdpa 안전 경로\n\n[CONTEXT]\n<<<FILE examples/ai149/model_load.py>>>\nfrom transformers import AutoModelForCausalLM\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"Qwen/Qwen3-8B\",\n)\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai149/model_load.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["attn_implementation"]}, "code": "from transformers import AutoModelForCausalLM\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"Qwen/Qwen3-8B\",\n    attn_implementation=\"sdpa\",\n)\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nflash 미지원 시 sdpa 안전 경로\n\n[CONTEXT]\n<<<FILE examples/ai430/model_load.py>>>\nfrom transformers import AutoModelForCausalLM\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"Qwen/Qwen3-8B\",\n)\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai430/model_load.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["attn_implementation"]}, "code": "from transformers import AutoModelForCausalLM\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"Qwen/Qwen3-8B\",\n    attn_implementation=\"sdpa\",\n)\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n재현성: 시드 함수 추가\n\n[CONTEXT]\n<<<FILE examples/ai112/seed_utils.py>>>\n# BUG: 시드 미설정\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai112/seed_utils.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["set_seed("]}, "code": "import os, random, numpy as np, torch\n\ndef set_seed(seed: int = 42):\n    random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nAMP 적용 + bf16 우선\n\n[CONTEXT]\n<<<FILE examples/ai298/train_step.py>>>\nimport torch\n\n# BUG: autocast 미사용\ndef train_step(model, batch):\n    x, y = batch\n    out = model(x)\n    loss = out.loss if hasattr(out, 'loss') else out.mean()\n    loss.backward()\n    return loss.item()\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai298/train_step.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["autocast"]}, "code": "import torch\n\n@torch.no_grad()\ndef _bf16_supported():\n    return torch.cuda.is_available() and torch.cuda.is_bf16_supported()\n\n\ndef train_step(model, batch):\n    x, y = batch\n    amp_dtype = torch.bfloat16 if _bf16_supported() else torch.float16\n    scaler = torch.cuda.amp.GradScaler(enabled=True)\n    with torch.cuda.amp.autocast(enabled=True, dtype=amp_dtype):\n        out = model(x)\n        loss = out.loss if hasattr(out, 'loss') else out.mean()\n    scaler.scale(loss).backward()\n    scaler.step(torch.optim.Adam(model.parameters(), lr=1e-4))\n    scaler.update()\n    model.zero_grad(set_to_none=True)\n    return float(loss.detach().item())\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nflash 미지원 시 sdpa 안전 경로\n\n[CONTEXT]\n<<<FILE examples/ai376/model_load.py>>>\nfrom transformers import AutoModelForCausalLM\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"Qwen/Qwen3-8B\",\n)\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai376/model_load.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["attn_implementation"]}, "code": "from transformers import AutoModelForCausalLM\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"Qwen/Qwen3-8B\",\n    attn_implementation=\"sdpa\",\n)\n"}]}}
{"instruction": "Propose a minimal JSON plan for code changes. Output PLAN JSON only.", "input": "[INTENT]\n입출력/런타임 안전성 개선\n\n[CONTEXT]\n<<<FILE examples/ai749/dataloader_main.py>>>\nloader = DataLoader(...); for b in loader: pass\n\n<<<END>>>\n<<<FILE examples/ai749/ddp_main.py>>>\ntrainer()\n\n<<<END>>>", "output": {"files": [{"path": "examples/ai749/dataloader_main.py", "reason": "Windows freeze 가능", "strategy": "regex", "tests": ["__main__ 가드"]}, {"path": "examples/ai749/ddp_main.py", "reason": "DDP 가드 없음", "strategy": "regex", "tests": ["__main__"]}], "notes": "spawn 보호"}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n체크포인트 재개 지원\n\n[CONTEXT]\n<<<FILE examples/ai468/trainer_resume.py>>>\nfrom transformers import Trainer\ntrainer = Trainer(... )\n# BUG: 재시작 미지원\ntrainer.train()\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai468/trainer_resume.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["resume_from_checkpoint"]}, "code": "from transformers import Trainer\ntrainer = Trainer(... )\ntrainer.train(resume_from_checkpoint=True)\n"}]}}
{"instruction": "Propose a minimal JSON plan for code changes. Output PLAN JSON only.", "input": "[INTENT]\n학습 안정화(AMP+훈련 설정)\n\n[CONTEXT]\n<<<FILE examples/ai730/train_utils.py>>>\ndef get_dtype():\n    return torch.float16\n\n<<<END>>>\n<<<FILE examples/ai730/training_args.py>>>\nargs = TrainingArguments(output_dir='out', per_device_train_batch_size=1, num_train_epochs=1)\n\n<<<END>>>", "output": {"files": [{"path": "examples/ai730/train_utils.py", "reason": "dtype가 고정 float16", "strategy": "regex", "tests": ["bf16 또는 fp16 반환"]}, {"path": "examples/ai730/training_args.py", "reason": "로깅/평가/GA 설정 누락", "strategy": "regex", "tests": ["eval_strategy=='steps'"]}], "notes": "bf16 지원 시 우선"}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n훈련 안정화: GA steps/로깅/평가/체크포인팅/스케줄러\n\n[CONTEXT]\n<<<FILE examples/ai110/training_args.py>>>\nfrom transformers import TrainingArguments\n\nargs = TrainingArguments(\n    output_dir=\"out\",\n    per_device_train_batch_size=1,\n    num_train_epochs=1,\n)\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai110/training_args.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["gradient_accumulation_steps"]}, "code": "from transformers import TrainingArguments\n\nargs = TrainingArguments(\n    output_dir=\"out\",\n    per_device_train_batch_size=1,\n    gradient_accumulation_steps=8,\n    num_train_epochs=1,\n    logging_steps=10,\n    eval_strategy=\"steps\",\n    eval_steps=50,\n    save_steps=50,\n    lr_scheduler_type=\"cosine\",\n    warmup_ratio=0.03,\n    fp16=False, bf16=True,\n    gradient_checkpointing=True,\n    max_grad_norm=1.0,\n)\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n체크포인트 재개 지원\n\n[CONTEXT]\n<<<FILE examples/ai420/trainer_resume.py>>>\nfrom transformers import Trainer\ntrainer = Trainer(... )\n# BUG: 재시작 미지원\ntrainer.train()\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai420/trainer_resume.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["resume_from_checkpoint"]}, "code": "from transformers import Trainer\ntrainer = Trainer(... )\ntrainer.train(resume_from_checkpoint=True)\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nWindows에서 DataLoader 프리즈 방지 + pin_memory/persistent_workers\n\n[CONTEXT]\n<<<FILE examples/ai620/dataloader_main.py>>>\nimport torch, torch.utils.data as tud\n\nclass Dummy(torch.utils.data.Dataset):\n    def __len__(self): return 8\n    def __getitem__(self, i): return i\n\n# BUG: 모듈 임포트만으로 워커가 스폰되어 Windows에서 프리즈\nloader = tud.DataLoader(Dummy(), batch_size=2, num_workers=4)\nfor b in loader:\n    pass\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai620/dataloader_main.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["if __name__ == \"__main__\":"]}, "code": "import torch, torch.utils.data as tud\n\nclass Dummy(torch.utils.data.Dataset):\n    def __len__(self): return 8\n    def __getitem__(self, i): return i\n\n\ndef main():\n    loader = tud.DataLoader(Dummy(), batch_size=2, num_workers=4, pin_memory=True, persistent_workers=True)\n    for _ in loader:\n        pass\n\n\nif __name__ == \"__main__\":\n    main()\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nDDP/torchrun 안전 가드\n\n[CONTEXT]\n<<<FILE examples/ai510/ddp_main.py>>>\nimport torch\n\n# BUG: torchrun에서 main 가드 없음\ntrainer()\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai510/ddp_main.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["__main__"]}, "code": "import torch\n\n\ndef main():\n    trainer()\n\n\nif __name__ == \"__main__\":\n    main()\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nWindows에서 DataLoader 프리즈 방지 + pin_memory/persistent_workers\n\n[CONTEXT]\n<<<FILE examples/ai458/dataloader_main.py>>>\nimport torch, torch.utils.data as tud\n\nclass Dummy(torch.utils.data.Dataset):\n    def __len__(self): return 8\n    def __getitem__(self, i): return i\n\n# BUG: 모듈 임포트만으로 워커가 스폰되어 Windows에서 프리즈\nloader = tud.DataLoader(Dummy(), batch_size=2, num_workers=4)\nfor b in loader:\n    pass\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai458/dataloader_main.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["if __name__ == \"__main__\":"]}, "code": "import torch, torch.utils.data as tud\n\nclass Dummy(torch.utils.data.Dataset):\n    def __len__(self): return 8\n    def __getitem__(self, i): return i\n\n\ndef main():\n    loader = tud.DataLoader(Dummy(), batch_size=2, num_workers=4, pin_memory=True, persistent_workers=True)\n    for _ in loader:\n        pass\n\n\nif __name__ == \"__main__\":\n    main()\n"}]}}
{"instruction": "Propose a minimal JSON plan for code changes. Output PLAN JSON only.", "input": "[INTENT]\n학습 안정화(AMP+훈련 설정)\n\n[CONTEXT]\n<<<FILE examples/ai695/train_utils.py>>>\ndef get_dtype():\n    return torch.float16\n\n<<<END>>>\n<<<FILE examples/ai695/training_args.py>>>\nargs = TrainingArguments(output_dir='out', per_device_train_batch_size=1, num_train_epochs=1)\n\n<<<END>>>", "output": {"files": [{"path": "examples/ai695/train_utils.py", "reason": "dtype가 고정 float16", "strategy": "regex", "tests": ["bf16 또는 fp16 반환"]}, {"path": "examples/ai695/training_args.py", "reason": "로깅/평가/GA 설정 누락", "strategy": "regex", "tests": ["eval_strategy=='steps'"]}], "notes": "bf16 지원 시 우선"}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n훈련 안정화: GA steps/로깅/평가/체크포인팅/스케줄러\n\n[CONTEXT]\n<<<FILE examples/ai563/training_args.py>>>\nfrom transformers import TrainingArguments\n\nargs = TrainingArguments(\n    output_dir=\"out\",\n    per_device_train_batch_size=1,\n    num_train_epochs=1,\n)\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai563/training_args.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["gradient_accumulation_steps"]}, "code": "from transformers import TrainingArguments\n\nargs = TrainingArguments(\n    output_dir=\"out\",\n    per_device_train_batch_size=1,\n    gradient_accumulation_steps=8,\n    num_train_epochs=1,\n    logging_steps=10,\n    eval_strategy=\"steps\",\n    eval_steps=50,\n    save_steps=50,\n    lr_scheduler_type=\"cosine\",\n    warmup_ratio=0.03,\n    fp16=False, bf16=True,\n    gradient_checkpointing=True,\n    max_grad_norm=1.0,\n)\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n훈련 안정화: GA steps/로깅/평가/체크포인팅/스케줄러\n\n[CONTEXT]\n<<<FILE examples/ai616/training_args.py>>>\nfrom transformers import TrainingArguments\n\nargs = TrainingArguments(\n    output_dir=\"out\",\n    per_device_train_batch_size=1,\n    num_train_epochs=1,\n)\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai616/training_args.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["gradient_accumulation_steps"]}, "code": "from transformers import TrainingArguments\n\nargs = TrainingArguments(\n    output_dir=\"out\",\n    per_device_train_batch_size=1,\n    gradient_accumulation_steps=8,\n    num_train_epochs=1,\n    logging_steps=10,\n    eval_strategy=\"steps\",\n    eval_steps=50,\n    save_steps=50,\n    lr_scheduler_type=\"cosine\",\n    warmup_ratio=0.03,\n    fp16=False, bf16=True,\n    gradient_checkpointing=True,\n    max_grad_norm=1.0,\n)\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n조기 종료 콜백 추가\n\n[CONTEXT]\n<<<FILE examples/ai183/callbacks.py>>>\nfrom transformers import TrainerCallback\n\nclass MyCB(TrainerCallback):\n    pass\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai183/callbacks.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["EarlyStoppingCallback"]}, "code": "from transformers import TrainerCallback, EarlyStoppingCallback\n\nclass MyCB(TrainerCallback):\n    pass\n\nCALLBACKS = [EarlyStoppingCallback(early_stopping_patience=2)]\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nflash 미지원 시 sdpa 안전 경로\n\n[CONTEXT]\n<<<FILE examples/ai467/model_load.py>>>\nfrom transformers import AutoModelForCausalLM\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"Qwen/Qwen3-8B\",\n)\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai467/model_load.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["attn_implementation"]}, "code": "from transformers import AutoModelForCausalLM\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"Qwen/Qwen3-8B\",\n    attn_implementation=\"sdpa\",\n)\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nDDP/torchrun 안전 가드\n\n[CONTEXT]\n<<<FILE examples/ai374/ddp_main.py>>>\nimport torch\n\n# BUG: torchrun에서 main 가드 없음\ntrainer()\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai374/ddp_main.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["__main__"]}, "code": "import torch\n\n\ndef main():\n    trainer()\n\n\nif __name__ == \"__main__\":\n    main()\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nAMP 적용 + bf16 우선\n\n[CONTEXT]\n<<<FILE examples/ai381/train_step.py>>>\nimport torch\n\n# BUG: autocast 미사용\ndef train_step(model, batch):\n    x, y = batch\n    out = model(x)\n    loss = out.loss if hasattr(out, 'loss') else out.mean()\n    loss.backward()\n    return loss.item()\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai381/train_step.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["autocast"]}, "code": "import torch\n\n@torch.no_grad()\ndef _bf16_supported():\n    return torch.cuda.is_available() and torch.cuda.is_bf16_supported()\n\n\ndef train_step(model, batch):\n    x, y = batch\n    amp_dtype = torch.bfloat16 if _bf16_supported() else torch.float16\n    scaler = torch.cuda.amp.GradScaler(enabled=True)\n    with torch.cuda.amp.autocast(enabled=True, dtype=amp_dtype):\n        out = model(x)\n        loss = out.loss if hasattr(out, 'loss') else out.mean()\n    scaler.scale(loss).backward()\n    scaler.step(torch.optim.Adam(model.parameters(), lr=1e-4))\n    scaler.update()\n    model.zero_grad(set_to_none=True)\n    return float(loss.detach().item())\n"}]}}
{"instruction": "Propose a minimal JSON plan for code changes. Output PLAN JSON only.", "input": "[INTENT]\n학습 안정화(AMP+훈련 설정)\n\n[CONTEXT]\n<<<FILE examples/ai798/train_utils.py>>>\ndef get_dtype():\n    return torch.float16\n\n<<<END>>>\n<<<FILE examples/ai798/training_args.py>>>\nargs = TrainingArguments(output_dir='out', per_device_train_batch_size=1, num_train_epochs=1)\n\n<<<END>>>", "output": {"files": [{"path": "examples/ai798/train_utils.py", "reason": "dtype가 고정 float16", "strategy": "regex", "tests": ["bf16 또는 fp16 반환"]}, {"path": "examples/ai798/training_args.py", "reason": "로깅/평가/GA 설정 누락", "strategy": "regex", "tests": ["eval_strategy=='steps'"]}], "notes": "bf16 지원 시 우선"}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nWindows에서 DataLoader 프리즈 방지 + pin_memory/persistent_workers\n\n[CONTEXT]\n<<<FILE examples/ai64/dataloader_main.py>>>\nimport torch, torch.utils.data as tud\n\nclass Dummy(torch.utils.data.Dataset):\n    def __len__(self): return 8\n    def __getitem__(self, i): return i\n\n# BUG: 모듈 임포트만으로 워커가 스폰되어 Windows에서 프리즈\nloader = tud.DataLoader(Dummy(), batch_size=2, num_workers=4)\nfor b in loader:\n    pass\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai64/dataloader_main.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["if __name__ == \"__main__\":"]}, "code": "import torch, torch.utils.data as tud\n\nclass Dummy(torch.utils.data.Dataset):\n    def __len__(self): return 8\n    def __getitem__(self, i): return i\n\n\ndef main():\n    loader = tud.DataLoader(Dummy(), batch_size=2, num_workers=4, pin_memory=True, persistent_workers=True)\n    for _ in loader:\n        pass\n\n\nif __name__ == \"__main__\":\n    main()\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n8bit 옵티마이저 + 코사인 스케줄러\n\n[CONTEXT]\n<<<FILE examples/ai592/optim.py>>>\nimport torch\nopt = torch.optim.Adam([], lr=3e-4)\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai592/optim.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["PagedAdamW8bit"]}, "code": "from transformers import get_scheduler\nfrom bitsandbytes.optim import PagedAdamW8bit\nopt = PagedAdamW8bit([], lr=2e-4)\nsched = get_scheduler(\"cosine\", optimizer=opt, num_warmup_steps=100, num_training_steps=1000)\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\npad 토큰 ID 정확히 설정\n\n[CONTEXT]\n<<<FILE examples/ai429/label_cfg.py>>>\nfrom transformers import AutoTokenizer\nfrom dataclasses import dataclass\n\n@dataclass\nclass Cfg:\n    pad_id: int = -100  # BUG\n\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai429/label_cfg.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["CFG = Cfg"]}, "code": "from transformers import AutoTokenizer\nfrom dataclasses import dataclass\n\n@dataclass\nclass Cfg:\n    pad_id: int | None = None\n\n# tokenizer 로드 후 설정\ntok = AutoTokenizer.from_pretrained(\"Qwen/Qwen3-8B\", use_fast=True)\nCFG = Cfg(pad_id=tok.pad_token_id)\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n조기 종료 콜백 추가\n\n[CONTEXT]\n<<<FILE examples/ai483/callbacks.py>>>\nfrom transformers import TrainerCallback\n\nclass MyCB(TrainerCallback):\n    pass\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai483/callbacks.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["EarlyStoppingCallback"]}, "code": "from transformers import TrainerCallback, EarlyStoppingCallback\n\nclass MyCB(TrainerCallback):\n    pass\n\nCALLBACKS = [EarlyStoppingCallback(early_stopping_patience=2)]\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n훈련 안정화: GA steps/로깅/평가/체크포인팅/스케줄러\n\n[CONTEXT]\n<<<FILE examples/ai245/training_args.py>>>\nfrom transformers import TrainingArguments\n\nargs = TrainingArguments(\n    output_dir=\"out\",\n    per_device_train_batch_size=1,\n    num_train_epochs=1,\n)\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai245/training_args.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["gradient_accumulation_steps"]}, "code": "from transformers import TrainingArguments\n\nargs = TrainingArguments(\n    output_dir=\"out\",\n    per_device_train_batch_size=1,\n    gradient_accumulation_steps=8,\n    num_train_epochs=1,\n    logging_steps=10,\n    eval_strategy=\"steps\",\n    eval_steps=50,\n    save_steps=50,\n    lr_scheduler_type=\"cosine\",\n    warmup_ratio=0.03,\n    fp16=False, bf16=True,\n    gradient_checkpointing=True,\n    max_grad_norm=1.0,\n)\n"}]}}
{"instruction": "Propose a minimal JSON plan for code changes. Output PLAN JSON only.", "input": "[INTENT]\n입출력/런타임 안전성 개선\n\n[CONTEXT]\n<<<FILE examples/ai776/dataloader_main.py>>>\nloader = DataLoader(...); for b in loader: pass\n\n<<<END>>>\n<<<FILE examples/ai776/ddp_main.py>>>\ntrainer()\n\n<<<END>>>", "output": {"files": [{"path": "examples/ai776/dataloader_main.py", "reason": "Windows freeze 가능", "strategy": "regex", "tests": ["__main__ 가드"]}, {"path": "examples/ai776/ddp_main.py", "reason": "DDP 가드 없음", "strategy": "regex", "tests": ["__main__"]}], "notes": "spawn 보호"}}
{"instruction": "Propose a minimal JSON plan for code changes. Output PLAN JSON only.", "input": "[INTENT]\n입출력/런타임 안전성 개선\n\n[CONTEXT]\n<<<FILE examples/ai663/dataloader_main.py>>>\nloader = DataLoader(...); for b in loader: pass\n\n<<<END>>>\n<<<FILE examples/ai663/ddp_main.py>>>\ntrainer()\n\n<<<END>>>", "output": {"files": [{"path": "examples/ai663/dataloader_main.py", "reason": "Windows freeze 가능", "strategy": "regex", "tests": ["__main__ 가드"]}, {"path": "examples/ai663/ddp_main.py", "reason": "DDP 가드 없음", "strategy": "regex", "tests": ["__main__"]}], "notes": "spawn 보호"}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nDDP/torchrun 안전 가드\n\n[CONTEXT]\n<<<FILE examples/ai202/ddp_main.py>>>\nimport torch\n\n# BUG: torchrun에서 main 가드 없음\ntrainer()\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai202/ddp_main.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["__main__"]}, "code": "import torch\n\n\ndef main():\n    trainer()\n\n\nif __name__ == \"__main__\":\n    main()\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n조기 종료 콜백 추가\n\n[CONTEXT]\n<<<FILE examples/ai518/callbacks.py>>>\nfrom transformers import TrainerCallback\n\nclass MyCB(TrainerCallback):\n    pass\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai518/callbacks.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["EarlyStoppingCallback"]}, "code": "from transformers import TrainerCallback, EarlyStoppingCallback\n\nclass MyCB(TrainerCallback):\n    pass\n\nCALLBACKS = [EarlyStoppingCallback(early_stopping_patience=2)]\n"}]}}
{"instruction": "Propose a minimal JSON plan for code changes. Output PLAN JSON only.", "input": "[INTENT]\n입출력/런타임 안전성 개선\n\n[CONTEXT]\n<<<FILE examples/ai674/dataloader_main.py>>>\nloader = DataLoader(...); for b in loader: pass\n\n<<<END>>>\n<<<FILE examples/ai674/ddp_main.py>>>\ntrainer()\n\n<<<END>>>", "output": {"files": [{"path": "examples/ai674/dataloader_main.py", "reason": "Windows freeze 가능", "strategy": "regex", "tests": ["__main__ 가드"]}, {"path": "examples/ai674/ddp_main.py", "reason": "DDP 가드 없음", "strategy": "regex", "tests": ["__main__"]}], "notes": "spawn 보호"}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n재현성: 시드 함수 추가\n\n[CONTEXT]\n<<<FILE examples/ai326/seed_utils.py>>>\n# BUG: 시드 미설정\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai326/seed_utils.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["set_seed("]}, "code": "import os, random, numpy as np, torch\n\ndef set_seed(seed: int = 42):\n    random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nAMP 적용 + bf16 우선\n\n[CONTEXT]\n<<<FILE examples/ai517/train_step.py>>>\nimport torch\n\n# BUG: autocast 미사용\ndef train_step(model, batch):\n    x, y = batch\n    out = model(x)\n    loss = out.loss if hasattr(out, 'loss') else out.mean()\n    loss.backward()\n    return loss.item()\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai517/train_step.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["autocast"]}, "code": "import torch\n\n@torch.no_grad()\ndef _bf16_supported():\n    return torch.cuda.is_available() and torch.cuda.is_bf16_supported()\n\n\ndef train_step(model, batch):\n    x, y = batch\n    amp_dtype = torch.bfloat16 if _bf16_supported() else torch.float16\n    scaler = torch.cuda.amp.GradScaler(enabled=True)\n    with torch.cuda.amp.autocast(enabled=True, dtype=amp_dtype):\n        out = model(x)\n        loss = out.loss if hasattr(out, 'loss') else out.mean()\n    scaler.scale(loss).backward()\n    scaler.step(torch.optim.Adam(model.parameters(), lr=1e-4))\n    scaler.update()\n    model.zero_grad(set_to_none=True)\n    return float(loss.detach().item())\n"}]}}
{"instruction": "Propose a minimal JSON plan for code changes. Output PLAN JSON only.", "input": "[INTENT]\n학습 안정화(AMP+훈련 설정)\n\n[CONTEXT]\n<<<FILE examples/ai758/train_utils.py>>>\ndef get_dtype():\n    return torch.float16\n\n<<<END>>>\n<<<FILE examples/ai758/training_args.py>>>\nargs = TrainingArguments(output_dir='out', per_device_train_batch_size=1, num_train_epochs=1)\n\n<<<END>>>", "output": {"files": [{"path": "examples/ai758/train_utils.py", "reason": "dtype가 고정 float16", "strategy": "regex", "tests": ["bf16 또는 fp16 반환"]}, {"path": "examples/ai758/training_args.py", "reason": "로깅/평가/GA 설정 누락", "strategy": "regex", "tests": ["eval_strategy=='steps'"]}], "notes": "bf16 지원 시 우선"}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nGPU bf16 지원 시 dtype 선택 개선\n\n[CONTEXT]\n<<<FILE examples/ai472/train_utils.py>>>\nimport torch\n\n# BUG: dtype가 고정되어 혼합정밀에 불리\ndef get_dtype():\n    return torch.float16\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai472/train_utils.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["torch.bfloat16"]}, "code": "import torch\n\n\ndef get_dtype():\n    \"\"\"GPU가 bf16 지원 시 bfloat16, 아니면 float16 반환\"\"\"\n    if torch.cuda.is_available() and torch.cuda.is_bf16_supported():\n        return torch.bfloat16\n    return torch.float16\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nAMP 적용 + bf16 우선\n\n[CONTEXT]\n<<<FILE examples/ai201/train_step.py>>>\nimport torch\n\n# BUG: autocast 미사용\ndef train_step(model, batch):\n    x, y = batch\n    out = model(x)\n    loss = out.loss if hasattr(out, 'loss') else out.mean()\n    loss.backward()\n    return loss.item()\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai201/train_step.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["autocast"]}, "code": "import torch\n\n@torch.no_grad()\ndef _bf16_supported():\n    return torch.cuda.is_available() and torch.cuda.is_bf16_supported()\n\n\ndef train_step(model, batch):\n    x, y = batch\n    amp_dtype = torch.bfloat16 if _bf16_supported() else torch.float16\n    scaler = torch.cuda.amp.GradScaler(enabled=True)\n    with torch.cuda.amp.autocast(enabled=True, dtype=amp_dtype):\n        out = model(x)\n        loss = out.loss if hasattr(out, 'loss') else out.mean()\n    scaler.scale(loss).backward()\n    scaler.step(torch.optim.Adam(model.parameters(), lr=1e-4))\n    scaler.update()\n    model.zero_grad(set_to_none=True)\n    return float(loss.detach().item())\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n조기 종료 콜백 추가\n\n[CONTEXT]\n<<<FILE examples/ai115/callbacks.py>>>\nfrom transformers import TrainerCallback\n\nclass MyCB(TrainerCallback):\n    pass\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai115/callbacks.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["EarlyStoppingCallback"]}, "code": "from transformers import TrainerCallback, EarlyStoppingCallback\n\nclass MyCB(TrainerCallback):\n    pass\n\nCALLBACKS = [EarlyStoppingCallback(early_stopping_patience=2)]\n"}]}}
{"instruction": "Propose a minimal JSON plan for code changes. Output PLAN JSON only.", "input": "[INTENT]\n학습 안정화(AMP+훈련 설정)\n\n[CONTEXT]\n<<<FILE examples/ai794/train_utils.py>>>\ndef get_dtype():\n    return torch.float16\n\n<<<END>>>\n<<<FILE examples/ai794/training_args.py>>>\nargs = TrainingArguments(output_dir='out', per_device_train_batch_size=1, num_train_epochs=1)\n\n<<<END>>>", "output": {"files": [{"path": "examples/ai794/train_utils.py", "reason": "dtype가 고정 float16", "strategy": "regex", "tests": ["bf16 또는 fp16 반환"]}, {"path": "examples/ai794/training_args.py", "reason": "로깅/평가/GA 설정 누락", "strategy": "regex", "tests": ["eval_strategy=='steps'"]}], "notes": "bf16 지원 시 우선"}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n학습 후 어댑터 저장 추가\n\n[CONTEXT]\n<<<FILE examples/ai363/save_after_train.py>>>\nfrom peft import PeftModel\n# ... 학습 후\n# BUG: 저장 누락\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai363/save_after_train.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["save_pretrained"]}, "code": "from peft import PeftModel\n# ... 학습 후\nmodel.save_pretrained(\"training/qlora-out/export\")\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n필수 import 추가\n\n[CONTEXT]\n<<<FILE examples/ai352/imports.py>>>\n# BUG: numpy 미사용이지만 추후 사용 예정\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai352/imports.py", "loc": {"type": "anchor", "before": "\\A"}, "action": "insert_after", "once": true, "pre": {"must_not_contain": ["import numpy as np"]}, "code": "import numpy as np\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n훈련 안정화: GA steps/로깅/평가/체크포인팅/스케줄러\n\n[CONTEXT]\n<<<FILE examples/ai368/training_args.py>>>\nfrom transformers import TrainingArguments\n\nargs = TrainingArguments(\n    output_dir=\"out\",\n    per_device_train_batch_size=1,\n    num_train_epochs=1,\n)\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai368/training_args.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["gradient_accumulation_steps"]}, "code": "from transformers import TrainingArguments\n\nargs = TrainingArguments(\n    output_dir=\"out\",\n    per_device_train_batch_size=1,\n    gradient_accumulation_steps=8,\n    num_train_epochs=1,\n    logging_steps=10,\n    eval_strategy=\"steps\",\n    eval_steps=50,\n    save_steps=50,\n    lr_scheduler_type=\"cosine\",\n    warmup_ratio=0.03,\n    fp16=False, bf16=True,\n    gradient_checkpointing=True,\n    max_grad_norm=1.0,\n)\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nflash 미지원 시 sdpa 안전 경로\n\n[CONTEXT]\n<<<FILE examples/ai99/model_load.py>>>\nfrom transformers import AutoModelForCausalLM\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"Qwen/Qwen3-8B\",\n)\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai99/model_load.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["attn_implementation"]}, "code": "from transformers import AutoModelForCausalLM\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"Qwen/Qwen3-8B\",\n    attn_implementation=\"sdpa\",\n)\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n재현성: 시드 함수 추가\n\n[CONTEXT]\n<<<FILE examples/ai608/seed_utils.py>>>\n# BUG: 시드 미설정\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai608/seed_utils.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["set_seed("]}, "code": "import os, random, numpy as np, torch\n\ndef set_seed(seed: int = 42):\n    random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n재현성: 시드 함수 추가\n\n[CONTEXT]\n<<<FILE examples/ai369/seed_utils.py>>>\n# BUG: 시드 미설정\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai369/seed_utils.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["set_seed("]}, "code": "import os, random, numpy as np, torch\n\ndef set_seed(seed: int = 42):\n    random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\npad 토큰 ID 정확히 설정\n\n[CONTEXT]\n<<<FILE examples/ai44/label_cfg.py>>>\nfrom transformers import AutoTokenizer\nfrom dataclasses import dataclass\n\n@dataclass\nclass Cfg:\n    pad_id: int = -100  # BUG\n\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai44/label_cfg.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["CFG = Cfg"]}, "code": "from transformers import AutoTokenizer\nfrom dataclasses import dataclass\n\n@dataclass\nclass Cfg:\n    pad_id: int | None = None\n\n# tokenizer 로드 후 설정\ntok = AutoTokenizer.from_pretrained(\"Qwen/Qwen3-8B\", use_fast=True)\nCFG = Cfg(pad_id=tok.pad_token_id)\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nflash 미지원 시 sdpa 안전 경로\n\n[CONTEXT]\n<<<FILE examples/ai208/model_load.py>>>\nfrom transformers import AutoModelForCausalLM\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"Qwen/Qwen3-8B\",\n)\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai208/model_load.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["attn_implementation"]}, "code": "from transformers import AutoModelForCausalLM\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"Qwen/Qwen3-8B\",\n    attn_implementation=\"sdpa\",\n)\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n필수 import 추가\n\n[CONTEXT]\n<<<FILE examples/ai425/imports.py>>>\n# BUG: numpy 미사용이지만 추후 사용 예정\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai425/imports.py", "loc": {"type": "anchor", "before": "\\A"}, "action": "insert_after", "once": true, "pre": {"must_not_contain": ["import numpy as np"]}, "code": "import numpy as np\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n학습 후 어댑터 저장 추가\n\n[CONTEXT]\n<<<FILE examples/ai442/save_after_train.py>>>\nfrom peft import PeftModel\n# ... 학습 후\n# BUG: 저장 누락\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai442/save_after_train.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["save_pretrained"]}, "code": "from peft import PeftModel\n# ... 학습 후\nmodel.save_pretrained(\"training/qlora-out/export\")\n"}]}}
{"instruction": "Propose a minimal JSON plan for code changes. Output PLAN JSON only.", "input": "[INTENT]\n입출력/런타임 안전성 개선\n\n[CONTEXT]\n<<<FILE examples/ai761/dataloader_main.py>>>\nloader = DataLoader(...); for b in loader: pass\n\n<<<END>>>\n<<<FILE examples/ai761/ddp_main.py>>>\ntrainer()\n\n<<<END>>>", "output": {"files": [{"path": "examples/ai761/dataloader_main.py", "reason": "Windows freeze 가능", "strategy": "regex", "tests": ["__main__ 가드"]}, {"path": "examples/ai761/ddp_main.py", "reason": "DDP 가드 없음", "strategy": "regex", "tests": ["__main__"]}], "notes": "spawn 보호"}}
{"instruction": "Propose a minimal JSON plan for code changes. Output PLAN JSON only.", "input": "[INTENT]\n학습 안정화(AMP+훈련 설정)\n\n[CONTEXT]\n<<<FILE examples/ai727/train_utils.py>>>\ndef get_dtype():\n    return torch.float16\n\n<<<END>>>\n<<<FILE examples/ai727/training_args.py>>>\nargs = TrainingArguments(output_dir='out', per_device_train_batch_size=1, num_train_epochs=1)\n\n<<<END>>>", "output": {"files": [{"path": "examples/ai727/train_utils.py", "reason": "dtype가 고정 float16", "strategy": "regex", "tests": ["bf16 또는 fp16 반환"]}, {"path": "examples/ai727/training_args.py", "reason": "로깅/평가/GA 설정 누락", "strategy": "regex", "tests": ["eval_strategy=='steps'"]}], "notes": "bf16 지원 시 우선"}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n8bit 옵티마이저 + 코사인 스케줄러\n\n[CONTEXT]\n<<<FILE examples/ai69/optim.py>>>\nimport torch\nopt = torch.optim.Adam([], lr=3e-4)\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai69/optim.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["PagedAdamW8bit"]}, "code": "from transformers import get_scheduler\nfrom bitsandbytes.optim import PagedAdamW8bit\nopt = PagedAdamW8bit([], lr=2e-4)\nsched = get_scheduler(\"cosine\", optimizer=opt, num_warmup_steps=100, num_training_steps=1000)\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nLoRA target_modules 확장\n\n[CONTEXT]\n<<<FILE examples/ai573/lora_cfg.py>>>\nfrom peft import LoraConfig\n\nLC = LoraConfig(r=8, target_modules=[\n    \"q_proj\", \"v_proj\"  # BUG: 부족\n])\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai573/lora_cfg.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["k_proj", "o_proj"]}, "code": "from peft import LoraConfig\n\nLC = LoraConfig(r=8, target_modules=[\n    \"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"\n])\n"}]}}
{"instruction": "Propose a minimal JSON plan for code changes. Output PLAN JSON only.", "input": "[INTENT]\n학습 안정화(AMP+훈련 설정)\n\n[CONTEXT]\n<<<FILE examples/ai644/train_utils.py>>>\ndef get_dtype():\n    return torch.float16\n\n<<<END>>>\n<<<FILE examples/ai644/training_args.py>>>\nargs = TrainingArguments(output_dir='out', per_device_train_batch_size=1, num_train_epochs=1)\n\n<<<END>>>", "output": {"files": [{"path": "examples/ai644/train_utils.py", "reason": "dtype가 고정 float16", "strategy": "regex", "tests": ["bf16 또는 fp16 반환"]}, {"path": "examples/ai644/training_args.py", "reason": "로깅/평가/GA 설정 누락", "strategy": "regex", "tests": ["eval_strategy=='steps'"]}], "notes": "bf16 지원 시 우선"}}
{"instruction": "Propose a minimal JSON plan for code changes. Output PLAN JSON only.", "input": "[INTENT]\n입출력/런타임 안전성 개선\n\n[CONTEXT]\n<<<FILE examples/ai652/dataloader_main.py>>>\nloader = DataLoader(...); for b in loader: pass\n\n<<<END>>>\n<<<FILE examples/ai652/ddp_main.py>>>\ntrainer()\n\n<<<END>>>", "output": {"files": [{"path": "examples/ai652/dataloader_main.py", "reason": "Windows freeze 가능", "strategy": "regex", "tests": ["__main__ 가드"]}, {"path": "examples/ai652/ddp_main.py", "reason": "DDP 가드 없음", "strategy": "regex", "tests": ["__main__"]}], "notes": "spawn 보호"}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nGPU bf16 지원 시 dtype 선택 개선\n\n[CONTEXT]\n<<<FILE examples/ai635/train_utils.py>>>\nimport torch\n\n# BUG: dtype가 고정되어 혼합정밀에 불리\ndef get_dtype():\n    return torch.float16\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai635/train_utils.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["torch.bfloat16"]}, "code": "import torch\n\n\ndef get_dtype():\n    \"\"\"GPU가 bf16 지원 시 bfloat16, 아니면 float16 반환\"\"\"\n    if torch.cuda.is_available() and torch.cuda.is_bf16_supported():\n        return torch.bfloat16\n    return torch.float16\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n필수 import 추가\n\n[CONTEXT]\n<<<FILE examples/ai609/imports.py>>>\n# BUG: numpy 미사용이지만 추후 사용 예정\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai609/imports.py", "loc": {"type": "anchor", "before": "\\A"}, "action": "insert_after", "once": true, "pre": {"must_not_contain": ["import numpy as np"]}, "code": "import numpy as np\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\npad 토큰 ID 정확히 설정\n\n[CONTEXT]\n<<<FILE examples/ai191/label_cfg.py>>>\nfrom transformers import AutoTokenizer\nfrom dataclasses import dataclass\n\n@dataclass\nclass Cfg:\n    pad_id: int = -100  # BUG\n\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai191/label_cfg.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["CFG = Cfg"]}, "code": "from transformers import AutoTokenizer\nfrom dataclasses import dataclass\n\n@dataclass\nclass Cfg:\n    pad_id: int | None = None\n\n# tokenizer 로드 후 설정\ntok = AutoTokenizer.from_pretrained(\"Qwen/Qwen3-8B\", use_fast=True)\nCFG = Cfg(pad_id=tok.pad_token_id)\n"}]}}
{"instruction": "Propose a minimal JSON plan for code changes. Output PLAN JSON only.", "input": "[INTENT]\n학습 안정화(AMP+훈련 설정)\n\n[CONTEXT]\n<<<FILE examples/ai641/train_utils.py>>>\ndef get_dtype():\n    return torch.float16\n\n<<<END>>>\n<<<FILE examples/ai641/training_args.py>>>\nargs = TrainingArguments(output_dir='out', per_device_train_batch_size=1, num_train_epochs=1)\n\n<<<END>>>", "output": {"files": [{"path": "examples/ai641/train_utils.py", "reason": "dtype가 고정 float16", "strategy": "regex", "tests": ["bf16 또는 fp16 반환"]}, {"path": "examples/ai641/training_args.py", "reason": "로깅/평가/GA 설정 누락", "strategy": "regex", "tests": ["eval_strategy=='steps'"]}], "notes": "bf16 지원 시 우선"}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n필수 import 추가\n\n[CONTEXT]\n<<<FILE examples/ai543/imports.py>>>\n# BUG: numpy 미사용이지만 추후 사용 예정\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai543/imports.py", "loc": {"type": "anchor", "before": "\\A"}, "action": "insert_after", "once": true, "pre": {"must_not_contain": ["import numpy as np"]}, "code": "import numpy as np\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n체크포인트 재개 지원\n\n[CONTEXT]\n<<<FILE examples/ai632/trainer_resume.py>>>\nfrom transformers import Trainer\ntrainer = Trainer(... )\n# BUG: 재시작 미지원\ntrainer.train()\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai632/trainer_resume.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["resume_from_checkpoint"]}, "code": "from transformers import Trainer\ntrainer = Trainer(... )\ntrainer.train(resume_from_checkpoint=True)\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n재현성: 시드 함수 추가\n\n[CONTEXT]\n<<<FILE examples/ai637/seed_utils.py>>>\n# BUG: 시드 미설정\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai637/seed_utils.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["set_seed("]}, "code": "import os, random, numpy as np, torch\n\ndef set_seed(seed: int = 42):\n    random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n훈련 안정화: GA steps/로깅/평가/체크포인팅/스케줄러\n\n[CONTEXT]\n<<<FILE examples/ai408/training_args.py>>>\nfrom transformers import TrainingArguments\n\nargs = TrainingArguments(\n    output_dir=\"out\",\n    per_device_train_batch_size=1,\n    num_train_epochs=1,\n)\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai408/training_args.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["gradient_accumulation_steps"]}, "code": "from transformers import TrainingArguments\n\nargs = TrainingArguments(\n    output_dir=\"out\",\n    per_device_train_batch_size=1,\n    gradient_accumulation_steps=8,\n    num_train_epochs=1,\n    logging_steps=10,\n    eval_strategy=\"steps\",\n    eval_steps=50,\n    save_steps=50,\n    lr_scheduler_type=\"cosine\",\n    warmup_ratio=0.03,\n    fp16=False, bf16=True,\n    gradient_checkpointing=True,\n    max_grad_norm=1.0,\n)\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n체크포인트 재개 지원\n\n[CONTEXT]\n<<<FILE examples/ai448/trainer_resume.py>>>\nfrom transformers import Trainer\ntrainer = Trainer(... )\n# BUG: 재시작 미지원\ntrainer.train()\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai448/trainer_resume.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["resume_from_checkpoint"]}, "code": "from transformers import Trainer\ntrainer = Trainer(... )\ntrainer.train(resume_from_checkpoint=True)\n"}]}}
{"instruction": "Propose a minimal JSON plan for code changes. Output PLAN JSON only.", "input": "[INTENT]\n입출력/런타임 안전성 개선\n\n[CONTEXT]\n<<<FILE examples/ai687/dataloader_main.py>>>\nloader = DataLoader(...); for b in loader: pass\n\n<<<END>>>\n<<<FILE examples/ai687/ddp_main.py>>>\ntrainer()\n\n<<<END>>>", "output": {"files": [{"path": "examples/ai687/dataloader_main.py", "reason": "Windows freeze 가능", "strategy": "regex", "tests": ["__main__ 가드"]}, {"path": "examples/ai687/ddp_main.py", "reason": "DDP 가드 없음", "strategy": "regex", "tests": ["__main__"]}], "notes": "spawn 보호"}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\npad 토큰 ID 정확히 설정\n\n[CONTEXT]\n<<<FILE examples/ai215/label_cfg.py>>>\nfrom transformers import AutoTokenizer\nfrom dataclasses import dataclass\n\n@dataclass\nclass Cfg:\n    pad_id: int = -100  # BUG\n\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai215/label_cfg.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["CFG = Cfg"]}, "code": "from transformers import AutoTokenizer\nfrom dataclasses import dataclass\n\n@dataclass\nclass Cfg:\n    pad_id: int | None = None\n\n# tokenizer 로드 후 설정\ntok = AutoTokenizer.from_pretrained(\"Qwen/Qwen3-8B\", use_fast=True)\nCFG = Cfg(pad_id=tok.pad_token_id)\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n필수 import 추가\n\n[CONTEXT]\n<<<FILE examples/ai412/imports.py>>>\n# BUG: numpy 미사용이지만 추후 사용 예정\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai412/imports.py", "loc": {"type": "anchor", "before": "\\A"}, "action": "insert_after", "once": true, "pre": {"must_not_contain": ["import numpy as np"]}, "code": "import numpy as np\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nAMP 적용 + bf16 우선\n\n[CONTEXT]\n<<<FILE examples/ai220/train_step.py>>>\nimport torch\n\n# BUG: autocast 미사용\ndef train_step(model, batch):\n    x, y = batch\n    out = model(x)\n    loss = out.loss if hasattr(out, 'loss') else out.mean()\n    loss.backward()\n    return loss.item()\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai220/train_step.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["autocast"]}, "code": "import torch\n\n@torch.no_grad()\ndef _bf16_supported():\n    return torch.cuda.is_available() and torch.cuda.is_bf16_supported()\n\n\ndef train_step(model, batch):\n    x, y = batch\n    amp_dtype = torch.bfloat16 if _bf16_supported() else torch.float16\n    scaler = torch.cuda.amp.GradScaler(enabled=True)\n    with torch.cuda.amp.autocast(enabled=True, dtype=amp_dtype):\n        out = model(x)\n        loss = out.loss if hasattr(out, 'loss') else out.mean()\n    scaler.scale(loss).backward()\n    scaler.step(torch.optim.Adam(model.parameters(), lr=1e-4))\n    scaler.update()\n    model.zero_grad(set_to_none=True)\n    return float(loss.detach().item())\n"}]}}
{"instruction": "Propose a minimal JSON plan for code changes. Output PLAN JSON only.", "input": "[INTENT]\n입출력/런타임 안전성 개선\n\n[CONTEXT]\n<<<FILE examples/ai726/dataloader_main.py>>>\nloader = DataLoader(...); for b in loader: pass\n\n<<<END>>>\n<<<FILE examples/ai726/ddp_main.py>>>\ntrainer()\n\n<<<END>>>", "output": {"files": [{"path": "examples/ai726/dataloader_main.py", "reason": "Windows freeze 가능", "strategy": "regex", "tests": ["__main__ 가드"]}, {"path": "examples/ai726/ddp_main.py", "reason": "DDP 가드 없음", "strategy": "regex", "tests": ["__main__"]}], "notes": "spawn 보호"}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n체크포인트 재개 지원\n\n[CONTEXT]\n<<<FILE examples/ai426/trainer_resume.py>>>\nfrom transformers import Trainer\ntrainer = Trainer(... )\n# BUG: 재시작 미지원\ntrainer.train()\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai426/trainer_resume.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["resume_from_checkpoint"]}, "code": "from transformers import Trainer\ntrainer = Trainer(... )\ntrainer.train(resume_from_checkpoint=True)\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nDDP/torchrun 안전 가드\n\n[CONTEXT]\n<<<FILE examples/ai219/ddp_main.py>>>\nimport torch\n\n# BUG: torchrun에서 main 가드 없음\ntrainer()\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai219/ddp_main.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["__main__"]}, "code": "import torch\n\n\ndef main():\n    trainer()\n\n\nif __name__ == \"__main__\":\n    main()\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\npad 토큰 ID 정확히 설정\n\n[CONTEXT]\n<<<FILE examples/ai135/label_cfg.py>>>\nfrom transformers import AutoTokenizer\nfrom dataclasses import dataclass\n\n@dataclass\nclass Cfg:\n    pad_id: int = -100  # BUG\n\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai135/label_cfg.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["CFG = Cfg"]}, "code": "from transformers import AutoTokenizer\nfrom dataclasses import dataclass\n\n@dataclass\nclass Cfg:\n    pad_id: int | None = None\n\n# tokenizer 로드 후 설정\ntok = AutoTokenizer.from_pretrained(\"Qwen/Qwen3-8B\", use_fast=True)\nCFG = Cfg(pad_id=tok.pad_token_id)\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nWindows에서 DataLoader 프리즈 방지 + pin_memory/persistent_workers\n\n[CONTEXT]\n<<<FILE examples/ai205/dataloader_main.py>>>\nimport torch, torch.utils.data as tud\n\nclass Dummy(torch.utils.data.Dataset):\n    def __len__(self): return 8\n    def __getitem__(self, i): return i\n\n# BUG: 모듈 임포트만으로 워커가 스폰되어 Windows에서 프리즈\nloader = tud.DataLoader(Dummy(), batch_size=2, num_workers=4)\nfor b in loader:\n    pass\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai205/dataloader_main.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["if __name__ == \"__main__\":"]}, "code": "import torch, torch.utils.data as tud\n\nclass Dummy(torch.utils.data.Dataset):\n    def __len__(self): return 8\n    def __getitem__(self, i): return i\n\n\ndef main():\n    loader = tud.DataLoader(Dummy(), batch_size=2, num_workers=4, pin_memory=True, persistent_workers=True)\n    for _ in loader:\n        pass\n\n\nif __name__ == \"__main__\":\n    main()\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nflash 미지원 시 sdpa 안전 경로\n\n[CONTEXT]\n<<<FILE examples/ai618/model_load.py>>>\nfrom transformers import AutoModelForCausalLM\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"Qwen/Qwen3-8B\",\n)\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai618/model_load.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["attn_implementation"]}, "code": "from transformers import AutoModelForCausalLM\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"Qwen/Qwen3-8B\",\n    attn_implementation=\"sdpa\",\n)\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n필수 import 추가\n\n[CONTEXT]\n<<<FILE examples/ai473/imports.py>>>\n# BUG: numpy 미사용이지만 추후 사용 예정\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai473/imports.py", "loc": {"type": "anchor", "before": "\\A"}, "action": "insert_after", "once": true, "pre": {"must_not_contain": ["import numpy as np"]}, "code": "import numpy as np\n"}]}}
{"instruction": "Propose a minimal JSON plan for code changes. Output PLAN JSON only.", "input": "[INTENT]\n입출력/런타임 안전성 개선\n\n[CONTEXT]\n<<<FILE examples/ai746/dataloader_main.py>>>\nloader = DataLoader(...); for b in loader: pass\n\n<<<END>>>\n<<<FILE examples/ai746/ddp_main.py>>>\ntrainer()\n\n<<<END>>>", "output": {"files": [{"path": "examples/ai746/dataloader_main.py", "reason": "Windows freeze 가능", "strategy": "regex", "tests": ["__main__ 가드"]}, {"path": "examples/ai746/ddp_main.py", "reason": "DDP 가드 없음", "strategy": "regex", "tests": ["__main__"]}], "notes": "spawn 보호"}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nflash 미지원 시 sdpa 안전 경로\n\n[CONTEXT]\n<<<FILE examples/ai461/model_load.py>>>\nfrom transformers import AutoModelForCausalLM\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"Qwen/Qwen3-8B\",\n)\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai461/model_load.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["attn_implementation"]}, "code": "from transformers import AutoModelForCausalLM\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"Qwen/Qwen3-8B\",\n    attn_implementation=\"sdpa\",\n)\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nflash 미지원 시 sdpa 안전 경로\n\n[CONTEXT]\n<<<FILE examples/ai634/model_load.py>>>\nfrom transformers import AutoModelForCausalLM\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"Qwen/Qwen3-8B\",\n)\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai634/model_load.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["attn_implementation"]}, "code": "from transformers import AutoModelForCausalLM\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"Qwen/Qwen3-8B\",\n    attn_implementation=\"sdpa\",\n)\n"}]}}
{"instruction": "Propose a minimal JSON plan for code changes. Output PLAN JSON only.", "input": "[INTENT]\n학습 안정화(AMP+훈련 설정)\n\n[CONTEXT]\n<<<FILE examples/ai689/train_utils.py>>>\ndef get_dtype():\n    return torch.float16\n\n<<<END>>>\n<<<FILE examples/ai689/training_args.py>>>\nargs = TrainingArguments(output_dir='out', per_device_train_batch_size=1, num_train_epochs=1)\n\n<<<END>>>", "output": {"files": [{"path": "examples/ai689/train_utils.py", "reason": "dtype가 고정 float16", "strategy": "regex", "tests": ["bf16 또는 fp16 반환"]}, {"path": "examples/ai689/training_args.py", "reason": "로깅/평가/GA 설정 누락", "strategy": "regex", "tests": ["eval_strategy=='steps'"]}], "notes": "bf16 지원 시 우선"}}
{"instruction": "Propose a minimal JSON plan for code changes. Output PLAN JSON only.", "input": "[INTENT]\n입출력/런타임 안전성 개선\n\n[CONTEXT]\n<<<FILE examples/ai783/dataloader_main.py>>>\nloader = DataLoader(...); for b in loader: pass\n\n<<<END>>>\n<<<FILE examples/ai783/ddp_main.py>>>\ntrainer()\n\n<<<END>>>", "output": {"files": [{"path": "examples/ai783/dataloader_main.py", "reason": "Windows freeze 가능", "strategy": "regex", "tests": ["__main__ 가드"]}, {"path": "examples/ai783/ddp_main.py", "reason": "DDP 가드 없음", "strategy": "regex", "tests": ["__main__"]}], "notes": "spawn 보호"}}
{"instruction": "Propose a minimal JSON plan for code changes. Output PLAN JSON only.", "input": "[INTENT]\n입출력/런타임 안전성 개선\n\n[CONTEXT]\n<<<FILE examples/ai760/dataloader_main.py>>>\nloader = DataLoader(...); for b in loader: pass\n\n<<<END>>>\n<<<FILE examples/ai760/ddp_main.py>>>\ntrainer()\n\n<<<END>>>", "output": {"files": [{"path": "examples/ai760/dataloader_main.py", "reason": "Windows freeze 가능", "strategy": "regex", "tests": ["__main__ 가드"]}, {"path": "examples/ai760/ddp_main.py", "reason": "DDP 가드 없음", "strategy": "regex", "tests": ["__main__"]}], "notes": "spawn 보호"}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nflash 미지원 시 sdpa 안전 경로\n\n[CONTEXT]\n<<<FILE examples/ai193/model_load.py>>>\nfrom transformers import AutoModelForCausalLM\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"Qwen/Qwen3-8B\",\n)\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai193/model_load.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["attn_implementation"]}, "code": "from transformers import AutoModelForCausalLM\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"Qwen/Qwen3-8B\",\n    attn_implementation=\"sdpa\",\n)\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nLoRA target_modules 확장\n\n[CONTEXT]\n<<<FILE examples/ai435/lora_cfg.py>>>\nfrom peft import LoraConfig\n\nLC = LoraConfig(r=8, target_modules=[\n    \"q_proj\", \"v_proj\"  # BUG: 부족\n])\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai435/lora_cfg.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["k_proj", "o_proj"]}, "code": "from peft import LoraConfig\n\nLC = LoraConfig(r=8, target_modules=[\n    \"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"\n])\n"}]}}
{"instruction": "Propose a minimal JSON plan for code changes. Output PLAN JSON only.", "input": "[INTENT]\n입출력/런타임 안전성 개선\n\n[CONTEXT]\n<<<FILE examples/ai704/dataloader_main.py>>>\nloader = DataLoader(...); for b in loader: pass\n\n<<<END>>>\n<<<FILE examples/ai704/ddp_main.py>>>\ntrainer()\n\n<<<END>>>", "output": {"files": [{"path": "examples/ai704/dataloader_main.py", "reason": "Windows freeze 가능", "strategy": "regex", "tests": ["__main__ 가드"]}, {"path": "examples/ai704/ddp_main.py", "reason": "DDP 가드 없음", "strategy": "regex", "tests": ["__main__"]}], "notes": "spawn 보호"}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n훈련 안정화: GA steps/로깅/평가/체크포인팅/스케줄러\n\n[CONTEXT]\n<<<FILE examples/ai244/training_args.py>>>\nfrom transformers import TrainingArguments\n\nargs = TrainingArguments(\n    output_dir=\"out\",\n    per_device_train_batch_size=1,\n    num_train_epochs=1,\n)\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai244/training_args.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["gradient_accumulation_steps"]}, "code": "from transformers import TrainingArguments\n\nargs = TrainingArguments(\n    output_dir=\"out\",\n    per_device_train_batch_size=1,\n    gradient_accumulation_steps=8,\n    num_train_epochs=1,\n    logging_steps=10,\n    eval_strategy=\"steps\",\n    eval_steps=50,\n    save_steps=50,\n    lr_scheduler_type=\"cosine\",\n    warmup_ratio=0.03,\n    fp16=False, bf16=True,\n    gradient_checkpointing=True,\n    max_grad_norm=1.0,\n)\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nWindows에서 DataLoader 프리즈 방지 + pin_memory/persistent_workers\n\n[CONTEXT]\n<<<FILE examples/ai478/dataloader_main.py>>>\nimport torch, torch.utils.data as tud\n\nclass Dummy(torch.utils.data.Dataset):\n    def __len__(self): return 8\n    def __getitem__(self, i): return i\n\n# BUG: 모듈 임포트만으로 워커가 스폰되어 Windows에서 프리즈\nloader = tud.DataLoader(Dummy(), batch_size=2, num_workers=4)\nfor b in loader:\n    pass\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai478/dataloader_main.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["if __name__ == \"__main__\":"]}, "code": "import torch, torch.utils.data as tud\n\nclass Dummy(torch.utils.data.Dataset):\n    def __len__(self): return 8\n    def __getitem__(self, i): return i\n\n\ndef main():\n    loader = tud.DataLoader(Dummy(), batch_size=2, num_workers=4, pin_memory=True, persistent_workers=True)\n    for _ in loader:\n        pass\n\n\nif __name__ == \"__main__\":\n    main()\n"}]}}
{"instruction": "Propose a minimal JSON plan for code changes. Output PLAN JSON only.", "input": "[INTENT]\n학습 안정화(AMP+훈련 설정)\n\n[CONTEXT]\n<<<FILE examples/ai788/train_utils.py>>>\ndef get_dtype():\n    return torch.float16\n\n<<<END>>>\n<<<FILE examples/ai788/training_args.py>>>\nargs = TrainingArguments(output_dir='out', per_device_train_batch_size=1, num_train_epochs=1)\n\n<<<END>>>", "output": {"files": [{"path": "examples/ai788/train_utils.py", "reason": "dtype가 고정 float16", "strategy": "regex", "tests": ["bf16 또는 fp16 반환"]}, {"path": "examples/ai788/training_args.py", "reason": "로깅/평가/GA 설정 누락", "strategy": "regex", "tests": ["eval_strategy=='steps'"]}], "notes": "bf16 지원 시 우선"}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n체크포인트 재개 지원\n\n[CONTEXT]\n<<<FILE examples/ai104/trainer_resume.py>>>\nfrom transformers import Trainer\ntrainer = Trainer(... )\n# BUG: 재시작 미지원\ntrainer.train()\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai104/trainer_resume.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["resume_from_checkpoint"]}, "code": "from transformers import Trainer\ntrainer = Trainer(... )\ntrainer.train(resume_from_checkpoint=True)\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n학습 후 어댑터 저장 추가\n\n[CONTEXT]\n<<<FILE examples/ai267/save_after_train.py>>>\nfrom peft import PeftModel\n# ... 학습 후\n# BUG: 저장 누락\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai267/save_after_train.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["save_pretrained"]}, "code": "from peft import PeftModel\n# ... 학습 후\nmodel.save_pretrained(\"training/qlora-out/export\")\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nWindows에서 DataLoader 프리즈 방지 + pin_memory/persistent_workers\n\n[CONTEXT]\n<<<FILE examples/ai344/dataloader_main.py>>>\nimport torch, torch.utils.data as tud\n\nclass Dummy(torch.utils.data.Dataset):\n    def __len__(self): return 8\n    def __getitem__(self, i): return i\n\n# BUG: 모듈 임포트만으로 워커가 스폰되어 Windows에서 프리즈\nloader = tud.DataLoader(Dummy(), batch_size=2, num_workers=4)\nfor b in loader:\n    pass\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai344/dataloader_main.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["if __name__ == \"__main__\":"]}, "code": "import torch, torch.utils.data as tud\n\nclass Dummy(torch.utils.data.Dataset):\n    def __len__(self): return 8\n    def __getitem__(self, i): return i\n\n\ndef main():\n    loader = tud.DataLoader(Dummy(), batch_size=2, num_workers=4, pin_memory=True, persistent_workers=True)\n    for _ in loader:\n        pass\n\n\nif __name__ == \"__main__\":\n    main()\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\npad 토큰 ID 정확히 설정\n\n[CONTEXT]\n<<<FILE examples/ai565/label_cfg.py>>>\nfrom transformers import AutoTokenizer\nfrom dataclasses import dataclass\n\n@dataclass\nclass Cfg:\n    pad_id: int = -100  # BUG\n\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai565/label_cfg.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["CFG = Cfg"]}, "code": "from transformers import AutoTokenizer\nfrom dataclasses import dataclass\n\n@dataclass\nclass Cfg:\n    pad_id: int | None = None\n\n# tokenizer 로드 후 설정\ntok = AutoTokenizer.from_pretrained(\"Qwen/Qwen3-8B\", use_fast=True)\nCFG = Cfg(pad_id=tok.pad_token_id)\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n조기 종료 콜백 추가\n\n[CONTEXT]\n<<<FILE examples/ai379/callbacks.py>>>\nfrom transformers import TrainerCallback\n\nclass MyCB(TrainerCallback):\n    pass\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai379/callbacks.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["EarlyStoppingCallback"]}, "code": "from transformers import TrainerCallback, EarlyStoppingCallback\n\nclass MyCB(TrainerCallback):\n    pass\n\nCALLBACKS = [EarlyStoppingCallback(early_stopping_patience=2)]\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n훈련 안정화: GA steps/로깅/평가/체크포인팅/스케줄러\n\n[CONTEXT]\n<<<FILE examples/ai560/training_args.py>>>\nfrom transformers import TrainingArguments\n\nargs = TrainingArguments(\n    output_dir=\"out\",\n    per_device_train_batch_size=1,\n    num_train_epochs=1,\n)\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai560/training_args.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["gradient_accumulation_steps"]}, "code": "from transformers import TrainingArguments\n\nargs = TrainingArguments(\n    output_dir=\"out\",\n    per_device_train_batch_size=1,\n    gradient_accumulation_steps=8,\n    num_train_epochs=1,\n    logging_steps=10,\n    eval_strategy=\"steps\",\n    eval_steps=50,\n    save_steps=50,\n    lr_scheduler_type=\"cosine\",\n    warmup_ratio=0.03,\n    fp16=False, bf16=True,\n    gradient_checkpointing=True,\n    max_grad_norm=1.0,\n)\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n재현성: 시드 함수 추가\n\n[CONTEXT]\n<<<FILE examples/ai551/seed_utils.py>>>\n# BUG: 시드 미설정\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai551/seed_utils.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["set_seed("]}, "code": "import os, random, numpy as np, torch\n\ndef set_seed(seed: int = 42):\n    random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n8bit 옵티마이저 + 코사인 스케줄러\n\n[CONTEXT]\n<<<FILE examples/ai464/optim.py>>>\nimport torch\nopt = torch.optim.Adam([], lr=3e-4)\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai464/optim.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["PagedAdamW8bit"]}, "code": "from transformers import get_scheduler\nfrom bitsandbytes.optim import PagedAdamW8bit\nopt = PagedAdamW8bit([], lr=2e-4)\nsched = get_scheduler(\"cosine\", optimizer=opt, num_warmup_steps=100, num_training_steps=1000)\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nDDP/torchrun 안전 가드\n\n[CONTEXT]\n<<<FILE examples/ai373/ddp_main.py>>>\nimport torch\n\n# BUG: torchrun에서 main 가드 없음\ntrainer()\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai373/ddp_main.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["__main__"]}, "code": "import torch\n\n\ndef main():\n    trainer()\n\n\nif __name__ == \"__main__\":\n    main()\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n조기 종료 콜백 추가\n\n[CONTEXT]\n<<<FILE examples/ai624/callbacks.py>>>\nfrom transformers import TrainerCallback\n\nclass MyCB(TrainerCallback):\n    pass\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai624/callbacks.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["EarlyStoppingCallback"]}, "code": "from transformers import TrainerCallback, EarlyStoppingCallback\n\nclass MyCB(TrainerCallback):\n    pass\n\nCALLBACKS = [EarlyStoppingCallback(early_stopping_patience=2)]\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n재현성: 시드 함수 추가\n\n[CONTEXT]\n<<<FILE examples/ai308/seed_utils.py>>>\n# BUG: 시드 미설정\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai308/seed_utils.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["set_seed("]}, "code": "import os, random, numpy as np, torch\n\ndef set_seed(seed: int = 42):\n    random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nLoRA target_modules 확장\n\n[CONTEXT]\n<<<FILE examples/ai359/lora_cfg.py>>>\nfrom peft import LoraConfig\n\nLC = LoraConfig(r=8, target_modules=[\n    \"q_proj\", \"v_proj\"  # BUG: 부족\n])\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai359/lora_cfg.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["k_proj", "o_proj"]}, "code": "from peft import LoraConfig\n\nLC = LoraConfig(r=8, target_modules=[\n    \"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"\n])\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n필수 import 추가\n\n[CONTEXT]\n<<<FILE examples/ai182/imports.py>>>\n# BUG: numpy 미사용이지만 추후 사용 예정\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai182/imports.py", "loc": {"type": "anchor", "before": "\\A"}, "action": "insert_after", "once": true, "pre": {"must_not_contain": ["import numpy as np"]}, "code": "import numpy as np\n"}]}}
{"instruction": "Propose a minimal JSON plan for code changes. Output PLAN JSON only.", "input": "[INTENT]\n입출력/런타임 안전성 개선\n\n[CONTEXT]\n<<<FILE examples/ai692/dataloader_main.py>>>\nloader = DataLoader(...); for b in loader: pass\n\n<<<END>>>\n<<<FILE examples/ai692/ddp_main.py>>>\ntrainer()\n\n<<<END>>>", "output": {"files": [{"path": "examples/ai692/dataloader_main.py", "reason": "Windows freeze 가능", "strategy": "regex", "tests": ["__main__ 가드"]}, {"path": "examples/ai692/ddp_main.py", "reason": "DDP 가드 없음", "strategy": "regex", "tests": ["__main__"]}], "notes": "spawn 보호"}}
{"instruction": "Propose a minimal JSON plan for code changes. Output PLAN JSON only.", "input": "[INTENT]\n입출력/런타임 안전성 개선\n\n[CONTEXT]\n<<<FILE examples/ai775/dataloader_main.py>>>\nloader = DataLoader(...); for b in loader: pass\n\n<<<END>>>\n<<<FILE examples/ai775/ddp_main.py>>>\ntrainer()\n\n<<<END>>>", "output": {"files": [{"path": "examples/ai775/dataloader_main.py", "reason": "Windows freeze 가능", "strategy": "regex", "tests": ["__main__ 가드"]}, {"path": "examples/ai775/ddp_main.py", "reason": "DDP 가드 없음", "strategy": "regex", "tests": ["__main__"]}], "notes": "spawn 보호"}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n체크포인트 재개 지원\n\n[CONTEXT]\n<<<FILE examples/ai133/trainer_resume.py>>>\nfrom transformers import Trainer\ntrainer = Trainer(... )\n# BUG: 재시작 미지원\ntrainer.train()\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai133/trainer_resume.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["resume_from_checkpoint"]}, "code": "from transformers import Trainer\ntrainer = Trainer(... )\ntrainer.train(resume_from_checkpoint=True)\n"}]}}
{"instruction": "Propose a minimal JSON plan for code changes. Output PLAN JSON only.", "input": "[INTENT]\n입출력/런타임 안전성 개선\n\n[CONTEXT]\n<<<FILE examples/ai748/dataloader_main.py>>>\nloader = DataLoader(...); for b in loader: pass\n\n<<<END>>>\n<<<FILE examples/ai748/ddp_main.py>>>\ntrainer()\n\n<<<END>>>", "output": {"files": [{"path": "examples/ai748/dataloader_main.py", "reason": "Windows freeze 가능", "strategy": "regex", "tests": ["__main__ 가드"]}, {"path": "examples/ai748/ddp_main.py", "reason": "DDP 가드 없음", "strategy": "regex", "tests": ["__main__"]}], "notes": "spawn 보호"}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nWindows에서 DataLoader 프리즈 방지 + pin_memory/persistent_workers\n\n[CONTEXT]\n<<<FILE examples/ai14/dataloader_main.py>>>\nimport torch, torch.utils.data as tud\n\nclass Dummy(torch.utils.data.Dataset):\n    def __len__(self): return 8\n    def __getitem__(self, i): return i\n\n# BUG: 모듈 임포트만으로 워커가 스폰되어 Windows에서 프리즈\nloader = tud.DataLoader(Dummy(), batch_size=2, num_workers=4)\nfor b in loader:\n    pass\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai14/dataloader_main.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["if __name__ == \"__main__\":"]}, "code": "import torch, torch.utils.data as tud\n\nclass Dummy(torch.utils.data.Dataset):\n    def __len__(self): return 8\n    def __getitem__(self, i): return i\n\n\ndef main():\n    loader = tud.DataLoader(Dummy(), batch_size=2, num_workers=4, pin_memory=True, persistent_workers=True)\n    for _ in loader:\n        pass\n\n\nif __name__ == \"__main__\":\n    main()\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nWindows에서 DataLoader 프리즈 방지 + pin_memory/persistent_workers\n\n[CONTEXT]\n<<<FILE examples/ai406/dataloader_main.py>>>\nimport torch, torch.utils.data as tud\n\nclass Dummy(torch.utils.data.Dataset):\n    def __len__(self): return 8\n    def __getitem__(self, i): return i\n\n# BUG: 모듈 임포트만으로 워커가 스폰되어 Windows에서 프리즈\nloader = tud.DataLoader(Dummy(), batch_size=2, num_workers=4)\nfor b in loader:\n    pass\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai406/dataloader_main.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["if __name__ == \"__main__\":"]}, "code": "import torch, torch.utils.data as tud\n\nclass Dummy(torch.utils.data.Dataset):\n    def __len__(self): return 8\n    def __getitem__(self, i): return i\n\n\ndef main():\n    loader = tud.DataLoader(Dummy(), batch_size=2, num_workers=4, pin_memory=True, persistent_workers=True)\n    for _ in loader:\n        pass\n\n\nif __name__ == \"__main__\":\n    main()\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nWindows에서 DataLoader 프리즈 방지 + pin_memory/persistent_workers\n\n[CONTEXT]\n<<<FILE examples/ai66/dataloader_main.py>>>\nimport torch, torch.utils.data as tud\n\nclass Dummy(torch.utils.data.Dataset):\n    def __len__(self): return 8\n    def __getitem__(self, i): return i\n\n# BUG: 모듈 임포트만으로 워커가 스폰되어 Windows에서 프리즈\nloader = tud.DataLoader(Dummy(), batch_size=2, num_workers=4)\nfor b in loader:\n    pass\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai66/dataloader_main.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["if __name__ == \"__main__\":"]}, "code": "import torch, torch.utils.data as tud\n\nclass Dummy(torch.utils.data.Dataset):\n    def __len__(self): return 8\n    def __getitem__(self, i): return i\n\n\ndef main():\n    loader = tud.DataLoader(Dummy(), batch_size=2, num_workers=4, pin_memory=True, persistent_workers=True)\n    for _ in loader:\n        pass\n\n\nif __name__ == \"__main__\":\n    main()\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nAMP 적용 + bf16 우선\n\n[CONTEXT]\n<<<FILE examples/ai268/train_step.py>>>\nimport torch\n\n# BUG: autocast 미사용\ndef train_step(model, batch):\n    x, y = batch\n    out = model(x)\n    loss = out.loss if hasattr(out, 'loss') else out.mean()\n    loss.backward()\n    return loss.item()\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai268/train_step.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["autocast"]}, "code": "import torch\n\n@torch.no_grad()\ndef _bf16_supported():\n    return torch.cuda.is_available() and torch.cuda.is_bf16_supported()\n\n\ndef train_step(model, batch):\n    x, y = batch\n    amp_dtype = torch.bfloat16 if _bf16_supported() else torch.float16\n    scaler = torch.cuda.amp.GradScaler(enabled=True)\n    with torch.cuda.amp.autocast(enabled=True, dtype=amp_dtype):\n        out = model(x)\n        loss = out.loss if hasattr(out, 'loss') else out.mean()\n    scaler.scale(loss).backward()\n    scaler.step(torch.optim.Adam(model.parameters(), lr=1e-4))\n    scaler.update()\n    model.zero_grad(set_to_none=True)\n    return float(loss.detach().item())\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nDDP/torchrun 안전 가드\n\n[CONTEXT]\n<<<FILE examples/ai576/ddp_main.py>>>\nimport torch\n\n# BUG: torchrun에서 main 가드 없음\ntrainer()\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai576/ddp_main.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["__main__"]}, "code": "import torch\n\n\ndef main():\n    trainer()\n\n\nif __name__ == \"__main__\":\n    main()\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nGPU bf16 지원 시 dtype 선택 개선\n\n[CONTEXT]\n<<<FILE examples/ai397/train_utils.py>>>\nimport torch\n\n# BUG: dtype가 고정되어 혼합정밀에 불리\ndef get_dtype():\n    return torch.float16\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai397/train_utils.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["torch.bfloat16"]}, "code": "import torch\n\n\ndef get_dtype():\n    \"\"\"GPU가 bf16 지원 시 bfloat16, 아니면 float16 반환\"\"\"\n    if torch.cuda.is_available() and torch.cuda.is_bf16_supported():\n        return torch.bfloat16\n    return torch.float16\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nWindows에서 DataLoader 프리즈 방지 + pin_memory/persistent_workers\n\n[CONTEXT]\n<<<FILE examples/ai409/dataloader_main.py>>>\nimport torch, torch.utils.data as tud\n\nclass Dummy(torch.utils.data.Dataset):\n    def __len__(self): return 8\n    def __getitem__(self, i): return i\n\n# BUG: 모듈 임포트만으로 워커가 스폰되어 Windows에서 프리즈\nloader = tud.DataLoader(Dummy(), batch_size=2, num_workers=4)\nfor b in loader:\n    pass\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai409/dataloader_main.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["if __name__ == \"__main__\":"]}, "code": "import torch, torch.utils.data as tud\n\nclass Dummy(torch.utils.data.Dataset):\n    def __len__(self): return 8\n    def __getitem__(self, i): return i\n\n\ndef main():\n    loader = tud.DataLoader(Dummy(), batch_size=2, num_workers=4, pin_memory=True, persistent_workers=True)\n    for _ in loader:\n        pass\n\n\nif __name__ == \"__main__\":\n    main()\n"}]}}
{"instruction": "Propose a minimal JSON plan for code changes. Output PLAN JSON only.", "input": "[INTENT]\n학습 안정화(AMP+훈련 설정)\n\n[CONTEXT]\n<<<FILE examples/ai777/train_utils.py>>>\ndef get_dtype():\n    return torch.float16\n\n<<<END>>>\n<<<FILE examples/ai777/training_args.py>>>\nargs = TrainingArguments(output_dir='out', per_device_train_batch_size=1, num_train_epochs=1)\n\n<<<END>>>", "output": {"files": [{"path": "examples/ai777/train_utils.py", "reason": "dtype가 고정 float16", "strategy": "regex", "tests": ["bf16 또는 fp16 반환"]}, {"path": "examples/ai777/training_args.py", "reason": "로깅/평가/GA 설정 누락", "strategy": "regex", "tests": ["eval_strategy=='steps'"]}], "notes": "bf16 지원 시 우선"}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nDDP/torchrun 안전 가드\n\n[CONTEXT]\n<<<FILE examples/ai216/ddp_main.py>>>\nimport torch\n\n# BUG: torchrun에서 main 가드 없음\ntrainer()\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai216/ddp_main.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["__main__"]}, "code": "import torch\n\n\ndef main():\n    trainer()\n\n\nif __name__ == \"__main__\":\n    main()\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nGPU bf16 지원 시 dtype 선택 개선\n\n[CONTEXT]\n<<<FILE examples/ai393/train_utils.py>>>\nimport torch\n\n# BUG: dtype가 고정되어 혼합정밀에 불리\ndef get_dtype():\n    return torch.float16\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai393/train_utils.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["torch.bfloat16"]}, "code": "import torch\n\n\ndef get_dtype():\n    \"\"\"GPU가 bf16 지원 시 bfloat16, 아니면 float16 반환\"\"\"\n    if torch.cuda.is_available() and torch.cuda.is_bf16_supported():\n        return torch.bfloat16\n    return torch.float16\n"}]}}
{"instruction": "Propose a minimal JSON plan for code changes. Output PLAN JSON only.", "input": "[INTENT]\n입출력/런타임 안전성 개선\n\n[CONTEXT]\n<<<FILE examples/ai735/dataloader_main.py>>>\nloader = DataLoader(...); for b in loader: pass\n\n<<<END>>>\n<<<FILE examples/ai735/ddp_main.py>>>\ntrainer()\n\n<<<END>>>", "output": {"files": [{"path": "examples/ai735/dataloader_main.py", "reason": "Windows freeze 가능", "strategy": "regex", "tests": ["__main__ 가드"]}, {"path": "examples/ai735/ddp_main.py", "reason": "DDP 가드 없음", "strategy": "regex", "tests": ["__main__"]}], "notes": "spawn 보호"}}
{"instruction": "Propose a minimal JSON plan for code changes. Output PLAN JSON only.", "input": "[INTENT]\n학습 안정화(AMP+훈련 설정)\n\n[CONTEXT]\n<<<FILE examples/ai679/train_utils.py>>>\ndef get_dtype():\n    return torch.float16\n\n<<<END>>>\n<<<FILE examples/ai679/training_args.py>>>\nargs = TrainingArguments(output_dir='out', per_device_train_batch_size=1, num_train_epochs=1)\n\n<<<END>>>", "output": {"files": [{"path": "examples/ai679/train_utils.py", "reason": "dtype가 고정 float16", "strategy": "regex", "tests": ["bf16 또는 fp16 반환"]}, {"path": "examples/ai679/training_args.py", "reason": "로깅/평가/GA 설정 누락", "strategy": "regex", "tests": ["eval_strategy=='steps'"]}], "notes": "bf16 지원 시 우선"}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nGPU bf16 지원 시 dtype 선택 개선\n\n[CONTEXT]\n<<<FILE examples/ai162/train_utils.py>>>\nimport torch\n\n# BUG: dtype가 고정되어 혼합정밀에 불리\ndef get_dtype():\n    return torch.float16\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai162/train_utils.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["torch.bfloat16"]}, "code": "import torch\n\n\ndef get_dtype():\n    \"\"\"GPU가 bf16 지원 시 bfloat16, 아니면 float16 반환\"\"\"\n    if torch.cuda.is_available() and torch.cuda.is_bf16_supported():\n        return torch.bfloat16\n    return torch.float16\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\npad 토큰 ID 정확히 설정\n\n[CONTEXT]\n<<<FILE examples/ai457/label_cfg.py>>>\nfrom transformers import AutoTokenizer\nfrom dataclasses import dataclass\n\n@dataclass\nclass Cfg:\n    pad_id: int = -100  # BUG\n\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai457/label_cfg.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["CFG = Cfg"]}, "code": "from transformers import AutoTokenizer\nfrom dataclasses import dataclass\n\n@dataclass\nclass Cfg:\n    pad_id: int | None = None\n\n# tokenizer 로드 후 설정\ntok = AutoTokenizer.from_pretrained(\"Qwen/Qwen3-8B\", use_fast=True)\nCFG = Cfg(pad_id=tok.pad_token_id)\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nAMP 적용 + bf16 우선\n\n[CONTEXT]\n<<<FILE examples/ai639/train_step.py>>>\nimport torch\n\n# BUG: autocast 미사용\ndef train_step(model, batch):\n    x, y = batch\n    out = model(x)\n    loss = out.loss if hasattr(out, 'loss') else out.mean()\n    loss.backward()\n    return loss.item()\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai639/train_step.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["autocast"]}, "code": "import torch\n\n@torch.no_grad()\ndef _bf16_supported():\n    return torch.cuda.is_available() and torch.cuda.is_bf16_supported()\n\n\ndef train_step(model, batch):\n    x, y = batch\n    amp_dtype = torch.bfloat16 if _bf16_supported() else torch.float16\n    scaler = torch.cuda.amp.GradScaler(enabled=True)\n    with torch.cuda.amp.autocast(enabled=True, dtype=amp_dtype):\n        out = model(x)\n        loss = out.loss if hasattr(out, 'loss') else out.mean()\n    scaler.scale(loss).backward()\n    scaler.step(torch.optim.Adam(model.parameters(), lr=1e-4))\n    scaler.update()\n    model.zero_grad(set_to_none=True)\n    return float(loss.detach().item())\n"}]}}
{"instruction": "Propose a minimal JSON plan for code changes. Output PLAN JSON only.", "input": "[INTENT]\n입출력/런타임 안전성 개선\n\n[CONTEXT]\n<<<FILE examples/ai737/dataloader_main.py>>>\nloader = DataLoader(...); for b in loader: pass\n\n<<<END>>>\n<<<FILE examples/ai737/ddp_main.py>>>\ntrainer()\n\n<<<END>>>", "output": {"files": [{"path": "examples/ai737/dataloader_main.py", "reason": "Windows freeze 가능", "strategy": "regex", "tests": ["__main__ 가드"]}, {"path": "examples/ai737/ddp_main.py", "reason": "DDP 가드 없음", "strategy": "regex", "tests": ["__main__"]}], "notes": "spawn 보호"}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nAMP 적용 + bf16 우선\n\n[CONTEXT]\n<<<FILE examples/ai400/train_step.py>>>\nimport torch\n\n# BUG: autocast 미사용\ndef train_step(model, batch):\n    x, y = batch\n    out = model(x)\n    loss = out.loss if hasattr(out, 'loss') else out.mean()\n    loss.backward()\n    return loss.item()\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai400/train_step.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["autocast"]}, "code": "import torch\n\n@torch.no_grad()\ndef _bf16_supported():\n    return torch.cuda.is_available() and torch.cuda.is_bf16_supported()\n\n\ndef train_step(model, batch):\n    x, y = batch\n    amp_dtype = torch.bfloat16 if _bf16_supported() else torch.float16\n    scaler = torch.cuda.amp.GradScaler(enabled=True)\n    with torch.cuda.amp.autocast(enabled=True, dtype=amp_dtype):\n        out = model(x)\n        loss = out.loss if hasattr(out, 'loss') else out.mean()\n    scaler.scale(loss).backward()\n    scaler.step(torch.optim.Adam(model.parameters(), lr=1e-4))\n    scaler.update()\n    model.zero_grad(set_to_none=True)\n    return float(loss.detach().item())\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n학습 후 어댑터 저장 추가\n\n[CONTEXT]\n<<<FILE examples/ai587/save_after_train.py>>>\nfrom peft import PeftModel\n# ... 학습 후\n# BUG: 저장 누락\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai587/save_after_train.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["save_pretrained"]}, "code": "from peft import PeftModel\n# ... 학습 후\nmodel.save_pretrained(\"training/qlora-out/export\")\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n8bit 옵티마이저 + 코사인 스케줄러\n\n[CONTEXT]\n<<<FILE examples/ai365/optim.py>>>\nimport torch\nopt = torch.optim.Adam([], lr=3e-4)\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai365/optim.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["PagedAdamW8bit"]}, "code": "from transformers import get_scheduler\nfrom bitsandbytes.optim import PagedAdamW8bit\nopt = PagedAdamW8bit([], lr=2e-4)\nsched = get_scheduler(\"cosine\", optimizer=opt, num_warmup_steps=100, num_training_steps=1000)\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n체크포인트 재개 지원\n\n[CONTEXT]\n<<<FILE examples/ai253/trainer_resume.py>>>\nfrom transformers import Trainer\ntrainer = Trainer(... )\n# BUG: 재시작 미지원\ntrainer.train()\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai253/trainer_resume.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["resume_from_checkpoint"]}, "code": "from transformers import Trainer\ntrainer = Trainer(... )\ntrainer.train(resume_from_checkpoint=True)\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n재현성: 시드 함수 추가\n\n[CONTEXT]\n<<<FILE examples/ai31/seed_utils.py>>>\n# BUG: 시드 미설정\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai31/seed_utils.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["set_seed("]}, "code": "import os, random, numpy as np, torch\n\ndef set_seed(seed: int = 42):\n    random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n필수 import 추가\n\n[CONTEXT]\n<<<FILE examples/ai101/imports.py>>>\n# BUG: numpy 미사용이지만 추후 사용 예정\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai101/imports.py", "loc": {"type": "anchor", "before": "\\A"}, "action": "insert_after", "once": true, "pre": {"must_not_contain": ["import numpy as np"]}, "code": "import numpy as np\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n훈련 안정화: GA steps/로깅/평가/체크포인팅/스케줄러\n\n[CONTEXT]\n<<<FILE examples/ai214/training_args.py>>>\nfrom transformers import TrainingArguments\n\nargs = TrainingArguments(\n    output_dir=\"out\",\n    per_device_train_batch_size=1,\n    num_train_epochs=1,\n)\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai214/training_args.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["gradient_accumulation_steps"]}, "code": "from transformers import TrainingArguments\n\nargs = TrainingArguments(\n    output_dir=\"out\",\n    per_device_train_batch_size=1,\n    gradient_accumulation_steps=8,\n    num_train_epochs=1,\n    logging_steps=10,\n    eval_strategy=\"steps\",\n    eval_steps=50,\n    save_steps=50,\n    lr_scheduler_type=\"cosine\",\n    warmup_ratio=0.03,\n    fp16=False, bf16=True,\n    gradient_checkpointing=True,\n    max_grad_norm=1.0,\n)\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n훈련 안정화: GA steps/로깅/평가/체크포인팅/스케줄러\n\n[CONTEXT]\n<<<FILE examples/ai46/training_args.py>>>\nfrom transformers import TrainingArguments\n\nargs = TrainingArguments(\n    output_dir=\"out\",\n    per_device_train_batch_size=1,\n    num_train_epochs=1,\n)\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai46/training_args.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["gradient_accumulation_steps"]}, "code": "from transformers import TrainingArguments\n\nargs = TrainingArguments(\n    output_dir=\"out\",\n    per_device_train_batch_size=1,\n    gradient_accumulation_steps=8,\n    num_train_epochs=1,\n    logging_steps=10,\n    eval_strategy=\"steps\",\n    eval_steps=50,\n    save_steps=50,\n    lr_scheduler_type=\"cosine\",\n    warmup_ratio=0.03,\n    fp16=False, bf16=True,\n    gradient_checkpointing=True,\n    max_grad_norm=1.0,\n)\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nflash 미지원 시 sdpa 안전 경로\n\n[CONTEXT]\n<<<FILE examples/ai230/model_load.py>>>\nfrom transformers import AutoModelForCausalLM\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"Qwen/Qwen3-8B\",\n)\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai230/model_load.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["attn_implementation"]}, "code": "from transformers import AutoModelForCausalLM\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"Qwen/Qwen3-8B\",\n    attn_implementation=\"sdpa\",\n)\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n조기 종료 콜백 추가\n\n[CONTEXT]\n<<<FILE examples/ai134/callbacks.py>>>\nfrom transformers import TrainerCallback\n\nclass MyCB(TrainerCallback):\n    pass\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai134/callbacks.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["EarlyStoppingCallback"]}, "code": "from transformers import TrainerCallback, EarlyStoppingCallback\n\nclass MyCB(TrainerCallback):\n    pass\n\nCALLBACKS = [EarlyStoppingCallback(early_stopping_patience=2)]\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nLoRA target_modules 확장\n\n[CONTEXT]\n<<<FILE examples/ai386/lora_cfg.py>>>\nfrom peft import LoraConfig\n\nLC = LoraConfig(r=8, target_modules=[\n    \"q_proj\", \"v_proj\"  # BUG: 부족\n])\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai386/lora_cfg.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["k_proj", "o_proj"]}, "code": "from peft import LoraConfig\n\nLC = LoraConfig(r=8, target_modules=[\n    \"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"\n])\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nWindows에서 DataLoader 프리즈 방지 + pin_memory/persistent_workers\n\n[CONTEXT]\n<<<FILE examples/ai522/dataloader_main.py>>>\nimport torch, torch.utils.data as tud\n\nclass Dummy(torch.utils.data.Dataset):\n    def __len__(self): return 8\n    def __getitem__(self, i): return i\n\n# BUG: 모듈 임포트만으로 워커가 스폰되어 Windows에서 프리즈\nloader = tud.DataLoader(Dummy(), batch_size=2, num_workers=4)\nfor b in loader:\n    pass\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai522/dataloader_main.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["if __name__ == \"__main__\":"]}, "code": "import torch, torch.utils.data as tud\n\nclass Dummy(torch.utils.data.Dataset):\n    def __len__(self): return 8\n    def __getitem__(self, i): return i\n\n\ndef main():\n    loader = tud.DataLoader(Dummy(), batch_size=2, num_workers=4, pin_memory=True, persistent_workers=True)\n    for _ in loader:\n        pass\n\n\nif __name__ == \"__main__\":\n    main()\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nflash 미지원 시 sdpa 안전 경로\n\n[CONTEXT]\n<<<FILE examples/ai45/model_load.py>>>\nfrom transformers import AutoModelForCausalLM\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"Qwen/Qwen3-8B\",\n)\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai45/model_load.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["attn_implementation"]}, "code": "from transformers import AutoModelForCausalLM\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"Qwen/Qwen3-8B\",\n    attn_implementation=\"sdpa\",\n)\n"}]}}
{"instruction": "Propose a minimal JSON plan for code changes. Output PLAN JSON only.", "input": "[INTENT]\n입출력/런타임 안전성 개선\n\n[CONTEXT]\n<<<FILE examples/ai660/dataloader_main.py>>>\nloader = DataLoader(...); for b in loader: pass\n\n<<<END>>>\n<<<FILE examples/ai660/ddp_main.py>>>\ntrainer()\n\n<<<END>>>", "output": {"files": [{"path": "examples/ai660/dataloader_main.py", "reason": "Windows freeze 가능", "strategy": "regex", "tests": ["__main__ 가드"]}, {"path": "examples/ai660/ddp_main.py", "reason": "DDP 가드 없음", "strategy": "regex", "tests": ["__main__"]}], "notes": "spawn 보호"}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n8bit 옵티마이저 + 코사인 스케줄러\n\n[CONTEXT]\n<<<FILE examples/ai582/optim.py>>>\nimport torch\nopt = torch.optim.Adam([], lr=3e-4)\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai582/optim.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["PagedAdamW8bit"]}, "code": "from transformers import get_scheduler\nfrom bitsandbytes.optim import PagedAdamW8bit\nopt = PagedAdamW8bit([], lr=2e-4)\nsched = get_scheduler(\"cosine\", optimizer=opt, num_warmup_steps=100, num_training_steps=1000)\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nflash 미지원 시 sdpa 안전 경로\n\n[CONTEXT]\n<<<FILE examples/ai441/model_load.py>>>\nfrom transformers import AutoModelForCausalLM\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"Qwen/Qwen3-8B\",\n)\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai441/model_load.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["attn_implementation"]}, "code": "from transformers import AutoModelForCausalLM\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"Qwen/Qwen3-8B\",\n    attn_implementation=\"sdpa\",\n)\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nLoRA target_modules 확장\n\n[CONTEXT]\n<<<FILE examples/ai492/lora_cfg.py>>>\nfrom peft import LoraConfig\n\nLC = LoraConfig(r=8, target_modules=[\n    \"q_proj\", \"v_proj\"  # BUG: 부족\n])\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai492/lora_cfg.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["k_proj", "o_proj"]}, "code": "from peft import LoraConfig\n\nLC = LoraConfig(r=8, target_modules=[\n    \"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"\n])\n"}]}}
{"instruction": "Propose a minimal JSON plan for code changes. Output PLAN JSON only.", "input": "[INTENT]\n입출력/런타임 안전성 개선\n\n[CONTEXT]\n<<<FILE examples/ai716/dataloader_main.py>>>\nloader = DataLoader(...); for b in loader: pass\n\n<<<END>>>\n<<<FILE examples/ai716/ddp_main.py>>>\ntrainer()\n\n<<<END>>>", "output": {"files": [{"path": "examples/ai716/dataloader_main.py", "reason": "Windows freeze 가능", "strategy": "regex", "tests": ["__main__ 가드"]}, {"path": "examples/ai716/ddp_main.py", "reason": "DDP 가드 없음", "strategy": "regex", "tests": ["__main__"]}], "notes": "spawn 보호"}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n학습 후 어댑터 저장 추가\n\n[CONTEXT]\n<<<FILE examples/ai306/save_after_train.py>>>\nfrom peft import PeftModel\n# ... 학습 후\n# BUG: 저장 누락\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai306/save_after_train.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["save_pretrained"]}, "code": "from peft import PeftModel\n# ... 학습 후\nmodel.save_pretrained(\"training/qlora-out/export\")\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nflash 미지원 시 sdpa 안전 경로\n\n[CONTEXT]\n<<<FILE examples/ai387/model_load.py>>>\nfrom transformers import AutoModelForCausalLM\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"Qwen/Qwen3-8B\",\n)\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai387/model_load.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["attn_implementation"]}, "code": "from transformers import AutoModelForCausalLM\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"Qwen/Qwen3-8B\",\n    attn_implementation=\"sdpa\",\n)\n"}]}}
{"instruction": "Propose a minimal JSON plan for code changes. Output PLAN JSON only.", "input": "[INTENT]\n입출력/런타임 안전성 개선\n\n[CONTEXT]\n<<<FILE examples/ai710/dataloader_main.py>>>\nloader = DataLoader(...); for b in loader: pass\n\n<<<END>>>\n<<<FILE examples/ai710/ddp_main.py>>>\ntrainer()\n\n<<<END>>>", "output": {"files": [{"path": "examples/ai710/dataloader_main.py", "reason": "Windows freeze 가능", "strategy": "regex", "tests": ["__main__ 가드"]}, {"path": "examples/ai710/ddp_main.py", "reason": "DDP 가드 없음", "strategy": "regex", "tests": ["__main__"]}], "notes": "spawn 보호"}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n학습 후 어댑터 저장 추가\n\n[CONTEXT]\n<<<FILE examples/ai176/save_after_train.py>>>\nfrom peft import PeftModel\n# ... 학습 후\n# BUG: 저장 누락\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai176/save_after_train.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["save_pretrained"]}, "code": "from peft import PeftModel\n# ... 학습 후\nmodel.save_pretrained(\"training/qlora-out/export\")\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\npad 토큰 ID 정확히 설정\n\n[CONTEXT]\n<<<FILE examples/ai54/label_cfg.py>>>\nfrom transformers import AutoTokenizer\nfrom dataclasses import dataclass\n\n@dataclass\nclass Cfg:\n    pad_id: int = -100  # BUG\n\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai54/label_cfg.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["CFG = Cfg"]}, "code": "from transformers import AutoTokenizer\nfrom dataclasses import dataclass\n\n@dataclass\nclass Cfg:\n    pad_id: int | None = None\n\n# tokenizer 로드 후 설정\ntok = AutoTokenizer.from_pretrained(\"Qwen/Qwen3-8B\", use_fast=True)\nCFG = Cfg(pad_id=tok.pad_token_id)\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n재현성: 시드 함수 추가\n\n[CONTEXT]\n<<<FILE examples/ai437/seed_utils.py>>>\n# BUG: 시드 미설정\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai437/seed_utils.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["set_seed("]}, "code": "import os, random, numpy as np, torch\n\ndef set_seed(seed: int = 42):\n    random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n훈련 안정화: GA steps/로깅/평가/체크포인팅/스케줄러\n\n[CONTEXT]\n<<<FILE examples/ai575/training_args.py>>>\nfrom transformers import TrainingArguments\n\nargs = TrainingArguments(\n    output_dir=\"out\",\n    per_device_train_batch_size=1,\n    num_train_epochs=1,\n)\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai575/training_args.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["gradient_accumulation_steps"]}, "code": "from transformers import TrainingArguments\n\nargs = TrainingArguments(\n    output_dir=\"out\",\n    per_device_train_batch_size=1,\n    gradient_accumulation_steps=8,\n    num_train_epochs=1,\n    logging_steps=10,\n    eval_strategy=\"steps\",\n    eval_steps=50,\n    save_steps=50,\n    lr_scheduler_type=\"cosine\",\n    warmup_ratio=0.03,\n    fp16=False, bf16=True,\n    gradient_checkpointing=True,\n    max_grad_norm=1.0,\n)\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n체크포인트 재개 지원\n\n[CONTEXT]\n<<<FILE examples/ai628/trainer_resume.py>>>\nfrom transformers import Trainer\ntrainer = Trainer(... )\n# BUG: 재시작 미지원\ntrainer.train()\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai628/trainer_resume.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["resume_from_checkpoint"]}, "code": "from transformers import Trainer\ntrainer = Trainer(... )\ntrainer.train(resume_from_checkpoint=True)\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\npad 토큰 ID 정확히 설정\n\n[CONTEXT]\n<<<FILE examples/ai460/label_cfg.py>>>\nfrom transformers import AutoTokenizer\nfrom dataclasses import dataclass\n\n@dataclass\nclass Cfg:\n    pad_id: int = -100  # BUG\n\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai460/label_cfg.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["CFG = Cfg"]}, "code": "from transformers import AutoTokenizer\nfrom dataclasses import dataclass\n\n@dataclass\nclass Cfg:\n    pad_id: int | None = None\n\n# tokenizer 로드 후 설정\ntok = AutoTokenizer.from_pretrained(\"Qwen/Qwen3-8B\", use_fast=True)\nCFG = Cfg(pad_id=tok.pad_token_id)\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n학습 후 어댑터 저장 추가\n\n[CONTEXT]\n<<<FILE examples/ai33/save_after_train.py>>>\nfrom peft import PeftModel\n# ... 학습 후\n# BUG: 저장 누락\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai33/save_after_train.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["save_pretrained"]}, "code": "from peft import PeftModel\n# ... 학습 후\nmodel.save_pretrained(\"training/qlora-out/export\")\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nDDP/torchrun 안전 가드\n\n[CONTEXT]\n<<<FILE examples/ai559/ddp_main.py>>>\nimport torch\n\n# BUG: torchrun에서 main 가드 없음\ntrainer()\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai559/ddp_main.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["__main__"]}, "code": "import torch\n\n\ndef main():\n    trainer()\n\n\nif __name__ == \"__main__\":\n    main()\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n8bit 옵티마이저 + 코사인 스케줄러\n\n[CONTEXT]\n<<<FILE examples/ai528/optim.py>>>\nimport torch\nopt = torch.optim.Adam([], lr=3e-4)\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai528/optim.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["PagedAdamW8bit"]}, "code": "from transformers import get_scheduler\nfrom bitsandbytes.optim import PagedAdamW8bit\nopt = PagedAdamW8bit([], lr=2e-4)\nsched = get_scheduler(\"cosine\", optimizer=opt, num_warmup_steps=100, num_training_steps=1000)\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n조기 종료 콜백 추가\n\n[CONTEXT]\n<<<FILE examples/ai636/callbacks.py>>>\nfrom transformers import TrainerCallback\n\nclass MyCB(TrainerCallback):\n    pass\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai636/callbacks.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["EarlyStoppingCallback"]}, "code": "from transformers import TrainerCallback, EarlyStoppingCallback\n\nclass MyCB(TrainerCallback):\n    pass\n\nCALLBACKS = [EarlyStoppingCallback(early_stopping_patience=2)]\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nLoRA target_modules 확장\n\n[CONTEXT]\n<<<FILE examples/ai6/lora_cfg.py>>>\nfrom peft import LoraConfig\n\nLC = LoraConfig(r=8, target_modules=[\n    \"q_proj\", \"v_proj\"  # BUG: 부족\n])\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai6/lora_cfg.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["k_proj", "o_proj"]}, "code": "from peft import LoraConfig\n\nLC = LoraConfig(r=8, target_modules=[\n    \"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"\n])\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nflash 미지원 시 sdpa 안전 경로\n\n[CONTEXT]\n<<<FILE examples/ai294/model_load.py>>>\nfrom transformers import AutoModelForCausalLM\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"Qwen/Qwen3-8B\",\n)\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai294/model_load.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["attn_implementation"]}, "code": "from transformers import AutoModelForCausalLM\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"Qwen/Qwen3-8B\",\n    attn_implementation=\"sdpa\",\n)\n"}]}}
{"instruction": "Propose a minimal JSON plan for code changes. Output PLAN JSON only.", "input": "[INTENT]\n입출력/런타임 안전성 개선\n\n[CONTEXT]\n<<<FILE examples/ai745/dataloader_main.py>>>\nloader = DataLoader(...); for b in loader: pass\n\n<<<END>>>\n<<<FILE examples/ai745/ddp_main.py>>>\ntrainer()\n\n<<<END>>>", "output": {"files": [{"path": "examples/ai745/dataloader_main.py", "reason": "Windows freeze 가능", "strategy": "regex", "tests": ["__main__ 가드"]}, {"path": "examples/ai745/ddp_main.py", "reason": "DDP 가드 없음", "strategy": "regex", "tests": ["__main__"]}], "notes": "spawn 보호"}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n필수 import 추가\n\n[CONTEXT]\n<<<FILE examples/ai185/imports.py>>>\n# BUG: numpy 미사용이지만 추후 사용 예정\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai185/imports.py", "loc": {"type": "anchor", "before": "\\A"}, "action": "insert_after", "once": true, "pre": {"must_not_contain": ["import numpy as np"]}, "code": "import numpy as np\n"}]}}
{"instruction": "Propose a minimal JSON plan for code changes. Output PLAN JSON only.", "input": "[INTENT]\n학습 안정화(AMP+훈련 설정)\n\n[CONTEXT]\n<<<FILE examples/ai658/train_utils.py>>>\ndef get_dtype():\n    return torch.float16\n\n<<<END>>>\n<<<FILE examples/ai658/training_args.py>>>\nargs = TrainingArguments(output_dir='out', per_device_train_batch_size=1, num_train_epochs=1)\n\n<<<END>>>", "output": {"files": [{"path": "examples/ai658/train_utils.py", "reason": "dtype가 고정 float16", "strategy": "regex", "tests": ["bf16 또는 fp16 반환"]}, {"path": "examples/ai658/training_args.py", "reason": "로깅/평가/GA 설정 누락", "strategy": "regex", "tests": ["eval_strategy=='steps'"]}], "notes": "bf16 지원 시 우선"}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\npad 토큰 ID 정확히 설정\n\n[CONTEXT]\n<<<FILE examples/ai49/label_cfg.py>>>\nfrom transformers import AutoTokenizer\nfrom dataclasses import dataclass\n\n@dataclass\nclass Cfg:\n    pad_id: int = -100  # BUG\n\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai49/label_cfg.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["CFG = Cfg"]}, "code": "from transformers import AutoTokenizer\nfrom dataclasses import dataclass\n\n@dataclass\nclass Cfg:\n    pad_id: int | None = None\n\n# tokenizer 로드 후 설정\ntok = AutoTokenizer.from_pretrained(\"Qwen/Qwen3-8B\", use_fast=True)\nCFG = Cfg(pad_id=tok.pad_token_id)\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nLoRA target_modules 확장\n\n[CONTEXT]\n<<<FILE examples/ai418/lora_cfg.py>>>\nfrom peft import LoraConfig\n\nLC = LoraConfig(r=8, target_modules=[\n    \"q_proj\", \"v_proj\"  # BUG: 부족\n])\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai418/lora_cfg.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["k_proj", "o_proj"]}, "code": "from peft import LoraConfig\n\nLC = LoraConfig(r=8, target_modules=[\n    \"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"\n])\n"}]}}
{"instruction": "Propose a minimal JSON plan for code changes. Output PLAN JSON only.", "input": "[INTENT]\n학습 안정화(AMP+훈련 설정)\n\n[CONTEXT]\n<<<FILE examples/ai705/train_utils.py>>>\ndef get_dtype():\n    return torch.float16\n\n<<<END>>>\n<<<FILE examples/ai705/training_args.py>>>\nargs = TrainingArguments(output_dir='out', per_device_train_batch_size=1, num_train_epochs=1)\n\n<<<END>>>", "output": {"files": [{"path": "examples/ai705/train_utils.py", "reason": "dtype가 고정 float16", "strategy": "regex", "tests": ["bf16 또는 fp16 반환"]}, {"path": "examples/ai705/training_args.py", "reason": "로깅/평가/GA 설정 누락", "strategy": "regex", "tests": ["eval_strategy=='steps'"]}], "notes": "bf16 지원 시 우선"}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n재현성: 시드 함수 추가\n\n[CONTEXT]\n<<<FILE examples/ai16/seed_utils.py>>>\n# BUG: 시드 미설정\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai16/seed_utils.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["set_seed("]}, "code": "import os, random, numpy as np, torch\n\ndef set_seed(seed: int = 42):\n    random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n8bit 옵티마이저 + 코사인 스케줄러\n\n[CONTEXT]\n<<<FILE examples/ai92/optim.py>>>\nimport torch\nopt = torch.optim.Adam([], lr=3e-4)\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai92/optim.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["PagedAdamW8bit"]}, "code": "from transformers import get_scheduler\nfrom bitsandbytes.optim import PagedAdamW8bit\nopt = PagedAdamW8bit([], lr=2e-4)\nsched = get_scheduler(\"cosine\", optimizer=opt, num_warmup_steps=100, num_training_steps=1000)\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n조기 종료 콜백 추가\n\n[CONTEXT]\n<<<FILE examples/ai515/callbacks.py>>>\nfrom transformers import TrainerCallback\n\nclass MyCB(TrainerCallback):\n    pass\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai515/callbacks.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["EarlyStoppingCallback"]}, "code": "from transformers import TrainerCallback, EarlyStoppingCallback\n\nclass MyCB(TrainerCallback):\n    pass\n\nCALLBACKS = [EarlyStoppingCallback(early_stopping_patience=2)]\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n8bit 옵티마이저 + 코사인 스케줄러\n\n[CONTEXT]\n<<<FILE examples/ai474/optim.py>>>\nimport torch\nopt = torch.optim.Adam([], lr=3e-4)\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai474/optim.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["PagedAdamW8bit"]}, "code": "from transformers import get_scheduler\nfrom bitsandbytes.optim import PagedAdamW8bit\nopt = PagedAdamW8bit([], lr=2e-4)\nsched = get_scheduler(\"cosine\", optimizer=opt, num_warmup_steps=100, num_training_steps=1000)\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n조기 종료 콜백 추가\n\n[CONTEXT]\n<<<FILE examples/ai433/callbacks.py>>>\nfrom transformers import TrainerCallback\n\nclass MyCB(TrainerCallback):\n    pass\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai433/callbacks.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["EarlyStoppingCallback"]}, "code": "from transformers import TrainerCallback, EarlyStoppingCallback\n\nclass MyCB(TrainerCallback):\n    pass\n\nCALLBACKS = [EarlyStoppingCallback(early_stopping_patience=2)]\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n조기 종료 콜백 추가\n\n[CONTEXT]\n<<<FILE examples/ai327/callbacks.py>>>\nfrom transformers import TrainerCallback\n\nclass MyCB(TrainerCallback):\n    pass\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai327/callbacks.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["EarlyStoppingCallback"]}, "code": "from transformers import TrainerCallback, EarlyStoppingCallback\n\nclass MyCB(TrainerCallback):\n    pass\n\nCALLBACKS = [EarlyStoppingCallback(early_stopping_patience=2)]\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nAMP 적용 + bf16 우선\n\n[CONTEXT]\n<<<FILE examples/ai629/train_step.py>>>\nimport torch\n\n# BUG: autocast 미사용\ndef train_step(model, batch):\n    x, y = batch\n    out = model(x)\n    loss = out.loss if hasattr(out, 'loss') else out.mean()\n    loss.backward()\n    return loss.item()\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai629/train_step.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["autocast"]}, "code": "import torch\n\n@torch.no_grad()\ndef _bf16_supported():\n    return torch.cuda.is_available() and torch.cuda.is_bf16_supported()\n\n\ndef train_step(model, batch):\n    x, y = batch\n    amp_dtype = torch.bfloat16 if _bf16_supported() else torch.float16\n    scaler = torch.cuda.amp.GradScaler(enabled=True)\n    with torch.cuda.amp.autocast(enabled=True, dtype=amp_dtype):\n        out = model(x)\n        loss = out.loss if hasattr(out, 'loss') else out.mean()\n    scaler.scale(loss).backward()\n    scaler.step(torch.optim.Adam(model.parameters(), lr=1e-4))\n    scaler.update()\n    model.zero_grad(set_to_none=True)\n    return float(loss.detach().item())\n"}]}}
{"instruction": "Propose a minimal JSON plan for code changes. Output PLAN JSON only.", "input": "[INTENT]\n입출력/런타임 안전성 개선\n\n[CONTEXT]\n<<<FILE examples/ai653/dataloader_main.py>>>\nloader = DataLoader(...); for b in loader: pass\n\n<<<END>>>\n<<<FILE examples/ai653/ddp_main.py>>>\ntrainer()\n\n<<<END>>>", "output": {"files": [{"path": "examples/ai653/dataloader_main.py", "reason": "Windows freeze 가능", "strategy": "regex", "tests": ["__main__ 가드"]}, {"path": "examples/ai653/ddp_main.py", "reason": "DDP 가드 없음", "strategy": "regex", "tests": ["__main__"]}], "notes": "spawn 보호"}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n필수 import 추가\n\n[CONTEXT]\n<<<FILE examples/ai114/imports.py>>>\n# BUG: numpy 미사용이지만 추후 사용 예정\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai114/imports.py", "loc": {"type": "anchor", "before": "\\A"}, "action": "insert_after", "once": true, "pre": {"must_not_contain": ["import numpy as np"]}, "code": "import numpy as np\n"}]}}
{"instruction": "Propose a minimal JSON plan for code changes. Output PLAN JSON only.", "input": "[INTENT]\n입출력/런타임 안전성 개선\n\n[CONTEXT]\n<<<FILE examples/ai642/dataloader_main.py>>>\nloader = DataLoader(...); for b in loader: pass\n\n<<<END>>>\n<<<FILE examples/ai642/ddp_main.py>>>\ntrainer()\n\n<<<END>>>", "output": {"files": [{"path": "examples/ai642/dataloader_main.py", "reason": "Windows freeze 가능", "strategy": "regex", "tests": ["__main__ 가드"]}, {"path": "examples/ai642/ddp_main.py", "reason": "DDP 가드 없음", "strategy": "regex", "tests": ["__main__"]}], "notes": "spawn 보호"}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nWindows에서 DataLoader 프리즈 방지 + pin_memory/persistent_workers\n\n[CONTEXT]\n<<<FILE examples/ai291/dataloader_main.py>>>\nimport torch, torch.utils.data as tud\n\nclass Dummy(torch.utils.data.Dataset):\n    def __len__(self): return 8\n    def __getitem__(self, i): return i\n\n# BUG: 모듈 임포트만으로 워커가 스폰되어 Windows에서 프리즈\nloader = tud.DataLoader(Dummy(), batch_size=2, num_workers=4)\nfor b in loader:\n    pass\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai291/dataloader_main.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["if __name__ == \"__main__\":"]}, "code": "import torch, torch.utils.data as tud\n\nclass Dummy(torch.utils.data.Dataset):\n    def __len__(self): return 8\n    def __getitem__(self, i): return i\n\n\ndef main():\n    loader = tud.DataLoader(Dummy(), batch_size=2, num_workers=4, pin_memory=True, persistent_workers=True)\n    for _ in loader:\n        pass\n\n\nif __name__ == \"__main__\":\n    main()\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n조기 종료 콜백 추가\n\n[CONTEXT]\n<<<FILE examples/ai9/callbacks.py>>>\nfrom transformers import TrainerCallback\n\nclass MyCB(TrainerCallback):\n    pass\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai9/callbacks.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["EarlyStoppingCallback"]}, "code": "from transformers import TrainerCallback, EarlyStoppingCallback\n\nclass MyCB(TrainerCallback):\n    pass\n\nCALLBACKS = [EarlyStoppingCallback(early_stopping_patience=2)]\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n8bit 옵티마이저 + 코사인 스케줄러\n\n[CONTEXT]\n<<<FILE examples/ai72/optim.py>>>\nimport torch\nopt = torch.optim.Adam([], lr=3e-4)\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai72/optim.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["PagedAdamW8bit"]}, "code": "from transformers import get_scheduler\nfrom bitsandbytes.optim import PagedAdamW8bit\nopt = PagedAdamW8bit([], lr=2e-4)\nsched = get_scheduler(\"cosine\", optimizer=opt, num_warmup_steps=100, num_training_steps=1000)\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n조기 종료 콜백 추가\n\n[CONTEXT]\n<<<FILE examples/ai394/callbacks.py>>>\nfrom transformers import TrainerCallback\n\nclass MyCB(TrainerCallback):\n    pass\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai394/callbacks.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["EarlyStoppingCallback"]}, "code": "from transformers import TrainerCallback, EarlyStoppingCallback\n\nclass MyCB(TrainerCallback):\n    pass\n\nCALLBACKS = [EarlyStoppingCallback(early_stopping_patience=2)]\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nAMP 적용 + bf16 우선\n\n[CONTEXT]\n<<<FILE examples/ai158/train_step.py>>>\nimport torch\n\n# BUG: autocast 미사용\ndef train_step(model, batch):\n    x, y = batch\n    out = model(x)\n    loss = out.loss if hasattr(out, 'loss') else out.mean()\n    loss.backward()\n    return loss.item()\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai158/train_step.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["autocast"]}, "code": "import torch\n\n@torch.no_grad()\ndef _bf16_supported():\n    return torch.cuda.is_available() and torch.cuda.is_bf16_supported()\n\n\ndef train_step(model, batch):\n    x, y = batch\n    amp_dtype = torch.bfloat16 if _bf16_supported() else torch.float16\n    scaler = torch.cuda.amp.GradScaler(enabled=True)\n    with torch.cuda.amp.autocast(enabled=True, dtype=amp_dtype):\n        out = model(x)\n        loss = out.loss if hasattr(out, 'loss') else out.mean()\n    scaler.scale(loss).backward()\n    scaler.step(torch.optim.Adam(model.parameters(), lr=1e-4))\n    scaler.update()\n    model.zero_grad(set_to_none=True)\n    return float(loss.detach().item())\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nDDP/torchrun 안전 가드\n\n[CONTEXT]\n<<<FILE examples/ai548/ddp_main.py>>>\nimport torch\n\n# BUG: torchrun에서 main 가드 없음\ntrainer()\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai548/ddp_main.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["__main__"]}, "code": "import torch\n\n\ndef main():\n    trainer()\n\n\nif __name__ == \"__main__\":\n    main()\n"}]}}
{"instruction": "Propose a minimal JSON plan for code changes. Output PLAN JSON only.", "input": "[INTENT]\n입출력/런타임 안전성 개선\n\n[CONTEXT]\n<<<FILE examples/ai659/dataloader_main.py>>>\nloader = DataLoader(...); for b in loader: pass\n\n<<<END>>>\n<<<FILE examples/ai659/ddp_main.py>>>\ntrainer()\n\n<<<END>>>", "output": {"files": [{"path": "examples/ai659/dataloader_main.py", "reason": "Windows freeze 가능", "strategy": "regex", "tests": ["__main__ 가드"]}, {"path": "examples/ai659/ddp_main.py", "reason": "DDP 가드 없음", "strategy": "regex", "tests": ["__main__"]}], "notes": "spawn 보호"}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nflash 미지원 시 sdpa 안전 경로\n\n[CONTEXT]\n<<<FILE examples/ai499/model_load.py>>>\nfrom transformers import AutoModelForCausalLM\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"Qwen/Qwen3-8B\",\n)\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai499/model_load.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["attn_implementation"]}, "code": "from transformers import AutoModelForCausalLM\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"Qwen/Qwen3-8B\",\n    attn_implementation=\"sdpa\",\n)\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nAMP 적용 + bf16 우선\n\n[CONTEXT]\n<<<FILE examples/ai107/train_step.py>>>\nimport torch\n\n# BUG: autocast 미사용\ndef train_step(model, batch):\n    x, y = batch\n    out = model(x)\n    loss = out.loss if hasattr(out, 'loss') else out.mean()\n    loss.backward()\n    return loss.item()\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai107/train_step.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["autocast"]}, "code": "import torch\n\n@torch.no_grad()\ndef _bf16_supported():\n    return torch.cuda.is_available() and torch.cuda.is_bf16_supported()\n\n\ndef train_step(model, batch):\n    x, y = batch\n    amp_dtype = torch.bfloat16 if _bf16_supported() else torch.float16\n    scaler = torch.cuda.amp.GradScaler(enabled=True)\n    with torch.cuda.amp.autocast(enabled=True, dtype=amp_dtype):\n        out = model(x)\n        loss = out.loss if hasattr(out, 'loss') else out.mean()\n    scaler.scale(loss).backward()\n    scaler.step(torch.optim.Adam(model.parameters(), lr=1e-4))\n    scaler.update()\n    model.zero_grad(set_to_none=True)\n    return float(loss.detach().item())\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nDDP/torchrun 안전 가드\n\n[CONTEXT]\n<<<FILE examples/ai40/ddp_main.py>>>\nimport torch\n\n# BUG: torchrun에서 main 가드 없음\ntrainer()\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai40/ddp_main.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["__main__"]}, "code": "import torch\n\n\ndef main():\n    trainer()\n\n\nif __name__ == \"__main__\":\n    main()\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n학습 후 어댑터 저장 추가\n\n[CONTEXT]\n<<<FILE examples/ai375/save_after_train.py>>>\nfrom peft import PeftModel\n# ... 학습 후\n# BUG: 저장 누락\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai375/save_after_train.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["save_pretrained"]}, "code": "from peft import PeftModel\n# ... 학습 후\nmodel.save_pretrained(\"training/qlora-out/export\")\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nAMP 적용 + bf16 우선\n\n[CONTEXT]\n<<<FILE examples/ai526/train_step.py>>>\nimport torch\n\n# BUG: autocast 미사용\ndef train_step(model, batch):\n    x, y = batch\n    out = model(x)\n    loss = out.loss if hasattr(out, 'loss') else out.mean()\n    loss.backward()\n    return loss.item()\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai526/train_step.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["autocast"]}, "code": "import torch\n\n@torch.no_grad()\ndef _bf16_supported():\n    return torch.cuda.is_available() and torch.cuda.is_bf16_supported()\n\n\ndef train_step(model, batch):\n    x, y = batch\n    amp_dtype = torch.bfloat16 if _bf16_supported() else torch.float16\n    scaler = torch.cuda.amp.GradScaler(enabled=True)\n    with torch.cuda.amp.autocast(enabled=True, dtype=amp_dtype):\n        out = model(x)\n        loss = out.loss if hasattr(out, 'loss') else out.mean()\n    scaler.scale(loss).backward()\n    scaler.step(torch.optim.Adam(model.parameters(), lr=1e-4))\n    scaler.update()\n    model.zero_grad(set_to_none=True)\n    return float(loss.detach().item())\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n재현성: 시드 함수 추가\n\n[CONTEXT]\n<<<FILE examples/ai139/seed_utils.py>>>\n# BUG: 시드 미설정\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai139/seed_utils.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["set_seed("]}, "code": "import os, random, numpy as np, torch\n\ndef set_seed(seed: int = 42):\n    random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nAMP 적용 + bf16 우선\n\n[CONTEXT]\n<<<FILE examples/ai179/train_step.py>>>\nimport torch\n\n# BUG: autocast 미사용\ndef train_step(model, batch):\n    x, y = batch\n    out = model(x)\n    loss = out.loss if hasattr(out, 'loss') else out.mean()\n    loss.backward()\n    return loss.item()\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai179/train_step.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["autocast"]}, "code": "import torch\n\n@torch.no_grad()\ndef _bf16_supported():\n    return torch.cuda.is_available() and torch.cuda.is_bf16_supported()\n\n\ndef train_step(model, batch):\n    x, y = batch\n    amp_dtype = torch.bfloat16 if _bf16_supported() else torch.float16\n    scaler = torch.cuda.amp.GradScaler(enabled=True)\n    with torch.cuda.amp.autocast(enabled=True, dtype=amp_dtype):\n        out = model(x)\n        loss = out.loss if hasattr(out, 'loss') else out.mean()\n    scaler.scale(loss).backward()\n    scaler.step(torch.optim.Adam(model.parameters(), lr=1e-4))\n    scaler.update()\n    model.zero_grad(set_to_none=True)\n    return float(loss.detach().item())\n"}]}}
{"instruction": "Propose a minimal JSON plan for code changes. Output PLAN JSON only.", "input": "[INTENT]\n학습 안정화(AMP+훈련 설정)\n\n[CONTEXT]\n<<<FILE examples/ai791/train_utils.py>>>\ndef get_dtype():\n    return torch.float16\n\n<<<END>>>\n<<<FILE examples/ai791/training_args.py>>>\nargs = TrainingArguments(output_dir='out', per_device_train_batch_size=1, num_train_epochs=1)\n\n<<<END>>>", "output": {"files": [{"path": "examples/ai791/train_utils.py", "reason": "dtype가 고정 float16", "strategy": "regex", "tests": ["bf16 또는 fp16 반환"]}, {"path": "examples/ai791/training_args.py", "reason": "로깅/평가/GA 설정 누락", "strategy": "regex", "tests": ["eval_strategy=='steps'"]}], "notes": "bf16 지원 시 우선"}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nAMP 적용 + bf16 우선\n\n[CONTEXT]\n<<<FILE examples/ai484/train_step.py>>>\nimport torch\n\n# BUG: autocast 미사용\ndef train_step(model, batch):\n    x, y = batch\n    out = model(x)\n    loss = out.loss if hasattr(out, 'loss') else out.mean()\n    loss.backward()\n    return loss.item()\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai484/train_step.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["autocast"]}, "code": "import torch\n\n@torch.no_grad()\ndef _bf16_supported():\n    return torch.cuda.is_available() and torch.cuda.is_bf16_supported()\n\n\ndef train_step(model, batch):\n    x, y = batch\n    amp_dtype = torch.bfloat16 if _bf16_supported() else torch.float16\n    scaler = torch.cuda.amp.GradScaler(enabled=True)\n    with torch.cuda.amp.autocast(enabled=True, dtype=amp_dtype):\n        out = model(x)\n        loss = out.loss if hasattr(out, 'loss') else out.mean()\n    scaler.scale(loss).backward()\n    scaler.step(torch.optim.Adam(model.parameters(), lr=1e-4))\n    scaler.update()\n    model.zero_grad(set_to_none=True)\n    return float(loss.detach().item())\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n8bit 옵티마이저 + 코사인 스케줄러\n\n[CONTEXT]\n<<<FILE examples/ai330/optim.py>>>\nimport torch\nopt = torch.optim.Adam([], lr=3e-4)\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai330/optim.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["PagedAdamW8bit"]}, "code": "from transformers import get_scheduler\nfrom bitsandbytes.optim import PagedAdamW8bit\nopt = PagedAdamW8bit([], lr=2e-4)\nsched = get_scheduler(\"cosine\", optimizer=opt, num_warmup_steps=100, num_training_steps=1000)\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n조기 종료 콜백 추가\n\n[CONTEXT]\n<<<FILE examples/ai100/callbacks.py>>>\nfrom transformers import TrainerCallback\n\nclass MyCB(TrainerCallback):\n    pass\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai100/callbacks.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["EarlyStoppingCallback"]}, "code": "from transformers import TrainerCallback, EarlyStoppingCallback\n\nclass MyCB(TrainerCallback):\n    pass\n\nCALLBACKS = [EarlyStoppingCallback(early_stopping_patience=2)]\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nflash 미지원 시 sdpa 안전 경로\n\n[CONTEXT]\n<<<FILE examples/ai35/model_load.py>>>\nfrom transformers import AutoModelForCausalLM\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"Qwen/Qwen3-8B\",\n)\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai35/model_load.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["attn_implementation"]}, "code": "from transformers import AutoModelForCausalLM\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"Qwen/Qwen3-8B\",\n    attn_implementation=\"sdpa\",\n)\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n8bit 옵티마이저 + 코사인 스케줄러\n\n[CONTEXT]\n<<<FILE examples/ai343/optim.py>>>\nimport torch\nopt = torch.optim.Adam([], lr=3e-4)\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai343/optim.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["PagedAdamW8bit"]}, "code": "from transformers import get_scheduler\nfrom bitsandbytes.optim import PagedAdamW8bit\nopt = PagedAdamW8bit([], lr=2e-4)\nsched = get_scheduler(\"cosine\", optimizer=opt, num_warmup_steps=100, num_training_steps=1000)\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nLoRA target_modules 확장\n\n[CONTEXT]\n<<<FILE examples/ai585/lora_cfg.py>>>\nfrom peft import LoraConfig\n\nLC = LoraConfig(r=8, target_modules=[\n    \"q_proj\", \"v_proj\"  # BUG: 부족\n])\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai585/lora_cfg.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["k_proj", "o_proj"]}, "code": "from peft import LoraConfig\n\nLC = LoraConfig(r=8, target_modules=[\n    \"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"\n])\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nAMP 적용 + bf16 우선\n\n[CONTEXT]\n<<<FILE examples/ai25/train_step.py>>>\nimport torch\n\n# BUG: autocast 미사용\ndef train_step(model, batch):\n    x, y = batch\n    out = model(x)\n    loss = out.loss if hasattr(out, 'loss') else out.mean()\n    loss.backward()\n    return loss.item()\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai25/train_step.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["autocast"]}, "code": "import torch\n\n@torch.no_grad()\ndef _bf16_supported():\n    return torch.cuda.is_available() and torch.cuda.is_bf16_supported()\n\n\ndef train_step(model, batch):\n    x, y = batch\n    amp_dtype = torch.bfloat16 if _bf16_supported() else torch.float16\n    scaler = torch.cuda.amp.GradScaler(enabled=True)\n    with torch.cuda.amp.autocast(enabled=True, dtype=amp_dtype):\n        out = model(x)\n        loss = out.loss if hasattr(out, 'loss') else out.mean()\n    scaler.scale(loss).backward()\n    scaler.step(torch.optim.Adam(model.parameters(), lr=1e-4))\n    scaler.update()\n    model.zero_grad(set_to_none=True)\n    return float(loss.detach().item())\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nGPU bf16 지원 시 dtype 선택 개선\n\n[CONTEXT]\n<<<FILE examples/ai521/train_utils.py>>>\nimport torch\n\n# BUG: dtype가 고정되어 혼합정밀에 불리\ndef get_dtype():\n    return torch.float16\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai521/train_utils.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["torch.bfloat16"]}, "code": "import torch\n\n\ndef get_dtype():\n    \"\"\"GPU가 bf16 지원 시 bfloat16, 아니면 float16 반환\"\"\"\n    if torch.cuda.is_available() and torch.cuda.is_bf16_supported():\n        return torch.bfloat16\n    return torch.float16\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nDDP/torchrun 안전 가드\n\n[CONTEXT]\n<<<FILE examples/ai127/ddp_main.py>>>\nimport torch\n\n# BUG: torchrun에서 main 가드 없음\ntrainer()\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai127/ddp_main.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["__main__"]}, "code": "import torch\n\n\ndef main():\n    trainer()\n\n\nif __name__ == \"__main__\":\n    main()\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nflash 미지원 시 sdpa 안전 경로\n\n[CONTEXT]\n<<<FILE examples/ai200/model_load.py>>>\nfrom transformers import AutoModelForCausalLM\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"Qwen/Qwen3-8B\",\n)\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai200/model_load.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["attn_implementation"]}, "code": "from transformers import AutoModelForCausalLM\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"Qwen/Qwen3-8B\",\n    attn_implementation=\"sdpa\",\n)\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nWindows에서 DataLoader 프리즈 방지 + pin_memory/persistent_workers\n\n[CONTEXT]\n<<<FILE examples/ai10/dataloader_main.py>>>\nimport torch, torch.utils.data as tud\n\nclass Dummy(torch.utils.data.Dataset):\n    def __len__(self): return 8\n    def __getitem__(self, i): return i\n\n# BUG: 모듈 임포트만으로 워커가 스폰되어 Windows에서 프리즈\nloader = tud.DataLoader(Dummy(), batch_size=2, num_workers=4)\nfor b in loader:\n    pass\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai10/dataloader_main.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["if __name__ == \"__main__\":"]}, "code": "import torch, torch.utils.data as tud\n\nclass Dummy(torch.utils.data.Dataset):\n    def __len__(self): return 8\n    def __getitem__(self, i): return i\n\n\ndef main():\n    loader = tud.DataLoader(Dummy(), batch_size=2, num_workers=4, pin_memory=True, persistent_workers=True)\n    for _ in loader:\n        pass\n\n\nif __name__ == \"__main__\":\n    main()\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nGPU bf16 지원 시 dtype 선택 개선\n\n[CONTEXT]\n<<<FILE examples/ai288/train_utils.py>>>\nimport torch\n\n# BUG: dtype가 고정되어 혼합정밀에 불리\ndef get_dtype():\n    return torch.float16\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai288/train_utils.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["torch.bfloat16"]}, "code": "import torch\n\n\ndef get_dtype():\n    \"\"\"GPU가 bf16 지원 시 bfloat16, 아니면 float16 반환\"\"\"\n    if torch.cuda.is_available() and torch.cuda.is_bf16_supported():\n        return torch.bfloat16\n    return torch.float16\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n8bit 옵티마이저 + 코사인 스케줄러\n\n[CONTEXT]\n<<<FILE examples/ai354/optim.py>>>\nimport torch\nopt = torch.optim.Adam([], lr=3e-4)\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai354/optim.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["PagedAdamW8bit"]}, "code": "from transformers import get_scheduler\nfrom bitsandbytes.optim import PagedAdamW8bit\nopt = PagedAdamW8bit([], lr=2e-4)\nsched = get_scheduler(\"cosine\", optimizer=opt, num_warmup_steps=100, num_training_steps=1000)\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n필수 import 추가\n\n[CONTEXT]\n<<<FILE examples/ai512/imports.py>>>\n# BUG: numpy 미사용이지만 추후 사용 예정\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai512/imports.py", "loc": {"type": "anchor", "before": "\\A"}, "action": "insert_after", "once": true, "pre": {"must_not_contain": ["import numpy as np"]}, "code": "import numpy as np\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\npad 토큰 ID 정확히 설정\n\n[CONTEXT]\n<<<FILE examples/ai626/label_cfg.py>>>\nfrom transformers import AutoTokenizer\nfrom dataclasses import dataclass\n\n@dataclass\nclass Cfg:\n    pad_id: int = -100  # BUG\n\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai626/label_cfg.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["CFG = Cfg"]}, "code": "from transformers import AutoTokenizer\nfrom dataclasses import dataclass\n\n@dataclass\nclass Cfg:\n    pad_id: int | None = None\n\n# tokenizer 로드 후 설정\ntok = AutoTokenizer.from_pretrained(\"Qwen/Qwen3-8B\", use_fast=True)\nCFG = Cfg(pad_id=tok.pad_token_id)\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nWindows에서 DataLoader 프리즈 방지 + pin_memory/persistent_workers\n\n[CONTEXT]\n<<<FILE examples/ai523/dataloader_main.py>>>\nimport torch, torch.utils.data as tud\n\nclass Dummy(torch.utils.data.Dataset):\n    def __len__(self): return 8\n    def __getitem__(self, i): return i\n\n# BUG: 모듈 임포트만으로 워커가 스폰되어 Windows에서 프리즈\nloader = tud.DataLoader(Dummy(), batch_size=2, num_workers=4)\nfor b in loader:\n    pass\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai523/dataloader_main.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["if __name__ == \"__main__\":"]}, "code": "import torch, torch.utils.data as tud\n\nclass Dummy(torch.utils.data.Dataset):\n    def __len__(self): return 8\n    def __getitem__(self, i): return i\n\n\ndef main():\n    loader = tud.DataLoader(Dummy(), batch_size=2, num_workers=4, pin_memory=True, persistent_workers=True)\n    for _ in loader:\n        pass\n\n\nif __name__ == \"__main__\":\n    main()\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n훈련 안정화: GA steps/로깅/평가/체크포인팅/스케줄러\n\n[CONTEXT]\n<<<FILE examples/ai557/training_args.py>>>\nfrom transformers import TrainingArguments\n\nargs = TrainingArguments(\n    output_dir=\"out\",\n    per_device_train_batch_size=1,\n    num_train_epochs=1,\n)\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai557/training_args.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["gradient_accumulation_steps"]}, "code": "from transformers import TrainingArguments\n\nargs = TrainingArguments(\n    output_dir=\"out\",\n    per_device_train_batch_size=1,\n    gradient_accumulation_steps=8,\n    num_train_epochs=1,\n    logging_steps=10,\n    eval_strategy=\"steps\",\n    eval_steps=50,\n    save_steps=50,\n    lr_scheduler_type=\"cosine\",\n    warmup_ratio=0.03,\n    fp16=False, bf16=True,\n    gradient_checkpointing=True,\n    max_grad_norm=1.0,\n)\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nGPU bf16 지원 시 dtype 선택 개선\n\n[CONTEXT]\n<<<FILE examples/ai540/train_utils.py>>>\nimport torch\n\n# BUG: dtype가 고정되어 혼합정밀에 불리\ndef get_dtype():\n    return torch.float16\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai540/train_utils.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["torch.bfloat16"]}, "code": "import torch\n\n\ndef get_dtype():\n    \"\"\"GPU가 bf16 지원 시 bfloat16, 아니면 float16 반환\"\"\"\n    if torch.cuda.is_available() and torch.cuda.is_bf16_supported():\n        return torch.bfloat16\n    return torch.float16\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n재현성: 시드 함수 추가\n\n[CONTEXT]\n<<<FILE examples/ai316/seed_utils.py>>>\n# BUG: 시드 미설정\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai316/seed_utils.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["set_seed("]}, "code": "import os, random, numpy as np, torch\n\ndef set_seed(seed: int = 42):\n    random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n훈련 안정화: GA steps/로깅/평가/체크포인팅/스케줄러\n\n[CONTEXT]\n<<<FILE examples/ai360/training_args.py>>>\nfrom transformers import TrainingArguments\n\nargs = TrainingArguments(\n    output_dir=\"out\",\n    per_device_train_batch_size=1,\n    num_train_epochs=1,\n)\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai360/training_args.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["gradient_accumulation_steps"]}, "code": "from transformers import TrainingArguments\n\nargs = TrainingArguments(\n    output_dir=\"out\",\n    per_device_train_batch_size=1,\n    gradient_accumulation_steps=8,\n    num_train_epochs=1,\n    logging_steps=10,\n    eval_strategy=\"steps\",\n    eval_steps=50,\n    save_steps=50,\n    lr_scheduler_type=\"cosine\",\n    warmup_ratio=0.03,\n    fp16=False, bf16=True,\n    gradient_checkpointing=True,\n    max_grad_norm=1.0,\n)\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nGPU bf16 지원 시 dtype 선택 개선\n\n[CONTEXT]\n<<<FILE examples/ai17/train_utils.py>>>\nimport torch\n\n# BUG: dtype가 고정되어 혼합정밀에 불리\ndef get_dtype():\n    return torch.float16\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai17/train_utils.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["torch.bfloat16"]}, "code": "import torch\n\n\ndef get_dtype():\n    \"\"\"GPU가 bf16 지원 시 bfloat16, 아니면 float16 반환\"\"\"\n    if torch.cuda.is_available() and torch.cuda.is_bf16_supported():\n        return torch.bfloat16\n    return torch.float16\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n필수 import 추가\n\n[CONTEXT]\n<<<FILE examples/ai166/imports.py>>>\n# BUG: numpy 미사용이지만 추후 사용 예정\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai166/imports.py", "loc": {"type": "anchor", "before": "\\A"}, "action": "insert_after", "once": true, "pre": {"must_not_contain": ["import numpy as np"]}, "code": "import numpy as np\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n체크포인트 재개 지원\n\n[CONTEXT]\n<<<FILE examples/ai57/trainer_resume.py>>>\nfrom transformers import Trainer\ntrainer = Trainer(... )\n# BUG: 재시작 미지원\ntrainer.train()\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai57/trainer_resume.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["resume_from_checkpoint"]}, "code": "from transformers import Trainer\ntrainer = Trainer(... )\ntrainer.train(resume_from_checkpoint=True)\n"}]}}
{"instruction": "Propose a minimal JSON plan for code changes. Output PLAN JSON only.", "input": "[INTENT]\n입출력/런타임 안전성 개선\n\n[CONTEXT]\n<<<FILE examples/ai742/dataloader_main.py>>>\nloader = DataLoader(...); for b in loader: pass\n\n<<<END>>>\n<<<FILE examples/ai742/ddp_main.py>>>\ntrainer()\n\n<<<END>>>", "output": {"files": [{"path": "examples/ai742/dataloader_main.py", "reason": "Windows freeze 가능", "strategy": "regex", "tests": ["__main__ 가드"]}, {"path": "examples/ai742/ddp_main.py", "reason": "DDP 가드 없음", "strategy": "regex", "tests": ["__main__"]}], "notes": "spawn 보호"}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nLoRA target_modules 확장\n\n[CONTEXT]\n<<<FILE examples/ai594/lora_cfg.py>>>\nfrom peft import LoraConfig\n\nLC = LoraConfig(r=8, target_modules=[\n    \"q_proj\", \"v_proj\"  # BUG: 부족\n])\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai594/lora_cfg.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["k_proj", "o_proj"]}, "code": "from peft import LoraConfig\n\nLC = LoraConfig(r=8, target_modules=[\n    \"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"\n])\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n훈련 안정화: GA steps/로깅/평가/체크포인팅/스케줄러\n\n[CONTEXT]\n<<<FILE examples/ai144/training_args.py>>>\nfrom transformers import TrainingArguments\n\nargs = TrainingArguments(\n    output_dir=\"out\",\n    per_device_train_batch_size=1,\n    num_train_epochs=1,\n)\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai144/training_args.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["gradient_accumulation_steps"]}, "code": "from transformers import TrainingArguments\n\nargs = TrainingArguments(\n    output_dir=\"out\",\n    per_device_train_batch_size=1,\n    gradient_accumulation_steps=8,\n    num_train_epochs=1,\n    logging_steps=10,\n    eval_strategy=\"steps\",\n    eval_steps=50,\n    save_steps=50,\n    lr_scheduler_type=\"cosine\",\n    warmup_ratio=0.03,\n    fp16=False, bf16=True,\n    gradient_checkpointing=True,\n    max_grad_norm=1.0,\n)\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nDDP/torchrun 안전 가드\n\n[CONTEXT]\n<<<FILE examples/ai568/ddp_main.py>>>\nimport torch\n\n# BUG: torchrun에서 main 가드 없음\ntrainer()\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai568/ddp_main.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["__main__"]}, "code": "import torch\n\n\ndef main():\n    trainer()\n\n\nif __name__ == \"__main__\":\n    main()\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nLoRA target_modules 확장\n\n[CONTEXT]\n<<<FILE examples/ai7/lora_cfg.py>>>\nfrom peft import LoraConfig\n\nLC = LoraConfig(r=8, target_modules=[\n    \"q_proj\", \"v_proj\"  # BUG: 부족\n])\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai7/lora_cfg.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["k_proj", "o_proj"]}, "code": "from peft import LoraConfig\n\nLC = LoraConfig(r=8, target_modules=[\n    \"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"\n])\n"}]}}
{"instruction": "Propose a minimal JSON plan for code changes. Output PLAN JSON only.", "input": "[INTENT]\n입출력/런타임 안전성 개선\n\n[CONTEXT]\n<<<FILE examples/ai757/dataloader_main.py>>>\nloader = DataLoader(...); for b in loader: pass\n\n<<<END>>>\n<<<FILE examples/ai757/ddp_main.py>>>\ntrainer()\n\n<<<END>>>", "output": {"files": [{"path": "examples/ai757/dataloader_main.py", "reason": "Windows freeze 가능", "strategy": "regex", "tests": ["__main__ 가드"]}, {"path": "examples/ai757/ddp_main.py", "reason": "DDP 가드 없음", "strategy": "regex", "tests": ["__main__"]}], "notes": "spawn 보호"}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nGPU bf16 지원 시 dtype 선택 개선\n\n[CONTEXT]\n<<<FILE examples/ai60/train_utils.py>>>\nimport torch\n\n# BUG: dtype가 고정되어 혼합정밀에 불리\ndef get_dtype():\n    return torch.float16\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai60/train_utils.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["torch.bfloat16"]}, "code": "import torch\n\n\ndef get_dtype():\n    \"\"\"GPU가 bf16 지원 시 bfloat16, 아니면 float16 반환\"\"\"\n    if torch.cuda.is_available() and torch.cuda.is_bf16_supported():\n        return torch.bfloat16\n    return torch.float16\n"}]}}
{"instruction": "Propose a minimal JSON plan for code changes. Output PLAN JSON only.", "input": "[INTENT]\n학습 안정화(AMP+훈련 설정)\n\n[CONTEXT]\n<<<FILE examples/ai661/train_utils.py>>>\ndef get_dtype():\n    return torch.float16\n\n<<<END>>>\n<<<FILE examples/ai661/training_args.py>>>\nargs = TrainingArguments(output_dir='out', per_device_train_batch_size=1, num_train_epochs=1)\n\n<<<END>>>", "output": {"files": [{"path": "examples/ai661/train_utils.py", "reason": "dtype가 고정 float16", "strategy": "regex", "tests": ["bf16 또는 fp16 반환"]}, {"path": "examples/ai661/training_args.py", "reason": "로깅/평가/GA 설정 누락", "strategy": "regex", "tests": ["eval_strategy=='steps'"]}], "notes": "bf16 지원 시 우선"}}
{"instruction": "Propose a minimal JSON plan for code changes. Output PLAN JSON only.", "input": "[INTENT]\n학습 안정화(AMP+훈련 설정)\n\n[CONTEXT]\n<<<FILE examples/ai698/train_utils.py>>>\ndef get_dtype():\n    return torch.float16\n\n<<<END>>>\n<<<FILE examples/ai698/training_args.py>>>\nargs = TrainingArguments(output_dir='out', per_device_train_batch_size=1, num_train_epochs=1)\n\n<<<END>>>", "output": {"files": [{"path": "examples/ai698/train_utils.py", "reason": "dtype가 고정 float16", "strategy": "regex", "tests": ["bf16 또는 fp16 반환"]}, {"path": "examples/ai698/training_args.py", "reason": "로깅/평가/GA 설정 누락", "strategy": "regex", "tests": ["eval_strategy=='steps'"]}], "notes": "bf16 지원 시 우선"}}
{"instruction": "Propose a minimal JSON plan for code changes. Output PLAN JSON only.", "input": "[INTENT]\n입출력/런타임 안전성 개선\n\n[CONTEXT]\n<<<FILE examples/ai690/dataloader_main.py>>>\nloader = DataLoader(...); for b in loader: pass\n\n<<<END>>>\n<<<FILE examples/ai690/ddp_main.py>>>\ntrainer()\n\n<<<END>>>", "output": {"files": [{"path": "examples/ai690/dataloader_main.py", "reason": "Windows freeze 가능", "strategy": "regex", "tests": ["__main__ 가드"]}, {"path": "examples/ai690/ddp_main.py", "reason": "DDP 가드 없음", "strategy": "regex", "tests": ["__main__"]}], "notes": "spawn 보호"}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nWindows에서 DataLoader 프리즈 방지 + pin_memory/persistent_workers\n\n[CONTEXT]\n<<<FILE examples/ai164/dataloader_main.py>>>\nimport torch, torch.utils.data as tud\n\nclass Dummy(torch.utils.data.Dataset):\n    def __len__(self): return 8\n    def __getitem__(self, i): return i\n\n# BUG: 모듈 임포트만으로 워커가 스폰되어 Windows에서 프리즈\nloader = tud.DataLoader(Dummy(), batch_size=2, num_workers=4)\nfor b in loader:\n    pass\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai164/dataloader_main.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["if __name__ == \"__main__\":"]}, "code": "import torch, torch.utils.data as tud\n\nclass Dummy(torch.utils.data.Dataset):\n    def __len__(self): return 8\n    def __getitem__(self, i): return i\n\n\ndef main():\n    loader = tud.DataLoader(Dummy(), batch_size=2, num_workers=4, pin_memory=True, persistent_workers=True)\n    for _ in loader:\n        pass\n\n\nif __name__ == \"__main__\":\n    main()\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nGPU bf16 지원 시 dtype 선택 개선\n\n[CONTEXT]\n<<<FILE examples/ai527/train_utils.py>>>\nimport torch\n\n# BUG: dtype가 고정되어 혼합정밀에 불리\ndef get_dtype():\n    return torch.float16\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai527/train_utils.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["torch.bfloat16"]}, "code": "import torch\n\n\ndef get_dtype():\n    \"\"\"GPU가 bf16 지원 시 bfloat16, 아니면 float16 반환\"\"\"\n    if torch.cuda.is_available() and torch.cuda.is_bf16_supported():\n        return torch.bfloat16\n    return torch.float16\n"}]}}
{"instruction": "Propose a minimal JSON plan for code changes. Output PLAN JSON only.", "input": "[INTENT]\n학습 안정화(AMP+훈련 설정)\n\n[CONTEXT]\n<<<FILE examples/ai703/train_utils.py>>>\ndef get_dtype():\n    return torch.float16\n\n<<<END>>>\n<<<FILE examples/ai703/training_args.py>>>\nargs = TrainingArguments(output_dir='out', per_device_train_batch_size=1, num_train_epochs=1)\n\n<<<END>>>", "output": {"files": [{"path": "examples/ai703/train_utils.py", "reason": "dtype가 고정 float16", "strategy": "regex", "tests": ["bf16 또는 fp16 반환"]}, {"path": "examples/ai703/training_args.py", "reason": "로깅/평가/GA 설정 누락", "strategy": "regex", "tests": ["eval_strategy=='steps'"]}], "notes": "bf16 지원 시 우선"}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n훈련 안정화: GA steps/로깅/평가/체크포인팅/스케줄러\n\n[CONTEXT]\n<<<FILE examples/ai456/training_args.py>>>\nfrom transformers import TrainingArguments\n\nargs = TrainingArguments(\n    output_dir=\"out\",\n    per_device_train_batch_size=1,\n    num_train_epochs=1,\n)\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai456/training_args.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["gradient_accumulation_steps"]}, "code": "from transformers import TrainingArguments\n\nargs = TrainingArguments(\n    output_dir=\"out\",\n    per_device_train_batch_size=1,\n    gradient_accumulation_steps=8,\n    num_train_epochs=1,\n    logging_steps=10,\n    eval_strategy=\"steps\",\n    eval_steps=50,\n    save_steps=50,\n    lr_scheduler_type=\"cosine\",\n    warmup_ratio=0.03,\n    fp16=False, bf16=True,\n    gradient_checkpointing=True,\n    max_grad_norm=1.0,\n)\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\npad 토큰 ID 정확히 설정\n\n[CONTEXT]\n<<<FILE examples/ai529/label_cfg.py>>>\nfrom transformers import AutoTokenizer\nfrom dataclasses import dataclass\n\n@dataclass\nclass Cfg:\n    pad_id: int = -100  # BUG\n\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai529/label_cfg.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["CFG = Cfg"]}, "code": "from transformers import AutoTokenizer\nfrom dataclasses import dataclass\n\n@dataclass\nclass Cfg:\n    pad_id: int | None = None\n\n# tokenizer 로드 후 설정\ntok = AutoTokenizer.from_pretrained(\"Qwen/Qwen3-8B\", use_fast=True)\nCFG = Cfg(pad_id=tok.pad_token_id)\n"}]}}
{"instruction": "Propose a minimal JSON plan for code changes. Output PLAN JSON only.", "input": "[INTENT]\n학습 안정화(AMP+훈련 설정)\n\n[CONTEXT]\n<<<FILE examples/ai797/train_utils.py>>>\ndef get_dtype():\n    return torch.float16\n\n<<<END>>>\n<<<FILE examples/ai797/training_args.py>>>\nargs = TrainingArguments(output_dir='out', per_device_train_batch_size=1, num_train_epochs=1)\n\n<<<END>>>", "output": {"files": [{"path": "examples/ai797/train_utils.py", "reason": "dtype가 고정 float16", "strategy": "regex", "tests": ["bf16 또는 fp16 반환"]}, {"path": "examples/ai797/training_args.py", "reason": "로깅/평가/GA 설정 누락", "strategy": "regex", "tests": ["eval_strategy=='steps'"]}], "notes": "bf16 지원 시 우선"}}
{"instruction": "Propose a minimal JSON plan for code changes. Output PLAN JSON only.", "input": "[INTENT]\n학습 안정화(AMP+훈련 설정)\n\n[CONTEXT]\n<<<FILE examples/ai728/train_utils.py>>>\ndef get_dtype():\n    return torch.float16\n\n<<<END>>>\n<<<FILE examples/ai728/training_args.py>>>\nargs = TrainingArguments(output_dir='out', per_device_train_batch_size=1, num_train_epochs=1)\n\n<<<END>>>", "output": {"files": [{"path": "examples/ai728/train_utils.py", "reason": "dtype가 고정 float16", "strategy": "regex", "tests": ["bf16 또는 fp16 반환"]}, {"path": "examples/ai728/training_args.py", "reason": "로깅/평가/GA 설정 누락", "strategy": "regex", "tests": ["eval_strategy=='steps'"]}], "notes": "bf16 지원 시 우선"}}
{"instruction": "Propose a minimal JSON plan for code changes. Output PLAN JSON only.", "input": "[INTENT]\n입출력/런타임 안전성 개선\n\n[CONTEXT]\n<<<FILE examples/ai648/dataloader_main.py>>>\nloader = DataLoader(...); for b in loader: pass\n\n<<<END>>>\n<<<FILE examples/ai648/ddp_main.py>>>\ntrainer()\n\n<<<END>>>", "output": {"files": [{"path": "examples/ai648/dataloader_main.py", "reason": "Windows freeze 가능", "strategy": "regex", "tests": ["__main__ 가드"]}, {"path": "examples/ai648/ddp_main.py", "reason": "DDP 가드 없음", "strategy": "regex", "tests": ["__main__"]}], "notes": "spawn 보호"}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\npad 토큰 ID 정확히 설정\n\n[CONTEXT]\n<<<FILE examples/ai95/label_cfg.py>>>\nfrom transformers import AutoTokenizer\nfrom dataclasses import dataclass\n\n@dataclass\nclass Cfg:\n    pad_id: int = -100  # BUG\n\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai95/label_cfg.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["CFG = Cfg"]}, "code": "from transformers import AutoTokenizer\nfrom dataclasses import dataclass\n\n@dataclass\nclass Cfg:\n    pad_id: int | None = None\n\n# tokenizer 로드 후 설정\ntok = AutoTokenizer.from_pretrained(\"Qwen/Qwen3-8B\", use_fast=True)\nCFG = Cfg(pad_id=tok.pad_token_id)\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nWindows에서 DataLoader 프리즈 방지 + pin_memory/persistent_workers\n\n[CONTEXT]\n<<<FILE examples/ai630/dataloader_main.py>>>\nimport torch, torch.utils.data as tud\n\nclass Dummy(torch.utils.data.Dataset):\n    def __len__(self): return 8\n    def __getitem__(self, i): return i\n\n# BUG: 모듈 임포트만으로 워커가 스폰되어 Windows에서 프리즈\nloader = tud.DataLoader(Dummy(), batch_size=2, num_workers=4)\nfor b in loader:\n    pass\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai630/dataloader_main.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["if __name__ == \"__main__\":"]}, "code": "import torch, torch.utils.data as tud\n\nclass Dummy(torch.utils.data.Dataset):\n    def __len__(self): return 8\n    def __getitem__(self, i): return i\n\n\ndef main():\n    loader = tud.DataLoader(Dummy(), batch_size=2, num_workers=4, pin_memory=True, persistent_workers=True)\n    for _ in loader:\n        pass\n\n\nif __name__ == \"__main__\":\n    main()\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nLoRA target_modules 확장\n\n[CONTEXT]\n<<<FILE examples/ai32/lora_cfg.py>>>\nfrom peft import LoraConfig\n\nLC = LoraConfig(r=8, target_modules=[\n    \"q_proj\", \"v_proj\"  # BUG: 부족\n])\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai32/lora_cfg.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["k_proj", "o_proj"]}, "code": "from peft import LoraConfig\n\nLC = LoraConfig(r=8, target_modules=[\n    \"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"\n])\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n학습 후 어댑터 저장 추가\n\n[CONTEXT]\n<<<FILE examples/ai138/save_after_train.py>>>\nfrom peft import PeftModel\n# ... 학습 후\n# BUG: 저장 누락\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai138/save_after_train.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["save_pretrained"]}, "code": "from peft import PeftModel\n# ... 학습 후\nmodel.save_pretrained(\"training/qlora-out/export\")\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nflash 미지원 시 sdpa 안전 경로\n\n[CONTEXT]\n<<<FILE examples/ai262/model_load.py>>>\nfrom transformers import AutoModelForCausalLM\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"Qwen/Qwen3-8B\",\n)\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai262/model_load.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["attn_implementation"]}, "code": "from transformers import AutoModelForCausalLM\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"Qwen/Qwen3-8B\",\n    attn_implementation=\"sdpa\",\n)\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n조기 종료 콜백 추가\n\n[CONTEXT]\n<<<FILE examples/ai4/callbacks.py>>>\nfrom transformers import TrainerCallback\n\nclass MyCB(TrainerCallback):\n    pass\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai4/callbacks.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["EarlyStoppingCallback"]}, "code": "from transformers import TrainerCallback, EarlyStoppingCallback\n\nclass MyCB(TrainerCallback):\n    pass\n\nCALLBACKS = [EarlyStoppingCallback(early_stopping_patience=2)]\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nGPU bf16 지원 시 dtype 선택 개선\n\n[CONTEXT]\n<<<FILE examples/ai581/train_utils.py>>>\nimport torch\n\n# BUG: dtype가 고정되어 혼합정밀에 불리\ndef get_dtype():\n    return torch.float16\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai581/train_utils.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["torch.bfloat16"]}, "code": "import torch\n\n\ndef get_dtype():\n    \"\"\"GPU가 bf16 지원 시 bfloat16, 아니면 float16 반환\"\"\"\n    if torch.cuda.is_available() and torch.cuda.is_bf16_supported():\n        return torch.bfloat16\n    return torch.float16\n"}]}}
{"instruction": "Propose a minimal JSON plan for code changes. Output PLAN JSON only.", "input": "[INTENT]\n입출력/런타임 안전성 개선\n\n[CONTEXT]\n<<<FILE examples/ai771/dataloader_main.py>>>\nloader = DataLoader(...); for b in loader: pass\n\n<<<END>>>\n<<<FILE examples/ai771/ddp_main.py>>>\ntrainer()\n\n<<<END>>>", "output": {"files": [{"path": "examples/ai771/dataloader_main.py", "reason": "Windows freeze 가능", "strategy": "regex", "tests": ["__main__ 가드"]}, {"path": "examples/ai771/ddp_main.py", "reason": "DDP 가드 없음", "strategy": "regex", "tests": ["__main__"]}], "notes": "spawn 보호"}}
{"instruction": "Propose a minimal JSON plan for code changes. Output PLAN JSON only.", "input": "[INTENT]\n입출력/런타임 안전성 개선\n\n[CONTEXT]\n<<<FILE examples/ai675/dataloader_main.py>>>\nloader = DataLoader(...); for b in loader: pass\n\n<<<END>>>\n<<<FILE examples/ai675/ddp_main.py>>>\ntrainer()\n\n<<<END>>>", "output": {"files": [{"path": "examples/ai675/dataloader_main.py", "reason": "Windows freeze 가능", "strategy": "regex", "tests": ["__main__ 가드"]}, {"path": "examples/ai675/ddp_main.py", "reason": "DDP 가드 없음", "strategy": "regex", "tests": ["__main__"]}], "notes": "spawn 보호"}}
{"instruction": "Propose a minimal JSON plan for code changes. Output PLAN JSON only.", "input": "[INTENT]\n입출력/런타임 안전성 개선\n\n[CONTEXT]\n<<<FILE examples/ai657/dataloader_main.py>>>\nloader = DataLoader(...); for b in loader: pass\n\n<<<END>>>\n<<<FILE examples/ai657/ddp_main.py>>>\ntrainer()\n\n<<<END>>>", "output": {"files": [{"path": "examples/ai657/dataloader_main.py", "reason": "Windows freeze 가능", "strategy": "regex", "tests": ["__main__ 가드"]}, {"path": "examples/ai657/ddp_main.py", "reason": "DDP 가드 없음", "strategy": "regex", "tests": ["__main__"]}], "notes": "spawn 보호"}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nflash 미지원 시 sdpa 안전 경로\n\n[CONTEXT]\n<<<FILE examples/ai143/model_load.py>>>\nfrom transformers import AutoModelForCausalLM\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"Qwen/Qwen3-8B\",\n)\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai143/model_load.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["attn_implementation"]}, "code": "from transformers import AutoModelForCausalLM\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"Qwen/Qwen3-8B\",\n    attn_implementation=\"sdpa\",\n)\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nLoRA target_modules 확장\n\n[CONTEXT]\n<<<FILE examples/ai633/lora_cfg.py>>>\nfrom peft import LoraConfig\n\nLC = LoraConfig(r=8, target_modules=[\n    \"q_proj\", \"v_proj\"  # BUG: 부족\n])\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai633/lora_cfg.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["k_proj", "o_proj"]}, "code": "from peft import LoraConfig\n\nLC = LoraConfig(r=8, target_modules=[\n    \"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"\n])\n"}]}}
{"instruction": "Propose a minimal JSON plan for code changes. Output PLAN JSON only.", "input": "[INTENT]\n학습 안정화(AMP+훈련 설정)\n\n[CONTEXT]\n<<<FILE examples/ai683/train_utils.py>>>\ndef get_dtype():\n    return torch.float16\n\n<<<END>>>\n<<<FILE examples/ai683/training_args.py>>>\nargs = TrainingArguments(output_dir='out', per_device_train_batch_size=1, num_train_epochs=1)\n\n<<<END>>>", "output": {"files": [{"path": "examples/ai683/train_utils.py", "reason": "dtype가 고정 float16", "strategy": "regex", "tests": ["bf16 또는 fp16 반환"]}, {"path": "examples/ai683/training_args.py", "reason": "로깅/평가/GA 설정 누락", "strategy": "regex", "tests": ["eval_strategy=='steps'"]}], "notes": "bf16 지원 시 우선"}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\npad 토큰 ID 정확히 설정\n\n[CONTEXT]\n<<<FILE examples/ai509/label_cfg.py>>>\nfrom transformers import AutoTokenizer\nfrom dataclasses import dataclass\n\n@dataclass\nclass Cfg:\n    pad_id: int = -100  # BUG\n\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai509/label_cfg.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["CFG = Cfg"]}, "code": "from transformers import AutoTokenizer\nfrom dataclasses import dataclass\n\n@dataclass\nclass Cfg:\n    pad_id: int | None = None\n\n# tokenizer 로드 후 설정\ntok = AutoTokenizer.from_pretrained(\"Qwen/Qwen3-8B\", use_fast=True)\nCFG = Cfg(pad_id=tok.pad_token_id)\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n재현성: 시드 함수 추가\n\n[CONTEXT]\n<<<FILE examples/ai173/seed_utils.py>>>\n# BUG: 시드 미설정\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai173/seed_utils.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["set_seed("]}, "code": "import os, random, numpy as np, torch\n\ndef set_seed(seed: int = 42):\n    random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n학습 후 어댑터 저장 추가\n\n[CONTEXT]\n<<<FILE examples/ai498/save_after_train.py>>>\nfrom peft import PeftModel\n# ... 학습 후\n# BUG: 저장 누락\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai498/save_after_train.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["save_pretrained"]}, "code": "from peft import PeftModel\n# ... 학습 후\nmodel.save_pretrained(\"training/qlora-out/export\")\n"}]}}
{"instruction": "Propose a minimal JSON plan for code changes. Output PLAN JSON only.", "input": "[INTENT]\n학습 안정화(AMP+훈련 설정)\n\n[CONTEXT]\n<<<FILE examples/ai682/train_utils.py>>>\ndef get_dtype():\n    return torch.float16\n\n<<<END>>>\n<<<FILE examples/ai682/training_args.py>>>\nargs = TrainingArguments(output_dir='out', per_device_train_batch_size=1, num_train_epochs=1)\n\n<<<END>>>", "output": {"files": [{"path": "examples/ai682/train_utils.py", "reason": "dtype가 고정 float16", "strategy": "regex", "tests": ["bf16 또는 fp16 반환"]}, {"path": "examples/ai682/training_args.py", "reason": "로깅/평가/GA 설정 누락", "strategy": "regex", "tests": ["eval_strategy=='steps'"]}], "notes": "bf16 지원 시 우선"}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n재현성: 시드 함수 추가\n\n[CONTEXT]\n<<<FILE examples/ai366/seed_utils.py>>>\n# BUG: 시드 미설정\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai366/seed_utils.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["set_seed("]}, "code": "import os, random, numpy as np, torch\n\ndef set_seed(seed: int = 42):\n    random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nflash 미지원 시 sdpa 안전 경로\n\n[CONTEXT]\n<<<FILE examples/ai58/model_load.py>>>\nfrom transformers import AutoModelForCausalLM\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"Qwen/Qwen3-8B\",\n)\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai58/model_load.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["attn_implementation"]}, "code": "from transformers import AutoModelForCausalLM\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"Qwen/Qwen3-8B\",\n    attn_implementation=\"sdpa\",\n)\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nAMP 적용 + bf16 우선\n\n[CONTEXT]\n<<<FILE examples/ai520/train_step.py>>>\nimport torch\n\n# BUG: autocast 미사용\ndef train_step(model, batch):\n    x, y = batch\n    out = model(x)\n    loss = out.loss if hasattr(out, 'loss') else out.mean()\n    loss.backward()\n    return loss.item()\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai520/train_step.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["autocast"]}, "code": "import torch\n\n@torch.no_grad()\ndef _bf16_supported():\n    return torch.cuda.is_available() and torch.cuda.is_bf16_supported()\n\n\ndef train_step(model, batch):\n    x, y = batch\n    amp_dtype = torch.bfloat16 if _bf16_supported() else torch.float16\n    scaler = torch.cuda.amp.GradScaler(enabled=True)\n    with torch.cuda.amp.autocast(enabled=True, dtype=amp_dtype):\n        out = model(x)\n        loss = out.loss if hasattr(out, 'loss') else out.mean()\n    scaler.scale(loss).backward()\n    scaler.step(torch.optim.Adam(model.parameters(), lr=1e-4))\n    scaler.update()\n    model.zero_grad(set_to_none=True)\n    return float(loss.detach().item())\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n8bit 옵티마이저 + 코사인 스케줄러\n\n[CONTEXT]\n<<<FILE examples/ai37/optim.py>>>\nimport torch\nopt = torch.optim.Adam([], lr=3e-4)\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai37/optim.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["PagedAdamW8bit"]}, "code": "from transformers import get_scheduler\nfrom bitsandbytes.optim import PagedAdamW8bit\nopt = PagedAdamW8bit([], lr=2e-4)\nsched = get_scheduler(\"cosine\", optimizer=opt, num_warmup_steps=100, num_training_steps=1000)\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nLoRA target_modules 확장\n\n[CONTEXT]\n<<<FILE examples/ai20/lora_cfg.py>>>\nfrom peft import LoraConfig\n\nLC = LoraConfig(r=8, target_modules=[\n    \"q_proj\", \"v_proj\"  # BUG: 부족\n])\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai20/lora_cfg.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["k_proj", "o_proj"]}, "code": "from peft import LoraConfig\n\nLC = LoraConfig(r=8, target_modules=[\n    \"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"\n])\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n훈련 안정화: GA steps/로깅/평가/체크포인팅/스케줄러\n\n[CONTEXT]\n<<<FILE examples/ai403/training_args.py>>>\nfrom transformers import TrainingArguments\n\nargs = TrainingArguments(\n    output_dir=\"out\",\n    per_device_train_batch_size=1,\n    num_train_epochs=1,\n)\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai403/training_args.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["gradient_accumulation_steps"]}, "code": "from transformers import TrainingArguments\n\nargs = TrainingArguments(\n    output_dir=\"out\",\n    per_device_train_batch_size=1,\n    gradient_accumulation_steps=8,\n    num_train_epochs=1,\n    logging_steps=10,\n    eval_strategy=\"steps\",\n    eval_steps=50,\n    save_steps=50,\n    lr_scheduler_type=\"cosine\",\n    warmup_ratio=0.03,\n    fp16=False, bf16=True,\n    gradient_checkpointing=True,\n    max_grad_norm=1.0,\n)\n"}]}}
{"instruction": "Propose a minimal JSON plan for code changes. Output PLAN JSON only.", "input": "[INTENT]\n학습 안정화(AMP+훈련 설정)\n\n[CONTEXT]\n<<<FILE examples/ai795/train_utils.py>>>\ndef get_dtype():\n    return torch.float16\n\n<<<END>>>\n<<<FILE examples/ai795/training_args.py>>>\nargs = TrainingArguments(output_dir='out', per_device_train_batch_size=1, num_train_epochs=1)\n\n<<<END>>>", "output": {"files": [{"path": "examples/ai795/train_utils.py", "reason": "dtype가 고정 float16", "strategy": "regex", "tests": ["bf16 또는 fp16 반환"]}, {"path": "examples/ai795/training_args.py", "reason": "로깅/평가/GA 설정 누락", "strategy": "regex", "tests": ["eval_strategy=='steps'"]}], "notes": "bf16 지원 시 우선"}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n8bit 옵티마이저 + 코사인 스케줄러\n\n[CONTEXT]\n<<<FILE examples/ai206/optim.py>>>\nimport torch\nopt = torch.optim.Adam([], lr=3e-4)\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai206/optim.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["PagedAdamW8bit"]}, "code": "from transformers import get_scheduler\nfrom bitsandbytes.optim import PagedAdamW8bit\nopt = PagedAdamW8bit([], lr=2e-4)\nsched = get_scheduler(\"cosine\", optimizer=opt, num_warmup_steps=100, num_training_steps=1000)\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nWindows에서 DataLoader 프리즈 방지 + pin_memory/persistent_workers\n\n[CONTEXT]\n<<<FILE examples/ai87/dataloader_main.py>>>\nimport torch, torch.utils.data as tud\n\nclass Dummy(torch.utils.data.Dataset):\n    def __len__(self): return 8\n    def __getitem__(self, i): return i\n\n# BUG: 모듈 임포트만으로 워커가 스폰되어 Windows에서 프리즈\nloader = tud.DataLoader(Dummy(), batch_size=2, num_workers=4)\nfor b in loader:\n    pass\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai87/dataloader_main.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["if __name__ == \"__main__\":"]}, "code": "import torch, torch.utils.data as tud\n\nclass Dummy(torch.utils.data.Dataset):\n    def __len__(self): return 8\n    def __getitem__(self, i): return i\n\n\ndef main():\n    loader = tud.DataLoader(Dummy(), batch_size=2, num_workers=4, pin_memory=True, persistent_workers=True)\n    for _ in loader:\n        pass\n\n\nif __name__ == \"__main__\":\n    main()\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n8bit 옵티마이저 + 코사인 스케줄러\n\n[CONTEXT]\n<<<FILE examples/ai347/optim.py>>>\nimport torch\nopt = torch.optim.Adam([], lr=3e-4)\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai347/optim.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["PagedAdamW8bit"]}, "code": "from transformers import get_scheduler\nfrom bitsandbytes.optim import PagedAdamW8bit\nopt = PagedAdamW8bit([], lr=2e-4)\nsched = get_scheduler(\"cosine\", optimizer=opt, num_warmup_steps=100, num_training_steps=1000)\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nAMP 적용 + bf16 우선\n\n[CONTEXT]\n<<<FILE examples/ai392/train_step.py>>>\nimport torch\n\n# BUG: autocast 미사용\ndef train_step(model, batch):\n    x, y = batch\n    out = model(x)\n    loss = out.loss if hasattr(out, 'loss') else out.mean()\n    loss.backward()\n    return loss.item()\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai392/train_step.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["autocast"]}, "code": "import torch\n\n@torch.no_grad()\ndef _bf16_supported():\n    return torch.cuda.is_available() and torch.cuda.is_bf16_supported()\n\n\ndef train_step(model, batch):\n    x, y = batch\n    amp_dtype = torch.bfloat16 if _bf16_supported() else torch.float16\n    scaler = torch.cuda.amp.GradScaler(enabled=True)\n    with torch.cuda.amp.autocast(enabled=True, dtype=amp_dtype):\n        out = model(x)\n        loss = out.loss if hasattr(out, 'loss') else out.mean()\n    scaler.scale(loss).backward()\n    scaler.step(torch.optim.Adam(model.parameters(), lr=1e-4))\n    scaler.update()\n    model.zero_grad(set_to_none=True)\n    return float(loss.detach().item())\n"}]}}
{"instruction": "Propose a minimal JSON plan for code changes. Output PLAN JSON only.", "input": "[INTENT]\n학습 안정화(AMP+훈련 설정)\n\n[CONTEXT]\n<<<FILE examples/ai790/train_utils.py>>>\ndef get_dtype():\n    return torch.float16\n\n<<<END>>>\n<<<FILE examples/ai790/training_args.py>>>\nargs = TrainingArguments(output_dir='out', per_device_train_batch_size=1, num_train_epochs=1)\n\n<<<END>>>", "output": {"files": [{"path": "examples/ai790/train_utils.py", "reason": "dtype가 고정 float16", "strategy": "regex", "tests": ["bf16 또는 fp16 반환"]}, {"path": "examples/ai790/training_args.py", "reason": "로깅/평가/GA 설정 누락", "strategy": "regex", "tests": ["eval_strategy=='steps'"]}], "notes": "bf16 지원 시 우선"}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n필수 import 추가\n\n[CONTEXT]\n<<<FILE examples/ai207/imports.py>>>\n# BUG: numpy 미사용이지만 추후 사용 예정\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai207/imports.py", "loc": {"type": "anchor", "before": "\\A"}, "action": "insert_after", "once": true, "pre": {"must_not_contain": ["import numpy as np"]}, "code": "import numpy as np\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n필수 import 추가\n\n[CONTEXT]\n<<<FILE examples/ai295/imports.py>>>\n# BUG: numpy 미사용이지만 추후 사용 예정\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai295/imports.py", "loc": {"type": "anchor", "before": "\\A"}, "action": "insert_after", "once": true, "pre": {"must_not_contain": ["import numpy as np"]}, "code": "import numpy as np\n"}]}}
{"instruction": "Propose a minimal JSON plan for code changes. Output PLAN JSON only.", "input": "[INTENT]\n학습 안정화(AMP+훈련 설정)\n\n[CONTEXT]\n<<<FILE examples/ai696/train_utils.py>>>\ndef get_dtype():\n    return torch.float16\n\n<<<END>>>\n<<<FILE examples/ai696/training_args.py>>>\nargs = TrainingArguments(output_dir='out', per_device_train_batch_size=1, num_train_epochs=1)\n\n<<<END>>>", "output": {"files": [{"path": "examples/ai696/train_utils.py", "reason": "dtype가 고정 float16", "strategy": "regex", "tests": ["bf16 또는 fp16 반환"]}, {"path": "examples/ai696/training_args.py", "reason": "로깅/평가/GA 설정 누락", "strategy": "regex", "tests": ["eval_strategy=='steps'"]}], "notes": "bf16 지원 시 우선"}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nLoRA target_modules 확장\n\n[CONTEXT]\n<<<FILE examples/ai132/lora_cfg.py>>>\nfrom peft import LoraConfig\n\nLC = LoraConfig(r=8, target_modules=[\n    \"q_proj\", \"v_proj\"  # BUG: 부족\n])\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai132/lora_cfg.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["k_proj", "o_proj"]}, "code": "from peft import LoraConfig\n\nLC = LoraConfig(r=8, target_modules=[\n    \"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"\n])\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nLoRA target_modules 확장\n\n[CONTEXT]\n<<<FILE examples/ai85/lora_cfg.py>>>\nfrom peft import LoraConfig\n\nLC = LoraConfig(r=8, target_modules=[\n    \"q_proj\", \"v_proj\"  # BUG: 부족\n])\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai85/lora_cfg.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["k_proj", "o_proj"]}, "code": "from peft import LoraConfig\n\nLC = LoraConfig(r=8, target_modules=[\n    \"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"\n])\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\npad 토큰 ID 정확히 설정\n\n[CONTEXT]\n<<<FILE examples/ai537/label_cfg.py>>>\nfrom transformers import AutoTokenizer\nfrom dataclasses import dataclass\n\n@dataclass\nclass Cfg:\n    pad_id: int = -100  # BUG\n\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai537/label_cfg.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["CFG = Cfg"]}, "code": "from transformers import AutoTokenizer\nfrom dataclasses import dataclass\n\n@dataclass\nclass Cfg:\n    pad_id: int | None = None\n\n# tokenizer 로드 후 설정\ntok = AutoTokenizer.from_pretrained(\"Qwen/Qwen3-8B\", use_fast=True)\nCFG = Cfg(pad_id=tok.pad_token_id)\n"}]}}
{"instruction": "Propose a minimal JSON plan for code changes. Output PLAN JSON only.", "input": "[INTENT]\n입출력/런타임 안전성 개선\n\n[CONTEXT]\n<<<FILE examples/ai646/dataloader_main.py>>>\nloader = DataLoader(...); for b in loader: pass\n\n<<<END>>>\n<<<FILE examples/ai646/ddp_main.py>>>\ntrainer()\n\n<<<END>>>", "output": {"files": [{"path": "examples/ai646/dataloader_main.py", "reason": "Windows freeze 가능", "strategy": "regex", "tests": ["__main__ 가드"]}, {"path": "examples/ai646/ddp_main.py", "reason": "DDP 가드 없음", "strategy": "regex", "tests": ["__main__"]}], "notes": "spawn 보호"}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nflash 미지원 시 sdpa 안전 경로\n\n[CONTEXT]\n<<<FILE examples/ai188/model_load.py>>>\nfrom transformers import AutoModelForCausalLM\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"Qwen/Qwen3-8B\",\n)\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai188/model_load.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["attn_implementation"]}, "code": "from transformers import AutoModelForCausalLM\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"Qwen/Qwen3-8B\",\n    attn_implementation=\"sdpa\",\n)\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n학습 후 어댑터 저장 추가\n\n[CONTEXT]\n<<<FILE examples/ai269/save_after_train.py>>>\nfrom peft import PeftModel\n# ... 학습 후\n# BUG: 저장 누락\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai269/save_after_train.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["save_pretrained"]}, "code": "from peft import PeftModel\n# ... 학습 후\nmodel.save_pretrained(\"training/qlora-out/export\")\n"}]}}
{"instruction": "Propose a minimal JSON plan for code changes. Output PLAN JSON only.", "input": "[INTENT]\n학습 안정화(AMP+훈련 설정)\n\n[CONTEXT]\n<<<FILE examples/ai654/train_utils.py>>>\ndef get_dtype():\n    return torch.float16\n\n<<<END>>>\n<<<FILE examples/ai654/training_args.py>>>\nargs = TrainingArguments(output_dir='out', per_device_train_batch_size=1, num_train_epochs=1)\n\n<<<END>>>", "output": {"files": [{"path": "examples/ai654/train_utils.py", "reason": "dtype가 고정 float16", "strategy": "regex", "tests": ["bf16 또는 fp16 반환"]}, {"path": "examples/ai654/training_args.py", "reason": "로깅/평가/GA 설정 누락", "strategy": "regex", "tests": ["eval_strategy=='steps'"]}], "notes": "bf16 지원 시 우선"}}
{"instruction": "Propose a minimal JSON plan for code changes. Output PLAN JSON only.", "input": "[INTENT]\n학습 안정화(AMP+훈련 설정)\n\n[CONTEXT]\n<<<FILE examples/ai718/train_utils.py>>>\ndef get_dtype():\n    return torch.float16\n\n<<<END>>>\n<<<FILE examples/ai718/training_args.py>>>\nargs = TrainingArguments(output_dir='out', per_device_train_batch_size=1, num_train_epochs=1)\n\n<<<END>>>", "output": {"files": [{"path": "examples/ai718/train_utils.py", "reason": "dtype가 고정 float16", "strategy": "regex", "tests": ["bf16 또는 fp16 반환"]}, {"path": "examples/ai718/training_args.py", "reason": "로깅/평가/GA 설정 누락", "strategy": "regex", "tests": ["eval_strategy=='steps'"]}], "notes": "bf16 지원 시 우선"}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\npad 토큰 ID 정확히 설정\n\n[CONTEXT]\n<<<FILE examples/ai434/label_cfg.py>>>\nfrom transformers import AutoTokenizer\nfrom dataclasses import dataclass\n\n@dataclass\nclass Cfg:\n    pad_id: int = -100  # BUG\n\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai434/label_cfg.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["CFG = Cfg"]}, "code": "from transformers import AutoTokenizer\nfrom dataclasses import dataclass\n\n@dataclass\nclass Cfg:\n    pad_id: int | None = None\n\n# tokenizer 로드 후 설정\ntok = AutoTokenizer.from_pretrained(\"Qwen/Qwen3-8B\", use_fast=True)\nCFG = Cfg(pad_id=tok.pad_token_id)\n"}]}}
{"instruction": "Propose a minimal JSON plan for code changes. Output PLAN JSON only.", "input": "[INTENT]\n학습 안정화(AMP+훈련 설정)\n\n[CONTEXT]\n<<<FILE examples/ai785/train_utils.py>>>\ndef get_dtype():\n    return torch.float16\n\n<<<END>>>\n<<<FILE examples/ai785/training_args.py>>>\nargs = TrainingArguments(output_dir='out', per_device_train_batch_size=1, num_train_epochs=1)\n\n<<<END>>>", "output": {"files": [{"path": "examples/ai785/train_utils.py", "reason": "dtype가 고정 float16", "strategy": "regex", "tests": ["bf16 또는 fp16 반환"]}, {"path": "examples/ai785/training_args.py", "reason": "로깅/평가/GA 설정 누락", "strategy": "regex", "tests": ["eval_strategy=='steps'"]}], "notes": "bf16 지원 시 우선"}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n8bit 옵티마이저 + 코사인 스케줄러\n\n[CONTEXT]\n<<<FILE examples/ai289/optim.py>>>\nimport torch\nopt = torch.optim.Adam([], lr=3e-4)\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai289/optim.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["PagedAdamW8bit"]}, "code": "from transformers import get_scheduler\nfrom bitsandbytes.optim import PagedAdamW8bit\nopt = PagedAdamW8bit([], lr=2e-4)\nsched = get_scheduler(\"cosine\", optimizer=opt, num_warmup_steps=100, num_training_steps=1000)\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n8bit 옵티마이저 + 코사인 스케줄러\n\n[CONTEXT]\n<<<FILE examples/ai348/optim.py>>>\nimport torch\nopt = torch.optim.Adam([], lr=3e-4)\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai348/optim.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["PagedAdamW8bit"]}, "code": "from transformers import get_scheduler\nfrom bitsandbytes.optim import PagedAdamW8bit\nopt = PagedAdamW8bit([], lr=2e-4)\nsched = get_scheduler(\"cosine\", optimizer=opt, num_warmup_steps=100, num_training_steps=1000)\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nflash 미지원 시 sdpa 안전 경로\n\n[CONTEXT]\n<<<FILE examples/ai487/model_load.py>>>\nfrom transformers import AutoModelForCausalLM\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"Qwen/Qwen3-8B\",\n)\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai487/model_load.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["attn_implementation"]}, "code": "from transformers import AutoModelForCausalLM\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"Qwen/Qwen3-8B\",\n    attn_implementation=\"sdpa\",\n)\n"}]}}
{"instruction": "Propose a minimal JSON plan for code changes. Output PLAN JSON only.", "input": "[INTENT]\n학습 안정화(AMP+훈련 설정)\n\n[CONTEXT]\n<<<FILE examples/ai767/train_utils.py>>>\ndef get_dtype():\n    return torch.float16\n\n<<<END>>>\n<<<FILE examples/ai767/training_args.py>>>\nargs = TrainingArguments(output_dir='out', per_device_train_batch_size=1, num_train_epochs=1)\n\n<<<END>>>", "output": {"files": [{"path": "examples/ai767/train_utils.py", "reason": "dtype가 고정 float16", "strategy": "regex", "tests": ["bf16 또는 fp16 반환"]}, {"path": "examples/ai767/training_args.py", "reason": "로깅/평가/GA 설정 누락", "strategy": "regex", "tests": ["eval_strategy=='steps'"]}], "notes": "bf16 지원 시 우선"}}
{"instruction": "Propose a minimal JSON plan for code changes. Output PLAN JSON only.", "input": "[INTENT]\n입출력/런타임 안전성 개선\n\n[CONTEXT]\n<<<FILE examples/ai691/dataloader_main.py>>>\nloader = DataLoader(...); for b in loader: pass\n\n<<<END>>>\n<<<FILE examples/ai691/ddp_main.py>>>\ntrainer()\n\n<<<END>>>", "output": {"files": [{"path": "examples/ai691/dataloader_main.py", "reason": "Windows freeze 가능", "strategy": "regex", "tests": ["__main__ 가드"]}, {"path": "examples/ai691/ddp_main.py", "reason": "DDP 가드 없음", "strategy": "regex", "tests": ["__main__"]}], "notes": "spawn 보호"}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nWindows에서 DataLoader 프리즈 방지 + pin_memory/persistent_workers\n\n[CONTEXT]\n<<<FILE examples/ai602/dataloader_main.py>>>\nimport torch, torch.utils.data as tud\n\nclass Dummy(torch.utils.data.Dataset):\n    def __len__(self): return 8\n    def __getitem__(self, i): return i\n\n# BUG: 모듈 임포트만으로 워커가 스폰되어 Windows에서 프리즈\nloader = tud.DataLoader(Dummy(), batch_size=2, num_workers=4)\nfor b in loader:\n    pass\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai602/dataloader_main.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["if __name__ == \"__main__\":"]}, "code": "import torch, torch.utils.data as tud\n\nclass Dummy(torch.utils.data.Dataset):\n    def __len__(self): return 8\n    def __getitem__(self, i): return i\n\n\ndef main():\n    loader = tud.DataLoader(Dummy(), batch_size=2, num_workers=4, pin_memory=True, persistent_workers=True)\n    for _ in loader:\n        pass\n\n\nif __name__ == \"__main__\":\n    main()\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n학습 후 어댑터 저장 추가\n\n[CONTEXT]\n<<<FILE examples/ai159/save_after_train.py>>>\nfrom peft import PeftModel\n# ... 학습 후\n# BUG: 저장 누락\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai159/save_after_train.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["save_pretrained"]}, "code": "from peft import PeftModel\n# ... 학습 후\nmodel.save_pretrained(\"training/qlora-out/export\")\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nLoRA target_modules 확장\n\n[CONTEXT]\n<<<FILE examples/ai21/lora_cfg.py>>>\nfrom peft import LoraConfig\n\nLC = LoraConfig(r=8, target_modules=[\n    \"q_proj\", \"v_proj\"  # BUG: 부족\n])\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai21/lora_cfg.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["k_proj", "o_proj"]}, "code": "from peft import LoraConfig\n\nLC = LoraConfig(r=8, target_modules=[\n    \"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"\n])\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nWindows에서 DataLoader 프리즈 방지 + pin_memory/persistent_workers\n\n[CONTEXT]\n<<<FILE examples/ai50/dataloader_main.py>>>\nimport torch, torch.utils.data as tud\n\nclass Dummy(torch.utils.data.Dataset):\n    def __len__(self): return 8\n    def __getitem__(self, i): return i\n\n# BUG: 모듈 임포트만으로 워커가 스폰되어 Windows에서 프리즈\nloader = tud.DataLoader(Dummy(), batch_size=2, num_workers=4)\nfor b in loader:\n    pass\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai50/dataloader_main.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["if __name__ == \"__main__\":"]}, "code": "import torch, torch.utils.data as tud\n\nclass Dummy(torch.utils.data.Dataset):\n    def __len__(self): return 8\n    def __getitem__(self, i): return i\n\n\ndef main():\n    loader = tud.DataLoader(Dummy(), batch_size=2, num_workers=4, pin_memory=True, persistent_workers=True)\n    for _ in loader:\n        pass\n\n\nif __name__ == \"__main__\":\n    main()\n"}]}}
{"instruction": "Propose a minimal JSON plan for code changes. Output PLAN JSON only.", "input": "[INTENT]\n입출력/런타임 안전성 개선\n\n[CONTEXT]\n<<<FILE examples/ai755/dataloader_main.py>>>\nloader = DataLoader(...); for b in loader: pass\n\n<<<END>>>\n<<<FILE examples/ai755/ddp_main.py>>>\ntrainer()\n\n<<<END>>>", "output": {"files": [{"path": "examples/ai755/dataloader_main.py", "reason": "Windows freeze 가능", "strategy": "regex", "tests": ["__main__ 가드"]}, {"path": "examples/ai755/ddp_main.py", "reason": "DDP 가드 없음", "strategy": "regex", "tests": ["__main__"]}], "notes": "spawn 보호"}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nDDP/torchrun 안전 가드\n\n[CONTEXT]\n<<<FILE examples/ai640/ddp_main.py>>>\nimport torch\n\n# BUG: torchrun에서 main 가드 없음\ntrainer()\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai640/ddp_main.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["__main__"]}, "code": "import torch\n\n\ndef main():\n    trainer()\n\n\nif __name__ == \"__main__\":\n    main()\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n훈련 안정화: GA steps/로깅/평가/체크포인팅/스케줄러\n\n[CONTEXT]\n<<<FILE examples/ai511/training_args.py>>>\nfrom transformers import TrainingArguments\n\nargs = TrainingArguments(\n    output_dir=\"out\",\n    per_device_train_batch_size=1,\n    num_train_epochs=1,\n)\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai511/training_args.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["gradient_accumulation_steps"]}, "code": "from transformers import TrainingArguments\n\nargs = TrainingArguments(\n    output_dir=\"out\",\n    per_device_train_batch_size=1,\n    gradient_accumulation_steps=8,\n    num_train_epochs=1,\n    logging_steps=10,\n    eval_strategy=\"steps\",\n    eval_steps=50,\n    save_steps=50,\n    lr_scheduler_type=\"cosine\",\n    warmup_ratio=0.03,\n    fp16=False, bf16=True,\n    gradient_checkpointing=True,\n    max_grad_norm=1.0,\n)\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nflash 미지원 시 sdpa 안전 경로\n\n[CONTEXT]\n<<<FILE examples/ai113/model_load.py>>>\nfrom transformers import AutoModelForCausalLM\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"Qwen/Qwen3-8B\",\n)\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai113/model_load.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["attn_implementation"]}, "code": "from transformers import AutoModelForCausalLM\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"Qwen/Qwen3-8B\",\n    attn_implementation=\"sdpa\",\n)\n"}]}}
{"instruction": "Propose a minimal JSON plan for code changes. Output PLAN JSON only.", "input": "[INTENT]\n학습 안정화(AMP+훈련 설정)\n\n[CONTEXT]\n<<<FILE examples/ai740/train_utils.py>>>\ndef get_dtype():\n    return torch.float16\n\n<<<END>>>\n<<<FILE examples/ai740/training_args.py>>>\nargs = TrainingArguments(output_dir='out', per_device_train_batch_size=1, num_train_epochs=1)\n\n<<<END>>>", "output": {"files": [{"path": "examples/ai740/train_utils.py", "reason": "dtype가 고정 float16", "strategy": "regex", "tests": ["bf16 또는 fp16 반환"]}, {"path": "examples/ai740/training_args.py", "reason": "로깅/평가/GA 설정 누락", "strategy": "regex", "tests": ["eval_strategy=='steps'"]}], "notes": "bf16 지원 시 우선"}}
{"instruction": "Propose a minimal JSON plan for code changes. Output PLAN JSON only.", "input": "[INTENT]\n입출력/런타임 안전성 개선\n\n[CONTEXT]\n<<<FILE examples/ai707/dataloader_main.py>>>\nloader = DataLoader(...); for b in loader: pass\n\n<<<END>>>\n<<<FILE examples/ai707/ddp_main.py>>>\ntrainer()\n\n<<<END>>>", "output": {"files": [{"path": "examples/ai707/dataloader_main.py", "reason": "Windows freeze 가능", "strategy": "regex", "tests": ["__main__ 가드"]}, {"path": "examples/ai707/ddp_main.py", "reason": "DDP 가드 없음", "strategy": "regex", "tests": ["__main__"]}], "notes": "spawn 보호"}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nAMP 적용 + bf16 우선\n\n[CONTEXT]\n<<<FILE examples/ai147/train_step.py>>>\nimport torch\n\n# BUG: autocast 미사용\ndef train_step(model, batch):\n    x, y = batch\n    out = model(x)\n    loss = out.loss if hasattr(out, 'loss') else out.mean()\n    loss.backward()\n    return loss.item()\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai147/train_step.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["autocast"]}, "code": "import torch\n\n@torch.no_grad()\ndef _bf16_supported():\n    return torch.cuda.is_available() and torch.cuda.is_bf16_supported()\n\n\ndef train_step(model, batch):\n    x, y = batch\n    amp_dtype = torch.bfloat16 if _bf16_supported() else torch.float16\n    scaler = torch.cuda.amp.GradScaler(enabled=True)\n    with torch.cuda.amp.autocast(enabled=True, dtype=amp_dtype):\n        out = model(x)\n        loss = out.loss if hasattr(out, 'loss') else out.mean()\n    scaler.scale(loss).backward()\n    scaler.step(torch.optim.Adam(model.parameters(), lr=1e-4))\n    scaler.update()\n    model.zero_grad(set_to_none=True)\n    return float(loss.detach().item())\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nAMP 적용 + bf16 우선\n\n[CONTEXT]\n<<<FILE examples/ai497/train_step.py>>>\nimport torch\n\n# BUG: autocast 미사용\ndef train_step(model, batch):\n    x, y = batch\n    out = model(x)\n    loss = out.loss if hasattr(out, 'loss') else out.mean()\n    loss.backward()\n    return loss.item()\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai497/train_step.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["autocast"]}, "code": "import torch\n\n@torch.no_grad()\ndef _bf16_supported():\n    return torch.cuda.is_available() and torch.cuda.is_bf16_supported()\n\n\ndef train_step(model, batch):\n    x, y = batch\n    amp_dtype = torch.bfloat16 if _bf16_supported() else torch.float16\n    scaler = torch.cuda.amp.GradScaler(enabled=True)\n    with torch.cuda.amp.autocast(enabled=True, dtype=amp_dtype):\n        out = model(x)\n        loss = out.loss if hasattr(out, 'loss') else out.mean()\n    scaler.scale(loss).backward()\n    scaler.step(torch.optim.Adam(model.parameters(), lr=1e-4))\n    scaler.update()\n    model.zero_grad(set_to_none=True)\n    return float(loss.detach().item())\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nLoRA target_modules 확장\n\n[CONTEXT]\n<<<FILE examples/ai26/lora_cfg.py>>>\nfrom peft import LoraConfig\n\nLC = LoraConfig(r=8, target_modules=[\n    \"q_proj\", \"v_proj\"  # BUG: 부족\n])\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai26/lora_cfg.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["k_proj", "o_proj"]}, "code": "from peft import LoraConfig\n\nLC = LoraConfig(r=8, target_modules=[\n    \"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"\n])\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nWindows에서 DataLoader 프리즈 방지 + pin_memory/persistent_workers\n\n[CONTEXT]\n<<<FILE examples/ai311/dataloader_main.py>>>\nimport torch, torch.utils.data as tud\n\nclass Dummy(torch.utils.data.Dataset):\n    def __len__(self): return 8\n    def __getitem__(self, i): return i\n\n# BUG: 모듈 임포트만으로 워커가 스폰되어 Windows에서 프리즈\nloader = tud.DataLoader(Dummy(), batch_size=2, num_workers=4)\nfor b in loader:\n    pass\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai311/dataloader_main.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["if __name__ == \"__main__\":"]}, "code": "import torch, torch.utils.data as tud\n\nclass Dummy(torch.utils.data.Dataset):\n    def __len__(self): return 8\n    def __getitem__(self, i): return i\n\n\ndef main():\n    loader = tud.DataLoader(Dummy(), batch_size=2, num_workers=4, pin_memory=True, persistent_workers=True)\n    for _ in loader:\n        pass\n\n\nif __name__ == \"__main__\":\n    main()\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n학습 후 어댑터 저장 추가\n\n[CONTEXT]\n<<<FILE examples/ai141/save_after_train.py>>>\nfrom peft import PeftModel\n# ... 학습 후\n# BUG: 저장 누락\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai141/save_after_train.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["save_pretrained"]}, "code": "from peft import PeftModel\n# ... 학습 후\nmodel.save_pretrained(\"training/qlora-out/export\")\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nflash 미지원 시 sdpa 안전 경로\n\n[CONTEXT]\n<<<FILE examples/ai574/model_load.py>>>\nfrom transformers import AutoModelForCausalLM\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"Qwen/Qwen3-8B\",\n)\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai574/model_load.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["attn_implementation"]}, "code": "from transformers import AutoModelForCausalLM\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"Qwen/Qwen3-8B\",\n    attn_implementation=\"sdpa\",\n)\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n조기 종료 콜백 추가\n\n[CONTEXT]\n<<<FILE examples/ai12/callbacks.py>>>\nfrom transformers import TrainerCallback\n\nclass MyCB(TrainerCallback):\n    pass\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai12/callbacks.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["EarlyStoppingCallback"]}, "code": "from transformers import TrainerCallback, EarlyStoppingCallback\n\nclass MyCB(TrainerCallback):\n    pass\n\nCALLBACKS = [EarlyStoppingCallback(early_stopping_patience=2)]\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\npad 토큰 ID 정확히 설정\n\n[CONTEXT]\n<<<FILE examples/ai328/label_cfg.py>>>\nfrom transformers import AutoTokenizer\nfrom dataclasses import dataclass\n\n@dataclass\nclass Cfg:\n    pad_id: int = -100  # BUG\n\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai328/label_cfg.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["CFG = Cfg"]}, "code": "from transformers import AutoTokenizer\nfrom dataclasses import dataclass\n\n@dataclass\nclass Cfg:\n    pad_id: int | None = None\n\n# tokenizer 로드 후 설정\ntok = AutoTokenizer.from_pretrained(\"Qwen/Qwen3-8B\", use_fast=True)\nCFG = Cfg(pad_id=tok.pad_token_id)\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n재현성: 시드 함수 추가\n\n[CONTEXT]\n<<<FILE examples/ai152/seed_utils.py>>>\n# BUG: 시드 미설정\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai152/seed_utils.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["set_seed("]}, "code": "import os, random, numpy as np, torch\n\ndef set_seed(seed: int = 42):\n    random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nAMP 적용 + bf16 우선\n\n[CONTEXT]\n<<<FILE examples/ai335/train_step.py>>>\nimport torch\n\n# BUG: autocast 미사용\ndef train_step(model, batch):\n    x, y = batch\n    out = model(x)\n    loss = out.loss if hasattr(out, 'loss') else out.mean()\n    loss.backward()\n    return loss.item()\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai335/train_step.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["autocast"]}, "code": "import torch\n\n@torch.no_grad()\ndef _bf16_supported():\n    return torch.cuda.is_available() and torch.cuda.is_bf16_supported()\n\n\ndef train_step(model, batch):\n    x, y = batch\n    amp_dtype = torch.bfloat16 if _bf16_supported() else torch.float16\n    scaler = torch.cuda.amp.GradScaler(enabled=True)\n    with torch.cuda.amp.autocast(enabled=True, dtype=amp_dtype):\n        out = model(x)\n        loss = out.loss if hasattr(out, 'loss') else out.mean()\n    scaler.scale(loss).backward()\n    scaler.step(torch.optim.Adam(model.parameters(), lr=1e-4))\n    scaler.update()\n    model.zero_grad(set_to_none=True)\n    return float(loss.detach().item())\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n필수 import 추가\n\n[CONTEXT]\n<<<FILE examples/ai322/imports.py>>>\n# BUG: numpy 미사용이지만 추후 사용 예정\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai322/imports.py", "loc": {"type": "anchor", "before": "\\A"}, "action": "insert_after", "once": true, "pre": {"must_not_contain": ["import numpy as np"]}, "code": "import numpy as np\n"}]}}
{"instruction": "Propose a minimal JSON plan for code changes. Output PLAN JSON only.", "input": "[INTENT]\n입출력/런타임 안전성 개선\n\n[CONTEXT]\n<<<FILE examples/ai656/dataloader_main.py>>>\nloader = DataLoader(...); for b in loader: pass\n\n<<<END>>>\n<<<FILE examples/ai656/ddp_main.py>>>\ntrainer()\n\n<<<END>>>", "output": {"files": [{"path": "examples/ai656/dataloader_main.py", "reason": "Windows freeze 가능", "strategy": "regex", "tests": ["__main__ 가드"]}, {"path": "examples/ai656/ddp_main.py", "reason": "DDP 가드 없음", "strategy": "regex", "tests": ["__main__"]}], "notes": "spawn 보호"}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nGPU bf16 지원 시 dtype 선택 개선\n\n[CONTEXT]\n<<<FILE examples/ai619/train_utils.py>>>\nimport torch\n\n# BUG: dtype가 고정되어 혼합정밀에 불리\ndef get_dtype():\n    return torch.float16\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai619/train_utils.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["torch.bfloat16"]}, "code": "import torch\n\n\ndef get_dtype():\n    \"\"\"GPU가 bf16 지원 시 bfloat16, 아니면 float16 반환\"\"\"\n    if torch.cuda.is_available() and torch.cuda.is_bf16_supported():\n        return torch.bfloat16\n    return torch.float16\n"}]}}
{"instruction": "Propose a minimal JSON plan for code changes. Output PLAN JSON only.", "input": "[INTENT]\n학습 안정화(AMP+훈련 설정)\n\n[CONTEXT]\n<<<FILE examples/ai699/train_utils.py>>>\ndef get_dtype():\n    return torch.float16\n\n<<<END>>>\n<<<FILE examples/ai699/training_args.py>>>\nargs = TrainingArguments(output_dir='out', per_device_train_batch_size=1, num_train_epochs=1)\n\n<<<END>>>", "output": {"files": [{"path": "examples/ai699/train_utils.py", "reason": "dtype가 고정 float16", "strategy": "regex", "tests": ["bf16 또는 fp16 반환"]}, {"path": "examples/ai699/training_args.py", "reason": "로깅/평가/GA 설정 누락", "strategy": "regex", "tests": ["eval_strategy=='steps'"]}], "notes": "bf16 지원 시 우선"}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n필수 import 추가\n\n[CONTEXT]\n<<<FILE examples/ai432/imports.py>>>\n# BUG: numpy 미사용이지만 추후 사용 예정\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai432/imports.py", "loc": {"type": "anchor", "before": "\\A"}, "action": "insert_after", "once": true, "pre": {"must_not_contain": ["import numpy as np"]}, "code": "import numpy as np\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n재현성: 시드 함수 추가\n\n[CONTEXT]\n<<<FILE examples/ai598/seed_utils.py>>>\n# BUG: 시드 미설정\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai598/seed_utils.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["set_seed("]}, "code": "import os, random, numpy as np, torch\n\ndef set_seed(seed: int = 42):\n    random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\npad 토큰 ID 정확히 설정\n\n[CONTEXT]\n<<<FILE examples/ai155/label_cfg.py>>>\nfrom transformers import AutoTokenizer\nfrom dataclasses import dataclass\n\n@dataclass\nclass Cfg:\n    pad_id: int = -100  # BUG\n\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai155/label_cfg.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["CFG = Cfg"]}, "code": "from transformers import AutoTokenizer\nfrom dataclasses import dataclass\n\n@dataclass\nclass Cfg:\n    pad_id: int | None = None\n\n# tokenizer 로드 후 설정\ntok = AutoTokenizer.from_pretrained(\"Qwen/Qwen3-8B\", use_fast=True)\nCFG = Cfg(pad_id=tok.pad_token_id)\n"}]}}
{"instruction": "Propose a minimal JSON plan for code changes. Output PLAN JSON only.", "input": "[INTENT]\n입출력/런타임 안전성 개선\n\n[CONTEXT]\n<<<FILE examples/ai756/dataloader_main.py>>>\nloader = DataLoader(...); for b in loader: pass\n\n<<<END>>>\n<<<FILE examples/ai756/ddp_main.py>>>\ntrainer()\n\n<<<END>>>", "output": {"files": [{"path": "examples/ai756/dataloader_main.py", "reason": "Windows freeze 가능", "strategy": "regex", "tests": ["__main__ 가드"]}, {"path": "examples/ai756/ddp_main.py", "reason": "DDP 가드 없음", "strategy": "regex", "tests": ["__main__"]}], "notes": "spawn 보호"}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n재현성: 시드 함수 추가\n\n[CONTEXT]\n<<<FILE examples/ai129/seed_utils.py>>>\n# BUG: 시드 미설정\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai129/seed_utils.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["set_seed("]}, "code": "import os, random, numpy as np, torch\n\ndef set_seed(seed: int = 42):\n    random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n"}]}}
{"instruction": "Propose a minimal JSON plan for code changes. Output PLAN JSON only.", "input": "[INTENT]\n입출력/런타임 안전성 개선\n\n[CONTEXT]\n<<<FILE examples/ai712/dataloader_main.py>>>\nloader = DataLoader(...); for b in loader: pass\n\n<<<END>>>\n<<<FILE examples/ai712/ddp_main.py>>>\ntrainer()\n\n<<<END>>>", "output": {"files": [{"path": "examples/ai712/dataloader_main.py", "reason": "Windows freeze 가능", "strategy": "regex", "tests": ["__main__ 가드"]}, {"path": "examples/ai712/ddp_main.py", "reason": "DDP 가드 없음", "strategy": "regex", "tests": ["__main__"]}], "notes": "spawn 보호"}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n학습 후 어댑터 저장 추가\n\n[CONTEXT]\n<<<FILE examples/ai341/save_after_train.py>>>\nfrom peft import PeftModel\n# ... 학습 후\n# BUG: 저장 누락\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai341/save_after_train.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["save_pretrained"]}, "code": "from peft import PeftModel\n# ... 학습 후\nmodel.save_pretrained(\"training/qlora-out/export\")\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n재현성: 시드 함수 추가\n\n[CONTEXT]\n<<<FILE examples/ai424/seed_utils.py>>>\n# BUG: 시드 미설정\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai424/seed_utils.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["set_seed("]}, "code": "import os, random, numpy as np, torch\n\ndef set_seed(seed: int = 42):\n    random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nWindows에서 DataLoader 프리즈 방지 + pin_memory/persistent_workers\n\n[CONTEXT]\n<<<FILE examples/ai416/dataloader_main.py>>>\nimport torch, torch.utils.data as tud\n\nclass Dummy(torch.utils.data.Dataset):\n    def __len__(self): return 8\n    def __getitem__(self, i): return i\n\n# BUG: 모듈 임포트만으로 워커가 스폰되어 Windows에서 프리즈\nloader = tud.DataLoader(Dummy(), batch_size=2, num_workers=4)\nfor b in loader:\n    pass\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai416/dataloader_main.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["if __name__ == \"__main__\":"]}, "code": "import torch, torch.utils.data as tud\n\nclass Dummy(torch.utils.data.Dataset):\n    def __len__(self): return 8\n    def __getitem__(self, i): return i\n\n\ndef main():\n    loader = tud.DataLoader(Dummy(), batch_size=2, num_workers=4, pin_memory=True, persistent_workers=True)\n    for _ in loader:\n        pass\n\n\nif __name__ == \"__main__\":\n    main()\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n필수 import 추가\n\n[CONTEXT]\n<<<FILE examples/ai493/imports.py>>>\n# BUG: numpy 미사용이지만 추후 사용 예정\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai493/imports.py", "loc": {"type": "anchor", "before": "\\A"}, "action": "insert_after", "once": true, "pre": {"must_not_contain": ["import numpy as np"]}, "code": "import numpy as np\n"}]}}
{"instruction": "Propose a minimal JSON plan for code changes. Output PLAN JSON only.", "input": "[INTENT]\n학습 안정화(AMP+훈련 설정)\n\n[CONTEXT]\n<<<FILE examples/ai754/train_utils.py>>>\ndef get_dtype():\n    return torch.float16\n\n<<<END>>>\n<<<FILE examples/ai754/training_args.py>>>\nargs = TrainingArguments(output_dir='out', per_device_train_batch_size=1, num_train_epochs=1)\n\n<<<END>>>", "output": {"files": [{"path": "examples/ai754/train_utils.py", "reason": "dtype가 고정 float16", "strategy": "regex", "tests": ["bf16 또는 fp16 반환"]}, {"path": "examples/ai754/training_args.py", "reason": "로깅/평가/GA 설정 누락", "strategy": "regex", "tests": ["eval_strategy=='steps'"]}], "notes": "bf16 지원 시 우선"}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nDDP/torchrun 안전 가드\n\n[CONTEXT]\n<<<FILE examples/ai81/ddp_main.py>>>\nimport torch\n\n# BUG: torchrun에서 main 가드 없음\ntrainer()\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai81/ddp_main.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["__main__"]}, "code": "import torch\n\n\ndef main():\n    trainer()\n\n\nif __name__ == \"__main__\":\n    main()\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nLoRA target_modules 확장\n\n[CONTEXT]\n<<<FILE examples/ai280/lora_cfg.py>>>\nfrom peft import LoraConfig\n\nLC = LoraConfig(r=8, target_modules=[\n    \"q_proj\", \"v_proj\"  # BUG: 부족\n])\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai280/lora_cfg.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["k_proj", "o_proj"]}, "code": "from peft import LoraConfig\n\nLC = LoraConfig(r=8, target_modules=[\n    \"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"\n])\n"}]}}
{"instruction": "Propose a minimal JSON plan for code changes. Output PLAN JSON only.", "input": "[INTENT]\n학습 안정화(AMP+훈련 설정)\n\n[CONTEXT]\n<<<FILE examples/ai752/train_utils.py>>>\ndef get_dtype():\n    return torch.float16\n\n<<<END>>>\n<<<FILE examples/ai752/training_args.py>>>\nargs = TrainingArguments(output_dir='out', per_device_train_batch_size=1, num_train_epochs=1)\n\n<<<END>>>", "output": {"files": [{"path": "examples/ai752/train_utils.py", "reason": "dtype가 고정 float16", "strategy": "regex", "tests": ["bf16 또는 fp16 반환"]}, {"path": "examples/ai752/training_args.py", "reason": "로깅/평가/GA 설정 누락", "strategy": "regex", "tests": ["eval_strategy=='steps'"]}], "notes": "bf16 지원 시 우선"}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n8bit 옵티마이저 + 코사인 스케줄러\n\n[CONTEXT]\n<<<FILE examples/ai239/optim.py>>>\nimport torch\nopt = torch.optim.Adam([], lr=3e-4)\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai239/optim.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["PagedAdamW8bit"]}, "code": "from transformers import get_scheduler\nfrom bitsandbytes.optim import PagedAdamW8bit\nopt = PagedAdamW8bit([], lr=2e-4)\nsched = get_scheduler(\"cosine\", optimizer=opt, num_warmup_steps=100, num_training_steps=1000)\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nflash 미지원 시 sdpa 안전 경로\n\n[CONTEXT]\n<<<FILE examples/ai5/model_load.py>>>\nfrom transformers import AutoModelForCausalLM\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"Qwen/Qwen3-8B\",\n)\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai5/model_load.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["attn_implementation"]}, "code": "from transformers import AutoModelForCausalLM\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"Qwen/Qwen3-8B\",\n    attn_implementation=\"sdpa\",\n)\n"}]}}
{"instruction": "Propose a minimal JSON plan for code changes. Output PLAN JSON only.", "input": "[INTENT]\n입출력/런타임 안전성 개선\n\n[CONTEXT]\n<<<FILE examples/ai671/dataloader_main.py>>>\nloader = DataLoader(...); for b in loader: pass\n\n<<<END>>>\n<<<FILE examples/ai671/ddp_main.py>>>\ntrainer()\n\n<<<END>>>", "output": {"files": [{"path": "examples/ai671/dataloader_main.py", "reason": "Windows freeze 가능", "strategy": "regex", "tests": ["__main__ 가드"]}, {"path": "examples/ai671/ddp_main.py", "reason": "DDP 가드 없음", "strategy": "regex", "tests": ["__main__"]}], "notes": "spawn 보호"}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nWindows에서 DataLoader 프리즈 방지 + pin_memory/persistent_workers\n\n[CONTEXT]\n<<<FILE examples/ai236/dataloader_main.py>>>\nimport torch, torch.utils.data as tud\n\nclass Dummy(torch.utils.data.Dataset):\n    def __len__(self): return 8\n    def __getitem__(self, i): return i\n\n# BUG: 모듈 임포트만으로 워커가 스폰되어 Windows에서 프리즈\nloader = tud.DataLoader(Dummy(), batch_size=2, num_workers=4)\nfor b in loader:\n    pass\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai236/dataloader_main.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["if __name__ == \"__main__\":"]}, "code": "import torch, torch.utils.data as tud\n\nclass Dummy(torch.utils.data.Dataset):\n    def __len__(self): return 8\n    def __getitem__(self, i): return i\n\n\ndef main():\n    loader = tud.DataLoader(Dummy(), batch_size=2, num_workers=4, pin_memory=True, persistent_workers=True)\n    for _ in loader:\n        pass\n\n\nif __name__ == \"__main__\":\n    main()\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nWindows에서 DataLoader 프리즈 방지 + pin_memory/persistent_workers\n\n[CONTEXT]\n<<<FILE examples/ai131/dataloader_main.py>>>\nimport torch, torch.utils.data as tud\n\nclass Dummy(torch.utils.data.Dataset):\n    def __len__(self): return 8\n    def __getitem__(self, i): return i\n\n# BUG: 모듈 임포트만으로 워커가 스폰되어 Windows에서 프리즈\nloader = tud.DataLoader(Dummy(), batch_size=2, num_workers=4)\nfor b in loader:\n    pass\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai131/dataloader_main.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["if __name__ == \"__main__\":"]}, "code": "import torch, torch.utils.data as tud\n\nclass Dummy(torch.utils.data.Dataset):\n    def __len__(self): return 8\n    def __getitem__(self, i): return i\n\n\ndef main():\n    loader = tud.DataLoader(Dummy(), batch_size=2, num_workers=4, pin_memory=True, persistent_workers=True)\n    for _ in loader:\n        pass\n\n\nif __name__ == \"__main__\":\n    main()\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n학습 후 어댑터 저장 추가\n\n[CONTEXT]\n<<<FILE examples/ai463/save_after_train.py>>>\nfrom peft import PeftModel\n# ... 학습 후\n# BUG: 저장 누락\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai463/save_after_train.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["save_pretrained"]}, "code": "from peft import PeftModel\n# ... 학습 후\nmodel.save_pretrained(\"training/qlora-out/export\")\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nLoRA target_modules 확장\n\n[CONTEXT]\n<<<FILE examples/ai555/lora_cfg.py>>>\nfrom peft import LoraConfig\n\nLC = LoraConfig(r=8, target_modules=[\n    \"q_proj\", \"v_proj\"  # BUG: 부족\n])\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai555/lora_cfg.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["k_proj", "o_proj"]}, "code": "from peft import LoraConfig\n\nLC = LoraConfig(r=8, target_modules=[\n    \"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"\n])\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n재현성: 시드 함수 추가\n\n[CONTEXT]\n<<<FILE examples/ai377/seed_utils.py>>>\n# BUG: 시드 미설정\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai377/seed_utils.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["set_seed("]}, "code": "import os, random, numpy as np, torch\n\ndef set_seed(seed: int = 42):\n    random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nflash 미지원 시 sdpa 안전 경로\n\n[CONTEXT]\n<<<FILE examples/ai600/model_load.py>>>\nfrom transformers import AutoModelForCausalLM\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"Qwen/Qwen3-8B\",\n)\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai600/model_load.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["attn_implementation"]}, "code": "from transformers import AutoModelForCausalLM\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"Qwen/Qwen3-8B\",\n    attn_implementation=\"sdpa\",\n)\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nGPU bf16 지원 시 dtype 선택 개선\n\n[CONTEXT]\n<<<FILE examples/ai613/train_utils.py>>>\nimport torch\n\n# BUG: dtype가 고정되어 혼합정밀에 불리\ndef get_dtype():\n    return torch.float16\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai613/train_utils.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["torch.bfloat16"]}, "code": "import torch\n\n\ndef get_dtype():\n    \"\"\"GPU가 bf16 지원 시 bfloat16, 아니면 float16 반환\"\"\"\n    if torch.cuda.is_available() and torch.cuda.is_bf16_supported():\n        return torch.bfloat16\n    return torch.float16\n"}]}}
{"instruction": "Propose a minimal JSON plan for code changes. Output PLAN JSON only.", "input": "[INTENT]\n입출력/런타임 안전성 개선\n\n[CONTEXT]\n<<<FILE examples/ai668/dataloader_main.py>>>\nloader = DataLoader(...); for b in loader: pass\n\n<<<END>>>\n<<<FILE examples/ai668/ddp_main.py>>>\ntrainer()\n\n<<<END>>>", "output": {"files": [{"path": "examples/ai668/dataloader_main.py", "reason": "Windows freeze 가능", "strategy": "regex", "tests": ["__main__ 가드"]}, {"path": "examples/ai668/ddp_main.py", "reason": "DDP 가드 없음", "strategy": "regex", "tests": ["__main__"]}], "notes": "spawn 보호"}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n8bit 옵티마이저 + 코사인 스케줄러\n\n[CONTEXT]\n<<<FILE examples/ai318/optim.py>>>\nimport torch\nopt = torch.optim.Adam([], lr=3e-4)\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai318/optim.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["PagedAdamW8bit"]}, "code": "from transformers import get_scheduler\nfrom bitsandbytes.optim import PagedAdamW8bit\nopt = PagedAdamW8bit([], lr=2e-4)\nsched = get_scheduler(\"cosine\", optimizer=opt, num_warmup_steps=100, num_training_steps=1000)\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\npad 토큰 ID 정확히 설정\n\n[CONTEXT]\n<<<FILE examples/ai395/label_cfg.py>>>\nfrom transformers import AutoTokenizer\nfrom dataclasses import dataclass\n\n@dataclass\nclass Cfg:\n    pad_id: int = -100  # BUG\n\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai395/label_cfg.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["CFG = Cfg"]}, "code": "from transformers import AutoTokenizer\nfrom dataclasses import dataclass\n\n@dataclass\nclass Cfg:\n    pad_id: int | None = None\n\n# tokenizer 로드 후 설정\ntok = AutoTokenizer.from_pretrained(\"Qwen/Qwen3-8B\", use_fast=True)\nCFG = Cfg(pad_id=tok.pad_token_id)\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n8bit 옵티마이저 + 코사인 스케줄러\n\n[CONTEXT]\n<<<FILE examples/ai601/optim.py>>>\nimport torch\nopt = torch.optim.Adam([], lr=3e-4)\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai601/optim.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["PagedAdamW8bit"]}, "code": "from transformers import get_scheduler\nfrom bitsandbytes.optim import PagedAdamW8bit\nopt = PagedAdamW8bit([], lr=2e-4)\nsched = get_scheduler(\"cosine\", optimizer=opt, num_warmup_steps=100, num_training_steps=1000)\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n8bit 옵티마이저 + 코사인 스케줄러\n\n[CONTEXT]\n<<<FILE examples/ai550/optim.py>>>\nimport torch\nopt = torch.optim.Adam([], lr=3e-4)\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai550/optim.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["PagedAdamW8bit"]}, "code": "from transformers import get_scheduler\nfrom bitsandbytes.optim import PagedAdamW8bit\nopt = PagedAdamW8bit([], lr=2e-4)\nsched = get_scheduler(\"cosine\", optimizer=opt, num_warmup_steps=100, num_training_steps=1000)\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n학습 후 어댑터 저장 추가\n\n[CONTEXT]\n<<<FILE examples/ai90/save_after_train.py>>>\nfrom peft import PeftModel\n# ... 학습 후\n# BUG: 저장 누락\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai90/save_after_train.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["save_pretrained"]}, "code": "from peft import PeftModel\n# ... 학습 후\nmodel.save_pretrained(\"training/qlora-out/export\")\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nWindows에서 DataLoader 프리즈 방지 + pin_memory/persistent_workers\n\n[CONTEXT]\n<<<FILE examples/ai614/dataloader_main.py>>>\nimport torch, torch.utils.data as tud\n\nclass Dummy(torch.utils.data.Dataset):\n    def __len__(self): return 8\n    def __getitem__(self, i): return i\n\n# BUG: 모듈 임포트만으로 워커가 스폰되어 Windows에서 프리즈\nloader = tud.DataLoader(Dummy(), batch_size=2, num_workers=4)\nfor b in loader:\n    pass\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai614/dataloader_main.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["if __name__ == \"__main__\":"]}, "code": "import torch, torch.utils.data as tud\n\nclass Dummy(torch.utils.data.Dataset):\n    def __len__(self): return 8\n    def __getitem__(self, i): return i\n\n\ndef main():\n    loader = tud.DataLoader(Dummy(), batch_size=2, num_workers=4, pin_memory=True, persistent_workers=True)\n    for _ in loader:\n        pass\n\n\nif __name__ == \"__main__\":\n    main()\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\npad 토큰 ID 정확히 설정\n\n[CONTEXT]\n<<<FILE examples/ai93/label_cfg.py>>>\nfrom transformers import AutoTokenizer\nfrom dataclasses import dataclass\n\n@dataclass\nclass Cfg:\n    pad_id: int = -100  # BUG\n\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai93/label_cfg.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["CFG = Cfg"]}, "code": "from transformers import AutoTokenizer\nfrom dataclasses import dataclass\n\n@dataclass\nclass Cfg:\n    pad_id: int | None = None\n\n# tokenizer 로드 후 설정\ntok = AutoTokenizer.from_pretrained(\"Qwen/Qwen3-8B\", use_fast=True)\nCFG = Cfg(pad_id=tok.pad_token_id)\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n재현성: 시드 함수 추가\n\n[CONTEXT]\n<<<FILE examples/ai579/seed_utils.py>>>\n# BUG: 시드 미설정\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai579/seed_utils.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["set_seed("]}, "code": "import os, random, numpy as np, torch\n\ndef set_seed(seed: int = 42):\n    random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n8bit 옵티마이저 + 코사인 스케줄러\n\n[CONTEXT]\n<<<FILE examples/ai401/optim.py>>>\nimport torch\nopt = torch.optim.Adam([], lr=3e-4)\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai401/optim.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["PagedAdamW8bit"]}, "code": "from transformers import get_scheduler\nfrom bitsandbytes.optim import PagedAdamW8bit\nopt = PagedAdamW8bit([], lr=2e-4)\nsched = get_scheduler(\"cosine\", optimizer=opt, num_warmup_steps=100, num_training_steps=1000)\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nLoRA target_modules 확장\n\n[CONTEXT]\n<<<FILE examples/ai272/lora_cfg.py>>>\nfrom peft import LoraConfig\n\nLC = LoraConfig(r=8, target_modules=[\n    \"q_proj\", \"v_proj\"  # BUG: 부족\n])\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai272/lora_cfg.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["k_proj", "o_proj"]}, "code": "from peft import LoraConfig\n\nLC = LoraConfig(r=8, target_modules=[\n    \"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"\n])\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nGPU bf16 지원 시 dtype 선택 개선\n\n[CONTEXT]\n<<<FILE examples/ai126/train_utils.py>>>\nimport torch\n\n# BUG: dtype가 고정되어 혼합정밀에 불리\ndef get_dtype():\n    return torch.float16\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai126/train_utils.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["torch.bfloat16"]}, "code": "import torch\n\n\ndef get_dtype():\n    \"\"\"GPU가 bf16 지원 시 bfloat16, 아니면 float16 반환\"\"\"\n    if torch.cuda.is_available() and torch.cuda.is_bf16_supported():\n        return torch.bfloat16\n    return torch.float16\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nWindows에서 DataLoader 프리즈 방지 + pin_memory/persistent_workers\n\n[CONTEXT]\n<<<FILE examples/ai445/dataloader_main.py>>>\nimport torch, torch.utils.data as tud\n\nclass Dummy(torch.utils.data.Dataset):\n    def __len__(self): return 8\n    def __getitem__(self, i): return i\n\n# BUG: 모듈 임포트만으로 워커가 스폰되어 Windows에서 프리즈\nloader = tud.DataLoader(Dummy(), batch_size=2, num_workers=4)\nfor b in loader:\n    pass\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai445/dataloader_main.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["if __name__ == \"__main__\":"]}, "code": "import torch, torch.utils.data as tud\n\nclass Dummy(torch.utils.data.Dataset):\n    def __len__(self): return 8\n    def __getitem__(self, i): return i\n\n\ndef main():\n    loader = tud.DataLoader(Dummy(), batch_size=2, num_workers=4, pin_memory=True, persistent_workers=True)\n    for _ in loader:\n        pass\n\n\nif __name__ == \"__main__\":\n    main()\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nGPU bf16 지원 시 dtype 선택 개선\n\n[CONTEXT]\n<<<FILE examples/ai221/train_utils.py>>>\nimport torch\n\n# BUG: dtype가 고정되어 혼합정밀에 불리\ndef get_dtype():\n    return torch.float16\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai221/train_utils.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["torch.bfloat16"]}, "code": "import torch\n\n\ndef get_dtype():\n    \"\"\"GPU가 bf16 지원 시 bfloat16, 아니면 float16 반환\"\"\"\n    if torch.cuda.is_available() and torch.cuda.is_bf16_supported():\n        return torch.bfloat16\n    return torch.float16\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nWindows에서 DataLoader 프리즈 방지 + pin_memory/persistent_workers\n\n[CONTEXT]\n<<<FILE examples/ai192/dataloader_main.py>>>\nimport torch, torch.utils.data as tud\n\nclass Dummy(torch.utils.data.Dataset):\n    def __len__(self): return 8\n    def __getitem__(self, i): return i\n\n# BUG: 모듈 임포트만으로 워커가 스폰되어 Windows에서 프리즈\nloader = tud.DataLoader(Dummy(), batch_size=2, num_workers=4)\nfor b in loader:\n    pass\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai192/dataloader_main.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["if __name__ == \"__main__\":"]}, "code": "import torch, torch.utils.data as tud\n\nclass Dummy(torch.utils.data.Dataset):\n    def __len__(self): return 8\n    def __getitem__(self, i): return i\n\n\ndef main():\n    loader = tud.DataLoader(Dummy(), batch_size=2, num_workers=4, pin_memory=True, persistent_workers=True)\n    for _ in loader:\n        pass\n\n\nif __name__ == \"__main__\":\n    main()\n"}]}}
{"instruction": "Propose a minimal JSON plan for code changes. Output PLAN JSON only.", "input": "[INTENT]\n입출력/런타임 안전성 개선\n\n[CONTEXT]\n<<<FILE examples/ai649/dataloader_main.py>>>\nloader = DataLoader(...); for b in loader: pass\n\n<<<END>>>\n<<<FILE examples/ai649/ddp_main.py>>>\ntrainer()\n\n<<<END>>>", "output": {"files": [{"path": "examples/ai649/dataloader_main.py", "reason": "Windows freeze 가능", "strategy": "regex", "tests": ["__main__ 가드"]}, {"path": "examples/ai649/ddp_main.py", "reason": "DDP 가드 없음", "strategy": "regex", "tests": ["__main__"]}], "notes": "spawn 보호"}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n체크포인트 재개 지원\n\n[CONTEXT]\n<<<FILE examples/ai151/trainer_resume.py>>>\nfrom transformers import Trainer\ntrainer = Trainer(... )\n# BUG: 재시작 미지원\ntrainer.train()\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai151/trainer_resume.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["resume_from_checkpoint"]}, "code": "from transformers import Trainer\ntrainer = Trainer(... )\ntrainer.train(resume_from_checkpoint=True)\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n조기 종료 콜백 추가\n\n[CONTEXT]\n<<<FILE examples/ai29/callbacks.py>>>\nfrom transformers import TrainerCallback\n\nclass MyCB(TrainerCallback):\n    pass\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai29/callbacks.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["EarlyStoppingCallback"]}, "code": "from transformers import TrainerCallback, EarlyStoppingCallback\n\nclass MyCB(TrainerCallback):\n    pass\n\nCALLBACKS = [EarlyStoppingCallback(early_stopping_patience=2)]\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n훈련 안정화: GA steps/로깅/평가/체크포인팅/스케줄러\n\n[CONTEXT]\n<<<FILE examples/ai167/training_args.py>>>\nfrom transformers import TrainingArguments\n\nargs = TrainingArguments(\n    output_dir=\"out\",\n    per_device_train_batch_size=1,\n    num_train_epochs=1,\n)\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai167/training_args.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["gradient_accumulation_steps"]}, "code": "from transformers import TrainingArguments\n\nargs = TrainingArguments(\n    output_dir=\"out\",\n    per_device_train_batch_size=1,\n    gradient_accumulation_steps=8,\n    num_train_epochs=1,\n    logging_steps=10,\n    eval_strategy=\"steps\",\n    eval_steps=50,\n    save_steps=50,\n    lr_scheduler_type=\"cosine\",\n    warmup_ratio=0.03,\n    fp16=False, bf16=True,\n    gradient_checkpointing=True,\n    max_grad_norm=1.0,\n)\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nGPU bf16 지원 시 dtype 선택 개선\n\n[CONTEXT]\n<<<FILE examples/ai24/train_utils.py>>>\nimport torch\n\n# BUG: dtype가 고정되어 혼합정밀에 불리\ndef get_dtype():\n    return torch.float16\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai24/train_utils.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["torch.bfloat16"]}, "code": "import torch\n\n\ndef get_dtype():\n    \"\"\"GPU가 bf16 지원 시 bfloat16, 아니면 float16 반환\"\"\"\n    if torch.cuda.is_available() and torch.cuda.is_bf16_supported():\n        return torch.bfloat16\n    return torch.float16\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n훈련 안정화: GA steps/로깅/평가/체크포인팅/스케줄러\n\n[CONTEXT]\n<<<FILE examples/ai479/training_args.py>>>\nfrom transformers import TrainingArguments\n\nargs = TrainingArguments(\n    output_dir=\"out\",\n    per_device_train_batch_size=1,\n    num_train_epochs=1,\n)\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai479/training_args.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["gradient_accumulation_steps"]}, "code": "from transformers import TrainingArguments\n\nargs = TrainingArguments(\n    output_dir=\"out\",\n    per_device_train_batch_size=1,\n    gradient_accumulation_steps=8,\n    num_train_epochs=1,\n    logging_steps=10,\n    eval_strategy=\"steps\",\n    eval_steps=50,\n    save_steps=50,\n    lr_scheduler_type=\"cosine\",\n    warmup_ratio=0.03,\n    fp16=False, bf16=True,\n    gradient_checkpointing=True,\n    max_grad_norm=1.0,\n)\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n재현성: 시드 함수 추가\n\n[CONTEXT]\n<<<FILE examples/ai154/seed_utils.py>>>\n# BUG: 시드 미설정\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai154/seed_utils.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["set_seed("]}, "code": "import os, random, numpy as np, torch\n\ndef set_seed(seed: int = 42):\n    random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n8bit 옵티마이저 + 코사인 스케줄러\n\n[CONTEXT]\n<<<FILE examples/ai86/optim.py>>>\nimport torch\nopt = torch.optim.Adam([], lr=3e-4)\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai86/optim.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["PagedAdamW8bit"]}, "code": "from transformers import get_scheduler\nfrom bitsandbytes.optim import PagedAdamW8bit\nopt = PagedAdamW8bit([], lr=2e-4)\nsched = get_scheduler(\"cosine\", optimizer=opt, num_warmup_steps=100, num_training_steps=1000)\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n8bit 옵티마이저 + 코사인 스케줄러\n\n[CONTEXT]\n<<<FILE examples/ai228/optim.py>>>\nimport torch\nopt = torch.optim.Adam([], lr=3e-4)\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai228/optim.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["PagedAdamW8bit"]}, "code": "from transformers import get_scheduler\nfrom bitsandbytes.optim import PagedAdamW8bit\nopt = PagedAdamW8bit([], lr=2e-4)\nsched = get_scheduler(\"cosine\", optimizer=opt, num_warmup_steps=100, num_training_steps=1000)\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nAMP 적용 + bf16 우선\n\n[CONTEXT]\n<<<FILE examples/ai402/train_step.py>>>\nimport torch\n\n# BUG: autocast 미사용\ndef train_step(model, batch):\n    x, y = batch\n    out = model(x)\n    loss = out.loss if hasattr(out, 'loss') else out.mean()\n    loss.backward()\n    return loss.item()\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai402/train_step.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["autocast"]}, "code": "import torch\n\n@torch.no_grad()\ndef _bf16_supported():\n    return torch.cuda.is_available() and torch.cuda.is_bf16_supported()\n\n\ndef train_step(model, batch):\n    x, y = batch\n    amp_dtype = torch.bfloat16 if _bf16_supported() else torch.float16\n    scaler = torch.cuda.amp.GradScaler(enabled=True)\n    with torch.cuda.amp.autocast(enabled=True, dtype=amp_dtype):\n        out = model(x)\n        loss = out.loss if hasattr(out, 'loss') else out.mean()\n    scaler.scale(loss).backward()\n    scaler.step(torch.optim.Adam(model.parameters(), lr=1e-4))\n    scaler.update()\n    model.zero_grad(set_to_none=True)\n    return float(loss.detach().item())\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nGPU bf16 지원 시 dtype 선택 개선\n\n[CONTEXT]\n<<<FILE examples/ai325/train_utils.py>>>\nimport torch\n\n# BUG: dtype가 고정되어 혼합정밀에 불리\ndef get_dtype():\n    return torch.float16\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai325/train_utils.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["torch.bfloat16"]}, "code": "import torch\n\n\ndef get_dtype():\n    \"\"\"GPU가 bf16 지원 시 bfloat16, 아니면 float16 반환\"\"\"\n    if torch.cuda.is_available() and torch.cuda.is_bf16_supported():\n        return torch.bfloat16\n    return torch.float16\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nWindows에서 DataLoader 프리즈 방지 + pin_memory/persistent_workers\n\n[CONTEXT]\n<<<FILE examples/ai51/dataloader_main.py>>>\nimport torch, torch.utils.data as tud\n\nclass Dummy(torch.utils.data.Dataset):\n    def __len__(self): return 8\n    def __getitem__(self, i): return i\n\n# BUG: 모듈 임포트만으로 워커가 스폰되어 Windows에서 프리즈\nloader = tud.DataLoader(Dummy(), batch_size=2, num_workers=4)\nfor b in loader:\n    pass\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai51/dataloader_main.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["if __name__ == \"__main__\":"]}, "code": "import torch, torch.utils.data as tud\n\nclass Dummy(torch.utils.data.Dataset):\n    def __len__(self): return 8\n    def __getitem__(self, i): return i\n\n\ndef main():\n    loader = tud.DataLoader(Dummy(), batch_size=2, num_workers=4, pin_memory=True, persistent_workers=True)\n    for _ in loader:\n        pass\n\n\nif __name__ == \"__main__\":\n    main()\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\npad 토큰 ID 정확히 설정\n\n[CONTEXT]\n<<<FILE examples/ai223/label_cfg.py>>>\nfrom transformers import AutoTokenizer\nfrom dataclasses import dataclass\n\n@dataclass\nclass Cfg:\n    pad_id: int = -100  # BUG\n\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai223/label_cfg.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["CFG = Cfg"]}, "code": "from transformers import AutoTokenizer\nfrom dataclasses import dataclass\n\n@dataclass\nclass Cfg:\n    pad_id: int | None = None\n\n# tokenizer 로드 후 설정\ntok = AutoTokenizer.from_pretrained(\"Qwen/Qwen3-8B\", use_fast=True)\nCFG = Cfg(pad_id=tok.pad_token_id)\n"}]}}
{"instruction": "Propose a minimal JSON plan for code changes. Output PLAN JSON only.", "input": "[INTENT]\n학습 안정화(AMP+훈련 설정)\n\n[CONTEXT]\n<<<FILE examples/ai770/train_utils.py>>>\ndef get_dtype():\n    return torch.float16\n\n<<<END>>>\n<<<FILE examples/ai770/training_args.py>>>\nargs = TrainingArguments(output_dir='out', per_device_train_batch_size=1, num_train_epochs=1)\n\n<<<END>>>", "output": {"files": [{"path": "examples/ai770/train_utils.py", "reason": "dtype가 고정 float16", "strategy": "regex", "tests": ["bf16 또는 fp16 반환"]}, {"path": "examples/ai770/training_args.py", "reason": "로깅/평가/GA 설정 누락", "strategy": "regex", "tests": ["eval_strategy=='steps'"]}], "notes": "bf16 지원 시 우선"}}
{"instruction": "Propose a minimal JSON plan for code changes. Output PLAN JSON only.", "input": "[INTENT]\n학습 안정화(AMP+훈련 설정)\n\n[CONTEXT]\n<<<FILE examples/ai721/train_utils.py>>>\ndef get_dtype():\n    return torch.float16\n\n<<<END>>>\n<<<FILE examples/ai721/training_args.py>>>\nargs = TrainingArguments(output_dir='out', per_device_train_batch_size=1, num_train_epochs=1)\n\n<<<END>>>", "output": {"files": [{"path": "examples/ai721/train_utils.py", "reason": "dtype가 고정 float16", "strategy": "regex", "tests": ["bf16 또는 fp16 반환"]}, {"path": "examples/ai721/training_args.py", "reason": "로깅/평가/GA 설정 누락", "strategy": "regex", "tests": ["eval_strategy=='steps'"]}], "notes": "bf16 지원 시 우선"}}
{"instruction": "Propose a minimal JSON plan for code changes. Output PLAN JSON only.", "input": "[INTENT]\n학습 안정화(AMP+훈련 설정)\n\n[CONTEXT]\n<<<FILE examples/ai681/train_utils.py>>>\ndef get_dtype():\n    return torch.float16\n\n<<<END>>>\n<<<FILE examples/ai681/training_args.py>>>\nargs = TrainingArguments(output_dir='out', per_device_train_batch_size=1, num_train_epochs=1)\n\n<<<END>>>", "output": {"files": [{"path": "examples/ai681/train_utils.py", "reason": "dtype가 고정 float16", "strategy": "regex", "tests": ["bf16 또는 fp16 반환"]}, {"path": "examples/ai681/training_args.py", "reason": "로깅/평가/GA 설정 누락", "strategy": "regex", "tests": ["eval_strategy=='steps'"]}], "notes": "bf16 지원 시 우선"}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nGPU bf16 지원 시 dtype 선택 개선\n\n[CONTEXT]\n<<<FILE examples/ai351/train_utils.py>>>\nimport torch\n\n# BUG: dtype가 고정되어 혼합정밀에 불리\ndef get_dtype():\n    return torch.float16\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai351/train_utils.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["torch.bfloat16"]}, "code": "import torch\n\n\ndef get_dtype():\n    \"\"\"GPU가 bf16 지원 시 bfloat16, 아니면 float16 반환\"\"\"\n    if torch.cuda.is_available() and torch.cuda.is_bf16_supported():\n        return torch.bfloat16\n    return torch.float16\n"}]}}
{"instruction": "Propose a minimal JSON plan for code changes. Output PLAN JSON only.", "input": "[INTENT]\n입출력/런타임 안전성 개선\n\n[CONTEXT]\n<<<FILE examples/ai666/dataloader_main.py>>>\nloader = DataLoader(...); for b in loader: pass\n\n<<<END>>>\n<<<FILE examples/ai666/ddp_main.py>>>\ntrainer()\n\n<<<END>>>", "output": {"files": [{"path": "examples/ai666/dataloader_main.py", "reason": "Windows freeze 가능", "strategy": "regex", "tests": ["__main__ 가드"]}, {"path": "examples/ai666/ddp_main.py", "reason": "DDP 가드 없음", "strategy": "regex", "tests": ["__main__"]}], "notes": "spawn 보호"}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nDDP/torchrun 안전 가드\n\n[CONTEXT]\n<<<FILE examples/ai121/ddp_main.py>>>\nimport torch\n\n# BUG: torchrun에서 main 가드 없음\ntrainer()\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai121/ddp_main.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["__main__"]}, "code": "import torch\n\n\ndef main():\n    trainer()\n\n\nif __name__ == \"__main__\":\n    main()\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n훈련 안정화: GA steps/로깅/평가/체크포인팅/스케줄러\n\n[CONTEXT]\n<<<FILE examples/ai438/training_args.py>>>\nfrom transformers import TrainingArguments\n\nargs = TrainingArguments(\n    output_dir=\"out\",\n    per_device_train_batch_size=1,\n    num_train_epochs=1,\n)\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai438/training_args.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["gradient_accumulation_steps"]}, "code": "from transformers import TrainingArguments\n\nargs = TrainingArguments(\n    output_dir=\"out\",\n    per_device_train_batch_size=1,\n    gradient_accumulation_steps=8,\n    num_train_epochs=1,\n    logging_steps=10,\n    eval_strategy=\"steps\",\n    eval_steps=50,\n    save_steps=50,\n    lr_scheduler_type=\"cosine\",\n    warmup_ratio=0.03,\n    fp16=False, bf16=True,\n    gradient_checkpointing=True,\n    max_grad_norm=1.0,\n)\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n학습 후 어댑터 저장 추가\n\n[CONTEXT]\n<<<FILE examples/ai317/save_after_train.py>>>\nfrom peft import PeftModel\n# ... 학습 후\n# BUG: 저장 누락\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai317/save_after_train.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["save_pretrained"]}, "code": "from peft import PeftModel\n# ... 학습 후\nmodel.save_pretrained(\"training/qlora-out/export\")\n"}]}}
{"instruction": "Propose a minimal JSON plan for code changes. Output PLAN JSON only.", "input": "[INTENT]\n입출력/런타임 안전성 개선\n\n[CONTEXT]\n<<<FILE examples/ai719/dataloader_main.py>>>\nloader = DataLoader(...); for b in loader: pass\n\n<<<END>>>\n<<<FILE examples/ai719/ddp_main.py>>>\ntrainer()\n\n<<<END>>>", "output": {"files": [{"path": "examples/ai719/dataloader_main.py", "reason": "Windows freeze 가능", "strategy": "regex", "tests": ["__main__ 가드"]}, {"path": "examples/ai719/ddp_main.py", "reason": "DDP 가드 없음", "strategy": "regex", "tests": ["__main__"]}], "notes": "spawn 보호"}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n재현성: 시드 함수 추가\n\n[CONTEXT]\n<<<FILE examples/ai533/seed_utils.py>>>\n# BUG: 시드 미설정\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai533/seed_utils.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["set_seed("]}, "code": "import os, random, numpy as np, torch\n\ndef set_seed(seed: int = 42):\n    random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n"}]}}
{"instruction": "Propose a minimal JSON plan for code changes. Output PLAN JSON only.", "input": "[INTENT]\n입출력/런타임 안전성 개선\n\n[CONTEXT]\n<<<FILE examples/ai800/dataloader_main.py>>>\nloader = DataLoader(...); for b in loader: pass\n\n<<<END>>>\n<<<FILE examples/ai800/ddp_main.py>>>\ntrainer()\n\n<<<END>>>", "output": {"files": [{"path": "examples/ai800/dataloader_main.py", "reason": "Windows freeze 가능", "strategy": "regex", "tests": ["__main__ 가드"]}, {"path": "examples/ai800/ddp_main.py", "reason": "DDP 가드 없음", "strategy": "regex", "tests": ["__main__"]}], "notes": "spawn 보호"}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n필수 import 추가\n\n[CONTEXT]\n<<<FILE examples/ai494/imports.py>>>\n# BUG: numpy 미사용이지만 추후 사용 예정\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai494/imports.py", "loc": {"type": "anchor", "before": "\\A"}, "action": "insert_after", "once": true, "pre": {"must_not_contain": ["import numpy as np"]}, "code": "import numpy as np\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n학습 후 어댑터 저장 추가\n\n[CONTEXT]\n<<<FILE examples/ai399/save_after_train.py>>>\nfrom peft import PeftModel\n# ... 학습 후\n# BUG: 저장 누락\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai399/save_after_train.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["save_pretrained"]}, "code": "from peft import PeftModel\n# ... 학습 후\nmodel.save_pretrained(\"training/qlora-out/export\")\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n훈련 안정화: GA steps/로깅/평가/체크포인팅/스케줄러\n\n[CONTEXT]\n<<<FILE examples/ai195/training_args.py>>>\nfrom transformers import TrainingArguments\n\nargs = TrainingArguments(\n    output_dir=\"out\",\n    per_device_train_batch_size=1,\n    num_train_epochs=1,\n)\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai195/training_args.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["gradient_accumulation_steps"]}, "code": "from transformers import TrainingArguments\n\nargs = TrainingArguments(\n    output_dir=\"out\",\n    per_device_train_batch_size=1,\n    gradient_accumulation_steps=8,\n    num_train_epochs=1,\n    logging_steps=10,\n    eval_strategy=\"steps\",\n    eval_steps=50,\n    save_steps=50,\n    lr_scheduler_type=\"cosine\",\n    warmup_ratio=0.03,\n    fp16=False, bf16=True,\n    gradient_checkpointing=True,\n    max_grad_norm=1.0,\n)\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\npad 토큰 ID 정확히 설정\n\n[CONTEXT]\n<<<FILE examples/ai274/label_cfg.py>>>\nfrom transformers import AutoTokenizer\nfrom dataclasses import dataclass\n\n@dataclass\nclass Cfg:\n    pad_id: int = -100  # BUG\n\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai274/label_cfg.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["CFG = Cfg"]}, "code": "from transformers import AutoTokenizer\nfrom dataclasses import dataclass\n\n@dataclass\nclass Cfg:\n    pad_id: int | None = None\n\n# tokenizer 로드 후 설정\ntok = AutoTokenizer.from_pretrained(\"Qwen/Qwen3-8B\", use_fast=True)\nCFG = Cfg(pad_id=tok.pad_token_id)\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nDDP/torchrun 안전 가드\n\n[CONTEXT]\n<<<FILE examples/ai346/ddp_main.py>>>\nimport torch\n\n# BUG: torchrun에서 main 가드 없음\ntrainer()\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai346/ddp_main.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["__main__"]}, "code": "import torch\n\n\ndef main():\n    trainer()\n\n\nif __name__ == \"__main__\":\n    main()\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n필수 import 추가\n\n[CONTEXT]\n<<<FILE examples/ai169/imports.py>>>\n# BUG: numpy 미사용이지만 추후 사용 예정\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai169/imports.py", "loc": {"type": "anchor", "before": "\\A"}, "action": "insert_after", "once": true, "pre": {"must_not_contain": ["import numpy as np"]}, "code": "import numpy as np\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\npad 토큰 ID 정확히 설정\n\n[CONTEXT]\n<<<FILE examples/ai541/label_cfg.py>>>\nfrom transformers import AutoTokenizer\nfrom dataclasses import dataclass\n\n@dataclass\nclass Cfg:\n    pad_id: int = -100  # BUG\n\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai541/label_cfg.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["CFG = Cfg"]}, "code": "from transformers import AutoTokenizer\nfrom dataclasses import dataclass\n\n@dataclass\nclass Cfg:\n    pad_id: int | None = None\n\n# tokenizer 로드 후 설정\ntok = AutoTokenizer.from_pretrained(\"Qwen/Qwen3-8B\", use_fast=True)\nCFG = Cfg(pad_id=tok.pad_token_id)\n"}]}}
{"instruction": "Propose a minimal JSON plan for code changes. Output PLAN JSON only.", "input": "[INTENT]\n학습 안정화(AMP+훈련 설정)\n\n[CONTEXT]\n<<<FILE examples/ai715/train_utils.py>>>\ndef get_dtype():\n    return torch.float16\n\n<<<END>>>\n<<<FILE examples/ai715/training_args.py>>>\nargs = TrainingArguments(output_dir='out', per_device_train_batch_size=1, num_train_epochs=1)\n\n<<<END>>>", "output": {"files": [{"path": "examples/ai715/train_utils.py", "reason": "dtype가 고정 float16", "strategy": "regex", "tests": ["bf16 또는 fp16 반환"]}, {"path": "examples/ai715/training_args.py", "reason": "로깅/평가/GA 설정 누락", "strategy": "regex", "tests": ["eval_strategy=='steps'"]}], "notes": "bf16 지원 시 우선"}}
{"instruction": "Propose a minimal JSON plan for code changes. Output PLAN JSON only.", "input": "[INTENT]\n입출력/런타임 안전성 개선\n\n[CONTEXT]\n<<<FILE examples/ai774/dataloader_main.py>>>\nloader = DataLoader(...); for b in loader: pass\n\n<<<END>>>\n<<<FILE examples/ai774/ddp_main.py>>>\ntrainer()\n\n<<<END>>>", "output": {"files": [{"path": "examples/ai774/dataloader_main.py", "reason": "Windows freeze 가능", "strategy": "regex", "tests": ["__main__ 가드"]}, {"path": "examples/ai774/ddp_main.py", "reason": "DDP 가드 없음", "strategy": "regex", "tests": ["__main__"]}], "notes": "spawn 보호"}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nGPU bf16 지원 시 dtype 선택 개선\n\n[CONTEXT]\n<<<FILE examples/ai355/train_utils.py>>>\nimport torch\n\n# BUG: dtype가 고정되어 혼합정밀에 불리\ndef get_dtype():\n    return torch.float16\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai355/train_utils.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["torch.bfloat16"]}, "code": "import torch\n\n\ndef get_dtype():\n    \"\"\"GPU가 bf16 지원 시 bfloat16, 아니면 float16 반환\"\"\"\n    if torch.cuda.is_available() and torch.cuda.is_bf16_supported():\n        return torch.bfloat16\n    return torch.float16\n"}]}}
{"instruction": "Propose a minimal JSON plan for code changes. Output PLAN JSON only.", "input": "[INTENT]\n입출력/런타임 안전성 개선\n\n[CONTEXT]\n<<<FILE examples/ai780/dataloader_main.py>>>\nloader = DataLoader(...); for b in loader: pass\n\n<<<END>>>\n<<<FILE examples/ai780/ddp_main.py>>>\ntrainer()\n\n<<<END>>>", "output": {"files": [{"path": "examples/ai780/dataloader_main.py", "reason": "Windows freeze 가능", "strategy": "regex", "tests": ["__main__ 가드"]}, {"path": "examples/ai780/ddp_main.py", "reason": "DDP 가드 없음", "strategy": "regex", "tests": ["__main__"]}], "notes": "spawn 보호"}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n재현성: 시드 함수 추가\n\n[CONTEXT]\n<<<FILE examples/ai174/seed_utils.py>>>\n# BUG: 시드 미설정\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai174/seed_utils.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["set_seed("]}, "code": "import os, random, numpy as np, torch\n\ndef set_seed(seed: int = 42):\n    random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nLoRA target_modules 확장\n\n[CONTEXT]\n<<<FILE examples/ai136/lora_cfg.py>>>\nfrom peft import LoraConfig\n\nLC = LoraConfig(r=8, target_modules=[\n    \"q_proj\", \"v_proj\"  # BUG: 부족\n])\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai136/lora_cfg.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["k_proj", "o_proj"]}, "code": "from peft import LoraConfig\n\nLC = LoraConfig(r=8, target_modules=[\n    \"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"\n])\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nDDP/torchrun 안전 가드\n\n[CONTEXT]\n<<<FILE examples/ai241/ddp_main.py>>>\nimport torch\n\n# BUG: torchrun에서 main 가드 없음\ntrainer()\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai241/ddp_main.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["__main__"]}, "code": "import torch\n\n\ndef main():\n    trainer()\n\n\nif __name__ == \"__main__\":\n    main()\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nDDP/torchrun 안전 가드\n\n[CONTEXT]\n<<<FILE examples/ai39/ddp_main.py>>>\nimport torch\n\n# BUG: torchrun에서 main 가드 없음\ntrainer()\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai39/ddp_main.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["__main__"]}, "code": "import torch\n\n\ndef main():\n    trainer()\n\n\nif __name__ == \"__main__\":\n    main()\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nLoRA target_modules 확장\n\n[CONTEXT]\n<<<FILE examples/ai80/lora_cfg.py>>>\nfrom peft import LoraConfig\n\nLC = LoraConfig(r=8, target_modules=[\n    \"q_proj\", \"v_proj\"  # BUG: 부족\n])\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai80/lora_cfg.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["k_proj", "o_proj"]}, "code": "from peft import LoraConfig\n\nLC = LoraConfig(r=8, target_modules=[\n    \"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"\n])\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n학습 후 어댑터 저장 추가\n\n[CONTEXT]\n<<<FILE examples/ai336/save_after_train.py>>>\nfrom peft import PeftModel\n# ... 학습 후\n# BUG: 저장 누락\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai336/save_after_train.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["save_pretrained"]}, "code": "from peft import PeftModel\n# ... 학습 후\nmodel.save_pretrained(\"training/qlora-out/export\")\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n필수 import 추가\n\n[CONTEXT]\n<<<FILE examples/ai210/imports.py>>>\n# BUG: numpy 미사용이지만 추후 사용 예정\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai210/imports.py", "loc": {"type": "anchor", "before": "\\A"}, "action": "insert_after", "once": true, "pre": {"must_not_contain": ["import numpy as np"]}, "code": "import numpy as np\n"}]}}
{"instruction": "Propose a minimal JSON plan for code changes. Output PLAN JSON only.", "input": "[INTENT]\n입출력/런타임 안전성 개선\n\n[CONTEXT]\n<<<FILE examples/ai779/dataloader_main.py>>>\nloader = DataLoader(...); for b in loader: pass\n\n<<<END>>>\n<<<FILE examples/ai779/ddp_main.py>>>\ntrainer()\n\n<<<END>>>", "output": {"files": [{"path": "examples/ai779/dataloader_main.py", "reason": "Windows freeze 가능", "strategy": "regex", "tests": ["__main__ 가드"]}, {"path": "examples/ai779/ddp_main.py", "reason": "DDP 가드 없음", "strategy": "regex", "tests": ["__main__"]}], "notes": "spawn 보호"}}
{"instruction": "Propose a minimal JSON plan for code changes. Output PLAN JSON only.", "input": "[INTENT]\n입출력/런타임 안전성 개선\n\n[CONTEXT]\n<<<FILE examples/ai650/dataloader_main.py>>>\nloader = DataLoader(...); for b in loader: pass\n\n<<<END>>>\n<<<FILE examples/ai650/ddp_main.py>>>\ntrainer()\n\n<<<END>>>", "output": {"files": [{"path": "examples/ai650/dataloader_main.py", "reason": "Windows freeze 가능", "strategy": "regex", "tests": ["__main__ 가드"]}, {"path": "examples/ai650/ddp_main.py", "reason": "DDP 가드 없음", "strategy": "regex", "tests": ["__main__"]}], "notes": "spawn 보호"}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n필수 import 추가\n\n[CONTEXT]\n<<<FILE examples/ai470/imports.py>>>\n# BUG: numpy 미사용이지만 추후 사용 예정\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai470/imports.py", "loc": {"type": "anchor", "before": "\\A"}, "action": "insert_after", "once": true, "pre": {"must_not_contain": ["import numpy as np"]}, "code": "import numpy as np\n"}]}}
{"instruction": "Propose a minimal JSON plan for code changes. Output PLAN JSON only.", "input": "[INTENT]\n학습 안정화(AMP+훈련 설정)\n\n[CONTEXT]\n<<<FILE examples/ai723/train_utils.py>>>\ndef get_dtype():\n    return torch.float16\n\n<<<END>>>\n<<<FILE examples/ai723/training_args.py>>>\nargs = TrainingArguments(output_dir='out', per_device_train_batch_size=1, num_train_epochs=1)\n\n<<<END>>>", "output": {"files": [{"path": "examples/ai723/train_utils.py", "reason": "dtype가 고정 float16", "strategy": "regex", "tests": ["bf16 또는 fp16 반환"]}, {"path": "examples/ai723/training_args.py", "reason": "로깅/평가/GA 설정 누락", "strategy": "regex", "tests": ["eval_strategy=='steps'"]}], "notes": "bf16 지원 시 우선"}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nWindows에서 DataLoader 프리즈 방지 + pin_memory/persistent_workers\n\n[CONTEXT]\n<<<FILE examples/ai270/dataloader_main.py>>>\nimport torch, torch.utils.data as tud\n\nclass Dummy(torch.utils.data.Dataset):\n    def __len__(self): return 8\n    def __getitem__(self, i): return i\n\n# BUG: 모듈 임포트만으로 워커가 스폰되어 Windows에서 프리즈\nloader = tud.DataLoader(Dummy(), batch_size=2, num_workers=4)\nfor b in loader:\n    pass\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai270/dataloader_main.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["if __name__ == \"__main__\":"]}, "code": "import torch, torch.utils.data as tud\n\nclass Dummy(torch.utils.data.Dataset):\n    def __len__(self): return 8\n    def __getitem__(self, i): return i\n\n\ndef main():\n    loader = tud.DataLoader(Dummy(), batch_size=2, num_workers=4, pin_memory=True, persistent_workers=True)\n    for _ in loader:\n        pass\n\n\nif __name__ == \"__main__\":\n    main()\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n학습 후 어댑터 저장 추가\n\n[CONTEXT]\n<<<FILE examples/ai384/save_after_train.py>>>\nfrom peft import PeftModel\n# ... 학습 후\n# BUG: 저장 누락\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai384/save_after_train.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["save_pretrained"]}, "code": "from peft import PeftModel\n# ... 학습 후\nmodel.save_pretrained(\"training/qlora-out/export\")\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nWindows에서 DataLoader 프리즈 방지 + pin_memory/persistent_workers\n\n[CONTEXT]\n<<<FILE examples/ai226/dataloader_main.py>>>\nimport torch, torch.utils.data as tud\n\nclass Dummy(torch.utils.data.Dataset):\n    def __len__(self): return 8\n    def __getitem__(self, i): return i\n\n# BUG: 모듈 임포트만으로 워커가 스폰되어 Windows에서 프리즈\nloader = tud.DataLoader(Dummy(), batch_size=2, num_workers=4)\nfor b in loader:\n    pass\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai226/dataloader_main.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["if __name__ == \"__main__\":"]}, "code": "import torch, torch.utils.data as tud\n\nclass Dummy(torch.utils.data.Dataset):\n    def __len__(self): return 8\n    def __getitem__(self, i): return i\n\n\ndef main():\n    loader = tud.DataLoader(Dummy(), batch_size=2, num_workers=4, pin_memory=True, persistent_workers=True)\n    for _ in loader:\n        pass\n\n\nif __name__ == \"__main__\":\n    main()\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n재현성: 시드 함수 추가\n\n[CONTEXT]\n<<<FILE examples/ai52/seed_utils.py>>>\n# BUG: 시드 미설정\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai52/seed_utils.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["set_seed("]}, "code": "import os, random, numpy as np, torch\n\ndef set_seed(seed: int = 42):\n    random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n"}]}}
{"instruction": "Propose a minimal JSON plan for code changes. Output PLAN JSON only.", "input": "[INTENT]\n학습 안정화(AMP+훈련 설정)\n\n[CONTEXT]\n<<<FILE examples/ai676/train_utils.py>>>\ndef get_dtype():\n    return torch.float16\n\n<<<END>>>\n<<<FILE examples/ai676/training_args.py>>>\nargs = TrainingArguments(output_dir='out', per_device_train_batch_size=1, num_train_epochs=1)\n\n<<<END>>>", "output": {"files": [{"path": "examples/ai676/train_utils.py", "reason": "dtype가 고정 float16", "strategy": "regex", "tests": ["bf16 또는 fp16 반환"]}, {"path": "examples/ai676/training_args.py", "reason": "로깅/평가/GA 설정 누락", "strategy": "regex", "tests": ["eval_strategy=='steps'"]}], "notes": "bf16 지원 시 우선"}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n학습 후 어댑터 저장 추가\n\n[CONTEXT]\n<<<FILE examples/ai303/save_after_train.py>>>\nfrom peft import PeftModel\n# ... 학습 후\n# BUG: 저장 누락\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai303/save_after_train.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["save_pretrained"]}, "code": "from peft import PeftModel\n# ... 학습 후\nmodel.save_pretrained(\"training/qlora-out/export\")\n"}]}}
{"instruction": "Propose a minimal JSON plan for code changes. Output PLAN JSON only.", "input": "[INTENT]\n학습 안정화(AMP+훈련 설정)\n\n[CONTEXT]\n<<<FILE examples/ai670/train_utils.py>>>\ndef get_dtype():\n    return torch.float16\n\n<<<END>>>\n<<<FILE examples/ai670/training_args.py>>>\nargs = TrainingArguments(output_dir='out', per_device_train_batch_size=1, num_train_epochs=1)\n\n<<<END>>>", "output": {"files": [{"path": "examples/ai670/train_utils.py", "reason": "dtype가 고정 float16", "strategy": "regex", "tests": ["bf16 또는 fp16 반환"]}, {"path": "examples/ai670/training_args.py", "reason": "로깅/평가/GA 설정 누락", "strategy": "regex", "tests": ["eval_strategy=='steps'"]}], "notes": "bf16 지원 시 우선"}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n재현성: 시드 함수 추가\n\n[CONTEXT]\n<<<FILE examples/ai65/seed_utils.py>>>\n# BUG: 시드 미설정\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai65/seed_utils.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["set_seed("]}, "code": "import os, random, numpy as np, torch\n\ndef set_seed(seed: int = 42):\n    random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nWindows에서 DataLoader 프리즈 방지 + pin_memory/persistent_workers\n\n[CONTEXT]\n<<<FILE examples/ai19/dataloader_main.py>>>\nimport torch, torch.utils.data as tud\n\nclass Dummy(torch.utils.data.Dataset):\n    def __len__(self): return 8\n    def __getitem__(self, i): return i\n\n# BUG: 모듈 임포트만으로 워커가 스폰되어 Windows에서 프리즈\nloader = tud.DataLoader(Dummy(), batch_size=2, num_workers=4)\nfor b in loader:\n    pass\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai19/dataloader_main.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["if __name__ == \"__main__\":"]}, "code": "import torch, torch.utils.data as tud\n\nclass Dummy(torch.utils.data.Dataset):\n    def __len__(self): return 8\n    def __getitem__(self, i): return i\n\n\ndef main():\n    loader = tud.DataLoader(Dummy(), batch_size=2, num_workers=4, pin_memory=True, persistent_workers=True)\n    for _ in loader:\n        pass\n\n\nif __name__ == \"__main__\":\n    main()\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n재현성: 시드 함수 추가\n\n[CONTEXT]\n<<<FILE examples/ai254/seed_utils.py>>>\n# BUG: 시드 미설정\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai254/seed_utils.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["set_seed("]}, "code": "import os, random, numpy as np, torch\n\ndef set_seed(seed: int = 42):\n    random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n"}]}}
{"instruction": "Propose a minimal JSON plan for code changes. Output PLAN JSON only.", "input": "[INTENT]\n학습 안정화(AMP+훈련 설정)\n\n[CONTEXT]\n<<<FILE examples/ai773/train_utils.py>>>\ndef get_dtype():\n    return torch.float16\n\n<<<END>>>\n<<<FILE examples/ai773/training_args.py>>>\nargs = TrainingArguments(output_dir='out', per_device_train_batch_size=1, num_train_epochs=1)\n\n<<<END>>>", "output": {"files": [{"path": "examples/ai773/train_utils.py", "reason": "dtype가 고정 float16", "strategy": "regex", "tests": ["bf16 또는 fp16 반환"]}, {"path": "examples/ai773/training_args.py", "reason": "로깅/평가/GA 설정 누락", "strategy": "regex", "tests": ["eval_strategy=='steps'"]}], "notes": "bf16 지원 시 우선"}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nLoRA target_modules 확장\n\n[CONTEXT]\n<<<FILE examples/ai334/lora_cfg.py>>>\nfrom peft import LoraConfig\n\nLC = LoraConfig(r=8, target_modules=[\n    \"q_proj\", \"v_proj\"  # BUG: 부족\n])\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai334/lora_cfg.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["k_proj", "o_proj"]}, "code": "from peft import LoraConfig\n\nLC = LoraConfig(r=8, target_modules=[\n    \"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"\n])\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nflash 미지원 시 sdpa 안전 경로\n\n[CONTEXT]\n<<<FILE examples/ai340/model_load.py>>>\nfrom transformers import AutoModelForCausalLM\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"Qwen/Qwen3-8B\",\n)\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai340/model_load.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["attn_implementation"]}, "code": "from transformers import AutoModelForCausalLM\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"Qwen/Qwen3-8B\",\n    attn_implementation=\"sdpa\",\n)\n"}]}}
{"instruction": "Propose a minimal JSON plan for code changes. Output PLAN JSON only.", "input": "[INTENT]\n입출력/런타임 안전성 개선\n\n[CONTEXT]\n<<<FILE examples/ai738/dataloader_main.py>>>\nloader = DataLoader(...); for b in loader: pass\n\n<<<END>>>\n<<<FILE examples/ai738/ddp_main.py>>>\ntrainer()\n\n<<<END>>>", "output": {"files": [{"path": "examples/ai738/dataloader_main.py", "reason": "Windows freeze 가능", "strategy": "regex", "tests": ["__main__ 가드"]}, {"path": "examples/ai738/ddp_main.py", "reason": "DDP 가드 없음", "strategy": "regex", "tests": ["__main__"]}], "notes": "spawn 보호"}}
{"instruction": "Propose a minimal JSON plan for code changes. Output PLAN JSON only.", "input": "[INTENT]\n학습 안정화(AMP+훈련 설정)\n\n[CONTEXT]\n<<<FILE examples/ai702/train_utils.py>>>\ndef get_dtype():\n    return torch.float16\n\n<<<END>>>\n<<<FILE examples/ai702/training_args.py>>>\nargs = TrainingArguments(output_dir='out', per_device_train_batch_size=1, num_train_epochs=1)\n\n<<<END>>>", "output": {"files": [{"path": "examples/ai702/train_utils.py", "reason": "dtype가 고정 float16", "strategy": "regex", "tests": ["bf16 또는 fp16 반환"]}, {"path": "examples/ai702/training_args.py", "reason": "로깅/평가/GA 설정 누락", "strategy": "regex", "tests": ["eval_strategy=='steps'"]}], "notes": "bf16 지원 시 우선"}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\npad 토큰 ID 정확히 설정\n\n[CONTEXT]\n<<<FILE examples/ai605/label_cfg.py>>>\nfrom transformers import AutoTokenizer\nfrom dataclasses import dataclass\n\n@dataclass\nclass Cfg:\n    pad_id: int = -100  # BUG\n\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai605/label_cfg.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["CFG = Cfg"]}, "code": "from transformers import AutoTokenizer\nfrom dataclasses import dataclass\n\n@dataclass\nclass Cfg:\n    pad_id: int | None = None\n\n# tokenizer 로드 후 설정\ntok = AutoTokenizer.from_pretrained(\"Qwen/Qwen3-8B\", use_fast=True)\nCFG = Cfg(pad_id=tok.pad_token_id)\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nDDP/torchrun 안전 가드\n\n[CONTEXT]\n<<<FILE examples/ai243/ddp_main.py>>>\nimport torch\n\n# BUG: torchrun에서 main 가드 없음\ntrainer()\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai243/ddp_main.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["__main__"]}, "code": "import torch\n\n\ndef main():\n    trainer()\n\n\nif __name__ == \"__main__\":\n    main()\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n필수 import 추가\n\n[CONTEXT]\n<<<FILE examples/ai382/imports.py>>>\n# BUG: numpy 미사용이지만 추후 사용 예정\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai382/imports.py", "loc": {"type": "anchor", "before": "\\A"}, "action": "insert_after", "once": true, "pre": {"must_not_contain": ["import numpy as np"]}, "code": "import numpy as np\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n8bit 옵티마이저 + 코사인 스케줄러\n\n[CONTEXT]\n<<<FILE examples/ai163/optim.py>>>\nimport torch\nopt = torch.optim.Adam([], lr=3e-4)\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai163/optim.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["PagedAdamW8bit"]}, "code": "from transformers import get_scheduler\nfrom bitsandbytes.optim import PagedAdamW8bit\nopt = PagedAdamW8bit([], lr=2e-4)\nsched = get_scheduler(\"cosine\", optimizer=opt, num_warmup_steps=100, num_training_steps=1000)\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n체크포인트 재개 지원\n\n[CONTEXT]\n<<<FILE examples/ai23/trainer_resume.py>>>\nfrom transformers import Trainer\ntrainer = Trainer(... )\n# BUG: 재시작 미지원\ntrainer.train()\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai23/trainer_resume.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["resume_from_checkpoint"]}, "code": "from transformers import Trainer\ntrainer = Trainer(... )\ntrainer.train(resume_from_checkpoint=True)\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nAMP 적용 + bf16 우선\n\n[CONTEXT]\n<<<FILE examples/ai211/train_step.py>>>\nimport torch\n\n# BUG: autocast 미사용\ndef train_step(model, batch):\n    x, y = batch\n    out = model(x)\n    loss = out.loss if hasattr(out, 'loss') else out.mean()\n    loss.backward()\n    return loss.item()\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai211/train_step.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["autocast"]}, "code": "import torch\n\n@torch.no_grad()\ndef _bf16_supported():\n    return torch.cuda.is_available() and torch.cuda.is_bf16_supported()\n\n\ndef train_step(model, batch):\n    x, y = batch\n    amp_dtype = torch.bfloat16 if _bf16_supported() else torch.float16\n    scaler = torch.cuda.amp.GradScaler(enabled=True)\n    with torch.cuda.amp.autocast(enabled=True, dtype=amp_dtype):\n        out = model(x)\n        loss = out.loss if hasattr(out, 'loss') else out.mean()\n    scaler.scale(loss).backward()\n    scaler.step(torch.optim.Adam(model.parameters(), lr=1e-4))\n    scaler.update()\n    model.zero_grad(set_to_none=True)\n    return float(loss.detach().item())\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n체크포인트 재개 지원\n\n[CONTEXT]\n<<<FILE examples/ai398/trainer_resume.py>>>\nfrom transformers import Trainer\ntrainer = Trainer(... )\n# BUG: 재시작 미지원\ntrainer.train()\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai398/trainer_resume.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["resume_from_checkpoint"]}, "code": "from transformers import Trainer\ntrainer = Trainer(... )\ntrainer.train(resume_from_checkpoint=True)\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n8bit 옵티마이저 + 코사인 스케줄러\n\n[CONTEXT]\n<<<FILE examples/ai120/optim.py>>>\nimport torch\nopt = torch.optim.Adam([], lr=3e-4)\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai120/optim.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["PagedAdamW8bit"]}, "code": "from transformers import get_scheduler\nfrom bitsandbytes.optim import PagedAdamW8bit\nopt = PagedAdamW8bit([], lr=2e-4)\nsched = get_scheduler(\"cosine\", optimizer=opt, num_warmup_steps=100, num_training_steps=1000)\n"}]}}
{"instruction": "Propose a minimal JSON plan for code changes. Output PLAN JSON only.", "input": "[INTENT]\n학습 안정화(AMP+훈련 설정)\n\n[CONTEXT]\n<<<FILE examples/ai747/train_utils.py>>>\ndef get_dtype():\n    return torch.float16\n\n<<<END>>>\n<<<FILE examples/ai747/training_args.py>>>\nargs = TrainingArguments(output_dir='out', per_device_train_batch_size=1, num_train_epochs=1)\n\n<<<END>>>", "output": {"files": [{"path": "examples/ai747/train_utils.py", "reason": "dtype가 고정 float16", "strategy": "regex", "tests": ["bf16 또는 fp16 반환"]}, {"path": "examples/ai747/training_args.py", "reason": "로깅/평가/GA 설정 누락", "strategy": "regex", "tests": ["eval_strategy=='steps'"]}], "notes": "bf16 지원 시 우선"}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nLoRA target_modules 확장\n\n[CONTEXT]\n<<<FILE examples/ai293/lora_cfg.py>>>\nfrom peft import LoraConfig\n\nLC = LoraConfig(r=8, target_modules=[\n    \"q_proj\", \"v_proj\"  # BUG: 부족\n])\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai293/lora_cfg.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["k_proj", "o_proj"]}, "code": "from peft import LoraConfig\n\nLC = LoraConfig(r=8, target_modules=[\n    \"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"\n])\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nflash 미지원 시 sdpa 안전 경로\n\n[CONTEXT]\n<<<FILE examples/ai486/model_load.py>>>\nfrom transformers import AutoModelForCausalLM\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"Qwen/Qwen3-8B\",\n)\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai486/model_load.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["attn_implementation"]}, "code": "from transformers import AutoModelForCausalLM\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"Qwen/Qwen3-8B\",\n    attn_implementation=\"sdpa\",\n)\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nAMP 적용 + bf16 우선\n\n[CONTEXT]\n<<<FILE examples/ai531/train_step.py>>>\nimport torch\n\n# BUG: autocast 미사용\ndef train_step(model, batch):\n    x, y = batch\n    out = model(x)\n    loss = out.loss if hasattr(out, 'loss') else out.mean()\n    loss.backward()\n    return loss.item()\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai531/train_step.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["autocast"]}, "code": "import torch\n\n@torch.no_grad()\ndef _bf16_supported():\n    return torch.cuda.is_available() and torch.cuda.is_bf16_supported()\n\n\ndef train_step(model, batch):\n    x, y = batch\n    amp_dtype = torch.bfloat16 if _bf16_supported() else torch.float16\n    scaler = torch.cuda.amp.GradScaler(enabled=True)\n    with torch.cuda.amp.autocast(enabled=True, dtype=amp_dtype):\n        out = model(x)\n        loss = out.loss if hasattr(out, 'loss') else out.mean()\n    scaler.scale(loss).backward()\n    scaler.step(torch.optim.Adam(model.parameters(), lr=1e-4))\n    scaler.update()\n    model.zero_grad(set_to_none=True)\n    return float(loss.detach().item())\n"}]}}
{"instruction": "Propose a minimal JSON plan for code changes. Output PLAN JSON only.", "input": "[INTENT]\n입출력/런타임 안전성 개선\n\n[CONTEXT]\n<<<FILE examples/ai713/dataloader_main.py>>>\nloader = DataLoader(...); for b in loader: pass\n\n<<<END>>>\n<<<FILE examples/ai713/ddp_main.py>>>\ntrainer()\n\n<<<END>>>", "output": {"files": [{"path": "examples/ai713/dataloader_main.py", "reason": "Windows freeze 가능", "strategy": "regex", "tests": ["__main__ 가드"]}, {"path": "examples/ai713/ddp_main.py", "reason": "DDP 가드 없음", "strategy": "regex", "tests": ["__main__"]}], "notes": "spawn 보호"}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n훈련 안정화: GA steps/로깅/평가/체크포인팅/스케줄러\n\n[CONTEXT]\n<<<FILE examples/ai525/training_args.py>>>\nfrom transformers import TrainingArguments\n\nargs = TrainingArguments(\n    output_dir=\"out\",\n    per_device_train_batch_size=1,\n    num_train_epochs=1,\n)\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai525/training_args.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["gradient_accumulation_steps"]}, "code": "from transformers import TrainingArguments\n\nargs = TrainingArguments(\n    output_dir=\"out\",\n    per_device_train_batch_size=1,\n    gradient_accumulation_steps=8,\n    num_train_epochs=1,\n    logging_steps=10,\n    eval_strategy=\"steps\",\n    eval_steps=50,\n    save_steps=50,\n    lr_scheduler_type=\"cosine\",\n    warmup_ratio=0.03,\n    fp16=False, bf16=True,\n    gradient_checkpointing=True,\n    max_grad_norm=1.0,\n)\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n훈련 안정화: GA steps/로깅/평가/체크포인팅/스케줄러\n\n[CONTEXT]\n<<<FILE examples/ai534/training_args.py>>>\nfrom transformers import TrainingArguments\n\nargs = TrainingArguments(\n    output_dir=\"out\",\n    per_device_train_batch_size=1,\n    num_train_epochs=1,\n)\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai534/training_args.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["gradient_accumulation_steps"]}, "code": "from transformers import TrainingArguments\n\nargs = TrainingArguments(\n    output_dir=\"out\",\n    per_device_train_batch_size=1,\n    gradient_accumulation_steps=8,\n    num_train_epochs=1,\n    logging_steps=10,\n    eval_strategy=\"steps\",\n    eval_steps=50,\n    save_steps=50,\n    lr_scheduler_type=\"cosine\",\n    warmup_ratio=0.03,\n    fp16=False, bf16=True,\n    gradient_checkpointing=True,\n    max_grad_norm=1.0,\n)\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n재현성: 시드 함수 추가\n\n[CONTEXT]\n<<<FILE examples/ai194/seed_utils.py>>>\n# BUG: 시드 미설정\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai194/seed_utils.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["set_seed("]}, "code": "import os, random, numpy as np, torch\n\ndef set_seed(seed: int = 42):\n    random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n8bit 옵티마이저 + 코사인 스케줄러\n\n[CONTEXT]\n<<<FILE examples/ai556/optim.py>>>\nimport torch\nopt = torch.optim.Adam([], lr=3e-4)\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai556/optim.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["PagedAdamW8bit"]}, "code": "from transformers import get_scheduler\nfrom bitsandbytes.optim import PagedAdamW8bit\nopt = PagedAdamW8bit([], lr=2e-4)\nsched = get_scheduler(\"cosine\", optimizer=opt, num_warmup_steps=100, num_training_steps=1000)\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n조기 종료 콜백 추가\n\n[CONTEXT]\n<<<FILE examples/ai391/callbacks.py>>>\nfrom transformers import TrainerCallback\n\nclass MyCB(TrainerCallback):\n    pass\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai391/callbacks.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["EarlyStoppingCallback"]}, "code": "from transformers import TrainerCallback, EarlyStoppingCallback\n\nclass MyCB(TrainerCallback):\n    pass\n\nCALLBACKS = [EarlyStoppingCallback(early_stopping_patience=2)]\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n8bit 옵티마이저 + 코사인 스케줄러\n\n[CONTEXT]\n<<<FILE examples/ai505/optim.py>>>\nimport torch\nopt = torch.optim.Adam([], lr=3e-4)\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai505/optim.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["PagedAdamW8bit"]}, "code": "from transformers import get_scheduler\nfrom bitsandbytes.optim import PagedAdamW8bit\nopt = PagedAdamW8bit([], lr=2e-4)\nsched = get_scheduler(\"cosine\", optimizer=opt, num_warmup_steps=100, num_training_steps=1000)\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n체크포인트 재개 지원\n\n[CONTEXT]\n<<<FILE examples/ai171/trainer_resume.py>>>\nfrom transformers import Trainer\ntrainer = Trainer(... )\n# BUG: 재시작 미지원\ntrainer.train()\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai171/trainer_resume.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["resume_from_checkpoint"]}, "code": "from transformers import Trainer\ntrainer = Trainer(... )\ntrainer.train(resume_from_checkpoint=True)\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nAMP 적용 + bf16 우선\n\n[CONTEXT]\n<<<FILE examples/ai177/train_step.py>>>\nimport torch\n\n# BUG: autocast 미사용\ndef train_step(model, batch):\n    x, y = batch\n    out = model(x)\n    loss = out.loss if hasattr(out, 'loss') else out.mean()\n    loss.backward()\n    return loss.item()\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai177/train_step.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["autocast"]}, "code": "import torch\n\n@torch.no_grad()\ndef _bf16_supported():\n    return torch.cuda.is_available() and torch.cuda.is_bf16_supported()\n\n\ndef train_step(model, batch):\n    x, y = batch\n    amp_dtype = torch.bfloat16 if _bf16_supported() else torch.float16\n    scaler = torch.cuda.amp.GradScaler(enabled=True)\n    with torch.cuda.amp.autocast(enabled=True, dtype=amp_dtype):\n        out = model(x)\n        loss = out.loss if hasattr(out, 'loss') else out.mean()\n    scaler.scale(loss).backward()\n    scaler.step(torch.optim.Adam(model.parameters(), lr=1e-4))\n    scaler.update()\n    model.zero_grad(set_to_none=True)\n    return float(loss.detach().item())\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n재현성: 시드 함수 추가\n\n[CONTEXT]\n<<<FILE examples/ai561/seed_utils.py>>>\n# BUG: 시드 미설정\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai561/seed_utils.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["set_seed("]}, "code": "import os, random, numpy as np, torch\n\ndef set_seed(seed: int = 42):\n    random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nLoRA target_modules 확장\n\n[CONTEXT]\n<<<FILE examples/ai542/lora_cfg.py>>>\nfrom peft import LoraConfig\n\nLC = LoraConfig(r=8, target_modules=[\n    \"q_proj\", \"v_proj\"  # BUG: 부족\n])\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai542/lora_cfg.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["k_proj", "o_proj"]}, "code": "from peft import LoraConfig\n\nLC = LoraConfig(r=8, target_modules=[\n    \"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"\n])\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nDDP/torchrun 안전 가드\n\n[CONTEXT]\n<<<FILE examples/ai229/ddp_main.py>>>\nimport torch\n\n# BUG: torchrun에서 main 가드 없음\ntrainer()\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai229/ddp_main.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["__main__"]}, "code": "import torch\n\n\ndef main():\n    trainer()\n\n\nif __name__ == \"__main__\":\n    main()\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nflash 미지원 시 sdpa 안전 경로\n\n[CONTEXT]\n<<<FILE examples/ai589/model_load.py>>>\nfrom transformers import AutoModelForCausalLM\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"Qwen/Qwen3-8B\",\n)\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai589/model_load.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["attn_implementation"]}, "code": "from transformers import AutoModelForCausalLM\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"Qwen/Qwen3-8B\",\n    attn_implementation=\"sdpa\",\n)\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nAMP 적용 + bf16 우선\n\n[CONTEXT]\n<<<FILE examples/ai251/train_step.py>>>\nimport torch\n\n# BUG: autocast 미사용\ndef train_step(model, batch):\n    x, y = batch\n    out = model(x)\n    loss = out.loss if hasattr(out, 'loss') else out.mean()\n    loss.backward()\n    return loss.item()\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai251/train_step.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["autocast"]}, "code": "import torch\n\n@torch.no_grad()\ndef _bf16_supported():\n    return torch.cuda.is_available() and torch.cuda.is_bf16_supported()\n\n\ndef train_step(model, batch):\n    x, y = batch\n    amp_dtype = torch.bfloat16 if _bf16_supported() else torch.float16\n    scaler = torch.cuda.amp.GradScaler(enabled=True)\n    with torch.cuda.amp.autocast(enabled=True, dtype=amp_dtype):\n        out = model(x)\n        loss = out.loss if hasattr(out, 'loss') else out.mean()\n    scaler.scale(loss).backward()\n    scaler.step(torch.optim.Adam(model.parameters(), lr=1e-4))\n    scaler.update()\n    model.zero_grad(set_to_none=True)\n    return float(loss.detach().item())\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n조기 종료 콜백 추가\n\n[CONTEXT]\n<<<FILE examples/ai524/callbacks.py>>>\nfrom transformers import TrainerCallback\n\nclass MyCB(TrainerCallback):\n    pass\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai524/callbacks.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["EarlyStoppingCallback"]}, "code": "from transformers import TrainerCallback, EarlyStoppingCallback\n\nclass MyCB(TrainerCallback):\n    pass\n\nCALLBACKS = [EarlyStoppingCallback(early_stopping_patience=2)]\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nLoRA target_modules 확장\n\n[CONTEXT]\n<<<FILE examples/ai297/lora_cfg.py>>>\nfrom peft import LoraConfig\n\nLC = LoraConfig(r=8, target_modules=[\n    \"q_proj\", \"v_proj\"  # BUG: 부족\n])\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai297/lora_cfg.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["k_proj", "o_proj"]}, "code": "from peft import LoraConfig\n\nLC = LoraConfig(r=8, target_modules=[\n    \"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"\n])\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n체크포인트 재개 지원\n\n[CONTEXT]\n<<<FILE examples/ai301/trainer_resume.py>>>\nfrom transformers import Trainer\ntrainer = Trainer(... )\n# BUG: 재시작 미지원\ntrainer.train()\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai301/trainer_resume.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["resume_from_checkpoint"]}, "code": "from transformers import Trainer\ntrainer = Trainer(... )\ntrainer.train(resume_from_checkpoint=True)\n"}]}}
{"instruction": "Propose a minimal JSON plan for code changes. Output PLAN JSON only.", "input": "[INTENT]\n입출력/런타임 안전성 개선\n\n[CONTEXT]\n<<<FILE examples/ai697/dataloader_main.py>>>\nloader = DataLoader(...); for b in loader: pass\n\n<<<END>>>\n<<<FILE examples/ai697/ddp_main.py>>>\ntrainer()\n\n<<<END>>>", "output": {"files": [{"path": "examples/ai697/dataloader_main.py", "reason": "Windows freeze 가능", "strategy": "regex", "tests": ["__main__ 가드"]}, {"path": "examples/ai697/ddp_main.py", "reason": "DDP 가드 없음", "strategy": "regex", "tests": ["__main__"]}], "notes": "spawn 보호"}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nflash 미지원 시 sdpa 안전 경로\n\n[CONTEXT]\n<<<FILE examples/ai372/model_load.py>>>\nfrom transformers import AutoModelForCausalLM\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"Qwen/Qwen3-8B\",\n)\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai372/model_load.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["attn_implementation"]}, "code": "from transformers import AutoModelForCausalLM\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"Qwen/Qwen3-8B\",\n    attn_implementation=\"sdpa\",\n)\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nLoRA target_modules 확장\n\n[CONTEXT]\n<<<FILE examples/ai145/lora_cfg.py>>>\nfrom peft import LoraConfig\n\nLC = LoraConfig(r=8, target_modules=[\n    \"q_proj\", \"v_proj\"  # BUG: 부족\n])\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai145/lora_cfg.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["k_proj", "o_proj"]}, "code": "from peft import LoraConfig\n\nLC = LoraConfig(r=8, target_modules=[\n    \"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"\n])\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nWindows에서 DataLoader 프리즈 방지 + pin_memory/persistent_workers\n\n[CONTEXT]\n<<<FILE examples/ai2/dataloader_main.py>>>\nimport torch, torch.utils.data as tud\n\nclass Dummy(torch.utils.data.Dataset):\n    def __len__(self): return 8\n    def __getitem__(self, i): return i\n\n# BUG: 모듈 임포트만으로 워커가 스폰되어 Windows에서 프리즈\nloader = tud.DataLoader(Dummy(), batch_size=2, num_workers=4)\nfor b in loader:\n    pass\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai2/dataloader_main.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["if __name__ == \"__main__\":"]}, "code": "import torch, torch.utils.data as tud\n\nclass Dummy(torch.utils.data.Dataset):\n    def __len__(self): return 8\n    def __getitem__(self, i): return i\n\n\ndef main():\n    loader = tud.DataLoader(Dummy(), batch_size=2, num_workers=4, pin_memory=True, persistent_workers=True)\n    for _ in loader:\n        pass\n\n\nif __name__ == \"__main__\":\n    main()\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nGPU bf16 지원 시 dtype 선택 개선\n\n[CONTEXT]\n<<<FILE examples/ai421/train_utils.py>>>\nimport torch\n\n# BUG: dtype가 고정되어 혼합정밀에 불리\ndef get_dtype():\n    return torch.float16\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai421/train_utils.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["torch.bfloat16"]}, "code": "import torch\n\n\ndef get_dtype():\n    \"\"\"GPU가 bf16 지원 시 bfloat16, 아니면 float16 반환\"\"\"\n    if torch.cuda.is_available() and torch.cuda.is_bf16_supported():\n        return torch.bfloat16\n    return torch.float16\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nLoRA target_modules 확장\n\n[CONTEXT]\n<<<FILE examples/ai489/lora_cfg.py>>>\nfrom peft import LoraConfig\n\nLC = LoraConfig(r=8, target_modules=[\n    \"q_proj\", \"v_proj\"  # BUG: 부족\n])\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai489/lora_cfg.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["k_proj", "o_proj"]}, "code": "from peft import LoraConfig\n\nLC = LoraConfig(r=8, target_modules=[\n    \"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"\n])\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nLoRA target_modules 확장\n\n[CONTEXT]\n<<<FILE examples/ai260/lora_cfg.py>>>\nfrom peft import LoraConfig\n\nLC = LoraConfig(r=8, target_modules=[\n    \"q_proj\", \"v_proj\"  # BUG: 부족\n])\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai260/lora_cfg.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["k_proj", "o_proj"]}, "code": "from peft import LoraConfig\n\nLC = LoraConfig(r=8, target_modules=[\n    \"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"\n])\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nGPU bf16 지원 시 dtype 선택 개선\n\n[CONTEXT]\n<<<FILE examples/ai404/train_utils.py>>>\nimport torch\n\n# BUG: dtype가 고정되어 혼합정밀에 불리\ndef get_dtype():\n    return torch.float16\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai404/train_utils.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["torch.bfloat16"]}, "code": "import torch\n\n\ndef get_dtype():\n    \"\"\"GPU가 bf16 지원 시 bfloat16, 아니면 float16 반환\"\"\"\n    if torch.cuda.is_available() and torch.cuda.is_bf16_supported():\n        return torch.bfloat16\n    return torch.float16\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n체크포인트 재개 지원\n\n[CONTEXT]\n<<<FILE examples/ai175/trainer_resume.py>>>\nfrom transformers import Trainer\ntrainer = Trainer(... )\n# BUG: 재시작 미지원\ntrainer.train()\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai175/trainer_resume.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["resume_from_checkpoint"]}, "code": "from transformers import Trainer\ntrainer = Trainer(... )\ntrainer.train(resume_from_checkpoint=True)\n"}]}}
{"instruction": "Propose a minimal JSON plan for code changes. Output PLAN JSON only.", "input": "[INTENT]\n학습 안정화(AMP+훈련 설정)\n\n[CONTEXT]\n<<<FILE examples/ai787/train_utils.py>>>\ndef get_dtype():\n    return torch.float16\n\n<<<END>>>\n<<<FILE examples/ai787/training_args.py>>>\nargs = TrainingArguments(output_dir='out', per_device_train_batch_size=1, num_train_epochs=1)\n\n<<<END>>>", "output": {"files": [{"path": "examples/ai787/train_utils.py", "reason": "dtype가 고정 float16", "strategy": "regex", "tests": ["bf16 또는 fp16 반환"]}, {"path": "examples/ai787/training_args.py", "reason": "로깅/평가/GA 설정 누락", "strategy": "regex", "tests": ["eval_strategy=='steps'"]}], "notes": "bf16 지원 시 우선"}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nAMP 적용 + bf16 우선\n\n[CONTEXT]\n<<<FILE examples/ai405/train_step.py>>>\nimport torch\n\n# BUG: autocast 미사용\ndef train_step(model, batch):\n    x, y = batch\n    out = model(x)\n    loss = out.loss if hasattr(out, 'loss') else out.mean()\n    loss.backward()\n    return loss.item()\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai405/train_step.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["autocast"]}, "code": "import torch\n\n@torch.no_grad()\ndef _bf16_supported():\n    return torch.cuda.is_available() and torch.cuda.is_bf16_supported()\n\n\ndef train_step(model, batch):\n    x, y = batch\n    amp_dtype = torch.bfloat16 if _bf16_supported() else torch.float16\n    scaler = torch.cuda.amp.GradScaler(enabled=True)\n    with torch.cuda.amp.autocast(enabled=True, dtype=amp_dtype):\n        out = model(x)\n        loss = out.loss if hasattr(out, 'loss') else out.mean()\n    scaler.scale(loss).backward()\n    scaler.step(torch.optim.Adam(model.parameters(), lr=1e-4))\n    scaler.update()\n    model.zero_grad(set_to_none=True)\n    return float(loss.detach().item())\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nGPU bf16 지원 시 dtype 선택 개선\n\n[CONTEXT]\n<<<FILE examples/ai396/train_utils.py>>>\nimport torch\n\n# BUG: dtype가 고정되어 혼합정밀에 불리\ndef get_dtype():\n    return torch.float16\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai396/train_utils.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["torch.bfloat16"]}, "code": "import torch\n\n\ndef get_dtype():\n    \"\"\"GPU가 bf16 지원 시 bfloat16, 아니면 float16 반환\"\"\"\n    if torch.cuda.is_available() and torch.cuda.is_bf16_supported():\n        return torch.bfloat16\n    return torch.float16\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n훈련 안정화: GA steps/로깅/평가/체크포인팅/스케줄러\n\n[CONTEXT]\n<<<FILE examples/ai142/training_args.py>>>\nfrom transformers import TrainingArguments\n\nargs = TrainingArguments(\n    output_dir=\"out\",\n    per_device_train_batch_size=1,\n    num_train_epochs=1,\n)\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai142/training_args.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["gradient_accumulation_steps"]}, "code": "from transformers import TrainingArguments\n\nargs = TrainingArguments(\n    output_dir=\"out\",\n    per_device_train_batch_size=1,\n    gradient_accumulation_steps=8,\n    num_train_epochs=1,\n    logging_steps=10,\n    eval_strategy=\"steps\",\n    eval_steps=50,\n    save_steps=50,\n    lr_scheduler_type=\"cosine\",\n    warmup_ratio=0.03,\n    fp16=False, bf16=True,\n    gradient_checkpointing=True,\n    max_grad_norm=1.0,\n)\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nDDP/torchrun 안전 가드\n\n[CONTEXT]\n<<<FILE examples/ai36/ddp_main.py>>>\nimport torch\n\n# BUG: torchrun에서 main 가드 없음\ntrainer()\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai36/ddp_main.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["__main__"]}, "code": "import torch\n\n\ndef main():\n    trainer()\n\n\nif __name__ == \"__main__\":\n    main()\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n필수 import 추가\n\n[CONTEXT]\n<<<FILE examples/ai70/imports.py>>>\n# BUG: numpy 미사용이지만 추후 사용 예정\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai70/imports.py", "loc": {"type": "anchor", "before": "\\A"}, "action": "insert_after", "once": true, "pre": {"must_not_contain": ["import numpy as np"]}, "code": "import numpy as np\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n조기 종료 콜백 추가\n\n[CONTEXT]\n<<<FILE examples/ai383/callbacks.py>>>\nfrom transformers import TrainerCallback\n\nclass MyCB(TrainerCallback):\n    pass\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai383/callbacks.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["EarlyStoppingCallback"]}, "code": "from transformers import TrainerCallback, EarlyStoppingCallback\n\nclass MyCB(TrainerCallback):\n    pass\n\nCALLBACKS = [EarlyStoppingCallback(early_stopping_patience=2)]\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nflash 미지원 시 sdpa 안전 경로\n\n[CONTEXT]\n<<<FILE examples/ai455/model_load.py>>>\nfrom transformers import AutoModelForCausalLM\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"Qwen/Qwen3-8B\",\n)\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai455/model_load.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["attn_implementation"]}, "code": "from transformers import AutoModelForCausalLM\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"Qwen/Qwen3-8B\",\n    attn_implementation=\"sdpa\",\n)\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nDDP/torchrun 안전 가드\n\n[CONTEXT]\n<<<FILE examples/ai307/ddp_main.py>>>\nimport torch\n\n# BUG: torchrun에서 main 가드 없음\ntrainer()\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai307/ddp_main.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["__main__"]}, "code": "import torch\n\n\ndef main():\n    trainer()\n\n\nif __name__ == \"__main__\":\n    main()\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n필수 import 추가\n\n[CONTEXT]\n<<<FILE examples/ai98/imports.py>>>\n# BUG: numpy 미사용이지만 추후 사용 예정\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai98/imports.py", "loc": {"type": "anchor", "before": "\\A"}, "action": "insert_after", "once": true, "pre": {"must_not_contain": ["import numpy as np"]}, "code": "import numpy as np\n"}]}}
{"instruction": "Propose a minimal JSON plan for code changes. Output PLAN JSON only.", "input": "[INTENT]\n입출력/런타임 안전성 개선\n\n[CONTEXT]\n<<<FILE examples/ai778/dataloader_main.py>>>\nloader = DataLoader(...); for b in loader: pass\n\n<<<END>>>\n<<<FILE examples/ai778/ddp_main.py>>>\ntrainer()\n\n<<<END>>>", "output": {"files": [{"path": "examples/ai778/dataloader_main.py", "reason": "Windows freeze 가능", "strategy": "regex", "tests": ["__main__ 가드"]}, {"path": "examples/ai778/ddp_main.py", "reason": "DDP 가드 없음", "strategy": "regex", "tests": ["__main__"]}], "notes": "spawn 보호"}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nLoRA target_modules 확장\n\n[CONTEXT]\n<<<FILE examples/ai75/lora_cfg.py>>>\nfrom peft import LoraConfig\n\nLC = LoraConfig(r=8, target_modules=[\n    \"q_proj\", \"v_proj\"  # BUG: 부족\n])\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai75/lora_cfg.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["k_proj", "o_proj"]}, "code": "from peft import LoraConfig\n\nLC = LoraConfig(r=8, target_modules=[\n    \"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"\n])\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nDDP/torchrun 안전 가드\n\n[CONTEXT]\n<<<FILE examples/ai469/ddp_main.py>>>\nimport torch\n\n# BUG: torchrun에서 main 가드 없음\ntrainer()\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai469/ddp_main.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["__main__"]}, "code": "import torch\n\n\ndef main():\n    trainer()\n\n\nif __name__ == \"__main__\":\n    main()\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n체크포인트 재개 지원\n\n[CONTEXT]\n<<<FILE examples/ai234/trainer_resume.py>>>\nfrom transformers import Trainer\ntrainer = Trainer(... )\n# BUG: 재시작 미지원\ntrainer.train()\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai234/trainer_resume.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["resume_from_checkpoint"]}, "code": "from transformers import Trainer\ntrainer = Trainer(... )\ntrainer.train(resume_from_checkpoint=True)\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n8bit 옵티마이저 + 코사인 스케줄러\n\n[CONTEXT]\n<<<FILE examples/ai125/optim.py>>>\nimport torch\nopt = torch.optim.Adam([], lr=3e-4)\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai125/optim.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["PagedAdamW8bit"]}, "code": "from transformers import get_scheduler\nfrom bitsandbytes.optim import PagedAdamW8bit\nopt = PagedAdamW8bit([], lr=2e-4)\nsched = get_scheduler(\"cosine\", optimizer=opt, num_warmup_steps=100, num_training_steps=1000)\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nAMP 적용 + bf16 우선\n\n[CONTEXT]\n<<<FILE examples/ai242/train_step.py>>>\nimport torch\n\n# BUG: autocast 미사용\ndef train_step(model, batch):\n    x, y = batch\n    out = model(x)\n    loss = out.loss if hasattr(out, 'loss') else out.mean()\n    loss.backward()\n    return loss.item()\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai242/train_step.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["autocast"]}, "code": "import torch\n\n@torch.no_grad()\ndef _bf16_supported():\n    return torch.cuda.is_available() and torch.cuda.is_bf16_supported()\n\n\ndef train_step(model, batch):\n    x, y = batch\n    amp_dtype = torch.bfloat16 if _bf16_supported() else torch.float16\n    scaler = torch.cuda.amp.GradScaler(enabled=True)\n    with torch.cuda.amp.autocast(enabled=True, dtype=amp_dtype):\n        out = model(x)\n        loss = out.loss if hasattr(out, 'loss') else out.mean()\n    scaler.scale(loss).backward()\n    scaler.step(torch.optim.Adam(model.parameters(), lr=1e-4))\n    scaler.update()\n    model.zero_grad(set_to_none=True)\n    return float(loss.detach().item())\n"}]}}
{"instruction": "Propose a minimal JSON plan for code changes. Output PLAN JSON only.", "input": "[INTENT]\n입출력/런타임 안전성 개선\n\n[CONTEXT]\n<<<FILE examples/ai700/dataloader_main.py>>>\nloader = DataLoader(...); for b in loader: pass\n\n<<<END>>>\n<<<FILE examples/ai700/ddp_main.py>>>\ntrainer()\n\n<<<END>>>", "output": {"files": [{"path": "examples/ai700/dataloader_main.py", "reason": "Windows freeze 가능", "strategy": "regex", "tests": ["__main__ 가드"]}, {"path": "examples/ai700/ddp_main.py", "reason": "DDP 가드 없음", "strategy": "regex", "tests": ["__main__"]}], "notes": "spawn 보호"}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nAMP 적용 + bf16 우선\n\n[CONTEXT]\n<<<FILE examples/ai428/train_step.py>>>\nimport torch\n\n# BUG: autocast 미사용\ndef train_step(model, batch):\n    x, y = batch\n    out = model(x)\n    loss = out.loss if hasattr(out, 'loss') else out.mean()\n    loss.backward()\n    return loss.item()\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai428/train_step.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["autocast"]}, "code": "import torch\n\n@torch.no_grad()\ndef _bf16_supported():\n    return torch.cuda.is_available() and torch.cuda.is_bf16_supported()\n\n\ndef train_step(model, batch):\n    x, y = batch\n    amp_dtype = torch.bfloat16 if _bf16_supported() else torch.float16\n    scaler = torch.cuda.amp.GradScaler(enabled=True)\n    with torch.cuda.amp.autocast(enabled=True, dtype=amp_dtype):\n        out = model(x)\n        loss = out.loss if hasattr(out, 'loss') else out.mean()\n    scaler.scale(loss).backward()\n    scaler.step(torch.optim.Adam(model.parameters(), lr=1e-4))\n    scaler.update()\n    model.zero_grad(set_to_none=True)\n    return float(loss.detach().item())\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nLoRA target_modules 확장\n\n[CONTEXT]\n<<<FILE examples/ai597/lora_cfg.py>>>\nfrom peft import LoraConfig\n\nLC = LoraConfig(r=8, target_modules=[\n    \"q_proj\", \"v_proj\"  # BUG: 부족\n])\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai597/lora_cfg.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["k_proj", "o_proj"]}, "code": "from peft import LoraConfig\n\nLC = LoraConfig(r=8, target_modules=[\n    \"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"\n])\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n필수 import 추가\n\n[CONTEXT]\n<<<FILE examples/ai140/imports.py>>>\n# BUG: numpy 미사용이지만 추후 사용 예정\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai140/imports.py", "loc": {"type": "anchor", "before": "\\A"}, "action": "insert_after", "once": true, "pre": {"must_not_contain": ["import numpy as np"]}, "code": "import numpy as np\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n학습 후 어댑터 저장 추가\n\n[CONTEXT]\n<<<FILE examples/ai362/save_after_train.py>>>\nfrom peft import PeftModel\n# ... 학습 후\n# BUG: 저장 누락\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai362/save_after_train.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["save_pretrained"]}, "code": "from peft import PeftModel\n# ... 학습 후\nmodel.save_pretrained(\"training/qlora-out/export\")\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n조기 종료 콜백 추가\n\n[CONTEXT]\n<<<FILE examples/ai261/callbacks.py>>>\nfrom transformers import TrainerCallback\n\nclass MyCB(TrainerCallback):\n    pass\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai261/callbacks.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["EarlyStoppingCallback"]}, "code": "from transformers import TrainerCallback, EarlyStoppingCallback\n\nclass MyCB(TrainerCallback):\n    pass\n\nCALLBACKS = [EarlyStoppingCallback(early_stopping_patience=2)]\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n조기 종료 콜백 추가\n\n[CONTEXT]\n<<<FILE examples/ai150/callbacks.py>>>\nfrom transformers import TrainerCallback\n\nclass MyCB(TrainerCallback):\n    pass\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai150/callbacks.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["EarlyStoppingCallback"]}, "code": "from transformers import TrainerCallback, EarlyStoppingCallback\n\nclass MyCB(TrainerCallback):\n    pass\n\nCALLBACKS = [EarlyStoppingCallback(early_stopping_patience=2)]\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nGPU bf16 지원 시 dtype 선택 개선\n\n[CONTEXT]\n<<<FILE examples/ai367/train_utils.py>>>\nimport torch\n\n# BUG: dtype가 고정되어 혼합정밀에 불리\ndef get_dtype():\n    return torch.float16\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai367/train_utils.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["torch.bfloat16"]}, "code": "import torch\n\n\ndef get_dtype():\n    \"\"\"GPU가 bf16 지원 시 bfloat16, 아니면 float16 반환\"\"\"\n    if torch.cuda.is_available() and torch.cuda.is_bf16_supported():\n        return torch.bfloat16\n    return torch.float16\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n8bit 옵티마이저 + 코사인 스케줄러\n\n[CONTEXT]\n<<<FILE examples/ai495/optim.py>>>\nimport torch\nopt = torch.optim.Adam([], lr=3e-4)\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai495/optim.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["PagedAdamW8bit"]}, "code": "from transformers import get_scheduler\nfrom bitsandbytes.optim import PagedAdamW8bit\nopt = PagedAdamW8bit([], lr=2e-4)\nsched = get_scheduler(\"cosine\", optimizer=opt, num_warmup_steps=100, num_training_steps=1000)\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n필수 import 추가\n\n[CONTEXT]\n<<<FILE examples/ai79/imports.py>>>\n# BUG: numpy 미사용이지만 추후 사용 예정\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai79/imports.py", "loc": {"type": "anchor", "before": "\\A"}, "action": "insert_after", "once": true, "pre": {"must_not_contain": ["import numpy as np"]}, "code": "import numpy as np\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n8bit 옵티마이저 + 코사인 스케줄러\n\n[CONTEXT]\n<<<FILE examples/ai610/optim.py>>>\nimport torch\nopt = torch.optim.Adam([], lr=3e-4)\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai610/optim.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["PagedAdamW8bit"]}, "code": "from transformers import get_scheduler\nfrom bitsandbytes.optim import PagedAdamW8bit\nopt = PagedAdamW8bit([], lr=2e-4)\nsched = get_scheduler(\"cosine\", optimizer=opt, num_warmup_steps=100, num_training_steps=1000)\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n훈련 안정화: GA steps/로깅/평가/체크포인팅/스케줄러\n\n[CONTEXT]\n<<<FILE examples/ai514/training_args.py>>>\nfrom transformers import TrainingArguments\n\nargs = TrainingArguments(\n    output_dir=\"out\",\n    per_device_train_batch_size=1,\n    num_train_epochs=1,\n)\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai514/training_args.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["gradient_accumulation_steps"]}, "code": "from transformers import TrainingArguments\n\nargs = TrainingArguments(\n    output_dir=\"out\",\n    per_device_train_batch_size=1,\n    gradient_accumulation_steps=8,\n    num_train_epochs=1,\n    logging_steps=10,\n    eval_strategy=\"steps\",\n    eval_steps=50,\n    save_steps=50,\n    lr_scheduler_type=\"cosine\",\n    warmup_ratio=0.03,\n    fp16=False, bf16=True,\n    gradient_checkpointing=True,\n    max_grad_norm=1.0,\n)\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n조기 종료 콜백 추가\n\n[CONTEXT]\n<<<FILE examples/ai61/callbacks.py>>>\nfrom transformers import TrainerCallback\n\nclass MyCB(TrainerCallback):\n    pass\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai61/callbacks.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["EarlyStoppingCallback"]}, "code": "from transformers import TrainerCallback, EarlyStoppingCallback\n\nclass MyCB(TrainerCallback):\n    pass\n\nCALLBACKS = [EarlyStoppingCallback(early_stopping_patience=2)]\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nGPU bf16 지원 시 dtype 선택 개선\n\n[CONTEXT]\n<<<FILE examples/ai638/train_utils.py>>>\nimport torch\n\n# BUG: dtype가 고정되어 혼합정밀에 불리\ndef get_dtype():\n    return torch.float16\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai638/train_utils.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["torch.bfloat16"]}, "code": "import torch\n\n\ndef get_dtype():\n    \"\"\"GPU가 bf16 지원 시 bfloat16, 아니면 float16 반환\"\"\"\n    if torch.cuda.is_available() and torch.cuda.is_bf16_supported():\n        return torch.bfloat16\n    return torch.float16\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nDDP/torchrun 안전 가드\n\n[CONTEXT]\n<<<FILE examples/ai59/ddp_main.py>>>\nimport torch\n\n# BUG: torchrun에서 main 가드 없음\ntrainer()\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai59/ddp_main.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["__main__"]}, "code": "import torch\n\n\ndef main():\n    trainer()\n\n\nif __name__ == \"__main__\":\n    main()\n"}]}}
{"instruction": "Propose a minimal JSON plan for code changes. Output PLAN JSON only.", "input": "[INTENT]\n입출력/런타임 안전성 개선\n\n[CONTEXT]\n<<<FILE examples/ai750/dataloader_main.py>>>\nloader = DataLoader(...); for b in loader: pass\n\n<<<END>>>\n<<<FILE examples/ai750/ddp_main.py>>>\ntrainer()\n\n<<<END>>>", "output": {"files": [{"path": "examples/ai750/dataloader_main.py", "reason": "Windows freeze 가능", "strategy": "regex", "tests": ["__main__ 가드"]}, {"path": "examples/ai750/ddp_main.py", "reason": "DDP 가드 없음", "strategy": "regex", "tests": ["__main__"]}], "notes": "spawn 보호"}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nAMP 적용 + bf16 우선\n\n[CONTEXT]\n<<<FILE examples/ai116/train_step.py>>>\nimport torch\n\n# BUG: autocast 미사용\ndef train_step(model, batch):\n    x, y = batch\n    out = model(x)\n    loss = out.loss if hasattr(out, 'loss') else out.mean()\n    loss.backward()\n    return loss.item()\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai116/train_step.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["autocast"]}, "code": "import torch\n\n@torch.no_grad()\ndef _bf16_supported():\n    return torch.cuda.is_available() and torch.cuda.is_bf16_supported()\n\n\ndef train_step(model, batch):\n    x, y = batch\n    amp_dtype = torch.bfloat16 if _bf16_supported() else torch.float16\n    scaler = torch.cuda.amp.GradScaler(enabled=True)\n    with torch.cuda.amp.autocast(enabled=True, dtype=amp_dtype):\n        out = model(x)\n        loss = out.loss if hasattr(out, 'loss') else out.mean()\n    scaler.scale(loss).backward()\n    scaler.step(torch.optim.Adam(model.parameters(), lr=1e-4))\n    scaler.update()\n    model.zero_grad(set_to_none=True)\n    return float(loss.detach().item())\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nDDP/torchrun 안전 가드\n\n[CONTEXT]\n<<<FILE examples/ai48/ddp_main.py>>>\nimport torch\n\n# BUG: torchrun에서 main 가드 없음\ntrainer()\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai48/ddp_main.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["__main__"]}, "code": "import torch\n\n\ndef main():\n    trainer()\n\n\nif __name__ == \"__main__\":\n    main()\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n8bit 옵티마이저 + 코사인 스케줄러\n\n[CONTEXT]\n<<<FILE examples/ai209/optim.py>>>\nimport torch\nopt = torch.optim.Adam([], lr=3e-4)\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai209/optim.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["PagedAdamW8bit"]}, "code": "from transformers import get_scheduler\nfrom bitsandbytes.optim import PagedAdamW8bit\nopt = PagedAdamW8bit([], lr=2e-4)\nsched = get_scheduler(\"cosine\", optimizer=opt, num_warmup_steps=100, num_training_steps=1000)\n"}]}}
{"instruction": "Propose a minimal JSON plan for code changes. Output PLAN JSON only.", "input": "[INTENT]\n학습 안정화(AMP+훈련 설정)\n\n[CONTEXT]\n<<<FILE examples/ai744/train_utils.py>>>\ndef get_dtype():\n    return torch.float16\n\n<<<END>>>\n<<<FILE examples/ai744/training_args.py>>>\nargs = TrainingArguments(output_dir='out', per_device_train_batch_size=1, num_train_epochs=1)\n\n<<<END>>>", "output": {"files": [{"path": "examples/ai744/train_utils.py", "reason": "dtype가 고정 float16", "strategy": "regex", "tests": ["bf16 또는 fp16 반환"]}, {"path": "examples/ai744/training_args.py", "reason": "로깅/평가/GA 설정 누락", "strategy": "regex", "tests": ["eval_strategy=='steps'"]}], "notes": "bf16 지원 시 우선"}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n훈련 안정화: GA steps/로깅/평가/체크포인팅/스케줄러\n\n[CONTEXT]\n<<<FILE examples/ai8/training_args.py>>>\nfrom transformers import TrainingArguments\n\nargs = TrainingArguments(\n    output_dir=\"out\",\n    per_device_train_batch_size=1,\n    num_train_epochs=1,\n)\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai8/training_args.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["gradient_accumulation_steps"]}, "code": "from transformers import TrainingArguments\n\nargs = TrainingArguments(\n    output_dir=\"out\",\n    per_device_train_batch_size=1,\n    gradient_accumulation_steps=8,\n    num_train_epochs=1,\n    logging_steps=10,\n    eval_strategy=\"steps\",\n    eval_steps=50,\n    save_steps=50,\n    lr_scheduler_type=\"cosine\",\n    warmup_ratio=0.03,\n    fp16=False, bf16=True,\n    gradient_checkpointing=True,\n    max_grad_norm=1.0,\n)\n"}]}}
{"instruction": "Propose a minimal JSON plan for code changes. Output PLAN JSON only.", "input": "[INTENT]\n입출력/런타임 안전성 개선\n\n[CONTEXT]\n<<<FILE examples/ai706/dataloader_main.py>>>\nloader = DataLoader(...); for b in loader: pass\n\n<<<END>>>\n<<<FILE examples/ai706/ddp_main.py>>>\ntrainer()\n\n<<<END>>>", "output": {"files": [{"path": "examples/ai706/dataloader_main.py", "reason": "Windows freeze 가능", "strategy": "regex", "tests": ["__main__ 가드"]}, {"path": "examples/ai706/ddp_main.py", "reason": "DDP 가드 없음", "strategy": "regex", "tests": ["__main__"]}], "notes": "spawn 보호"}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nWindows에서 DataLoader 프리즈 방지 + pin_memory/persistent_workers\n\n[CONTEXT]\n<<<FILE examples/ai160/dataloader_main.py>>>\nimport torch, torch.utils.data as tud\n\nclass Dummy(torch.utils.data.Dataset):\n    def __len__(self): return 8\n    def __getitem__(self, i): return i\n\n# BUG: 모듈 임포트만으로 워커가 스폰되어 Windows에서 프리즈\nloader = tud.DataLoader(Dummy(), batch_size=2, num_workers=4)\nfor b in loader:\n    pass\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai160/dataloader_main.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["if __name__ == \"__main__\":"]}, "code": "import torch, torch.utils.data as tud\n\nclass Dummy(torch.utils.data.Dataset):\n    def __len__(self): return 8\n    def __getitem__(self, i): return i\n\n\ndef main():\n    loader = tud.DataLoader(Dummy(), batch_size=2, num_workers=4, pin_memory=True, persistent_workers=True)\n    for _ in loader:\n        pass\n\n\nif __name__ == \"__main__\":\n    main()\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nflash 미지원 시 sdpa 안전 경로\n\n[CONTEXT]\n<<<FILE examples/ai82/model_load.py>>>\nfrom transformers import AutoModelForCausalLM\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"Qwen/Qwen3-8B\",\n)\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai82/model_load.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["attn_implementation"]}, "code": "from transformers import AutoModelForCausalLM\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"Qwen/Qwen3-8B\",\n    attn_implementation=\"sdpa\",\n)\n"}]}}
{"instruction": "Propose a minimal JSON plan for code changes. Output PLAN JSON only.", "input": "[INTENT]\n학습 안정화(AMP+훈련 설정)\n\n[CONTEXT]\n<<<FILE examples/ai711/train_utils.py>>>\ndef get_dtype():\n    return torch.float16\n\n<<<END>>>\n<<<FILE examples/ai711/training_args.py>>>\nargs = TrainingArguments(output_dir='out', per_device_train_batch_size=1, num_train_epochs=1)\n\n<<<END>>>", "output": {"files": [{"path": "examples/ai711/train_utils.py", "reason": "dtype가 고정 float16", "strategy": "regex", "tests": ["bf16 또는 fp16 반환"]}, {"path": "examples/ai711/training_args.py", "reason": "로깅/평가/GA 설정 누락", "strategy": "regex", "tests": ["eval_strategy=='steps'"]}], "notes": "bf16 지원 시 우선"}}
{"instruction": "Propose a minimal JSON plan for code changes. Output PLAN JSON only.", "input": "[INTENT]\n입출력/런타임 안전성 개선\n\n[CONTEXT]\n<<<FILE examples/ai665/dataloader_main.py>>>\nloader = DataLoader(...); for b in loader: pass\n\n<<<END>>>\n<<<FILE examples/ai665/ddp_main.py>>>\ntrainer()\n\n<<<END>>>", "output": {"files": [{"path": "examples/ai665/dataloader_main.py", "reason": "Windows freeze 가능", "strategy": "regex", "tests": ["__main__ 가드"]}, {"path": "examples/ai665/ddp_main.py", "reason": "DDP 가드 없음", "strategy": "regex", "tests": ["__main__"]}], "notes": "spawn 보호"}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n체크포인트 재개 지원\n\n[CONTEXT]\n<<<FILE examples/ai410/trainer_resume.py>>>\nfrom transformers import Trainer\ntrainer = Trainer(... )\n# BUG: 재시작 미지원\ntrainer.train()\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai410/trainer_resume.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["resume_from_checkpoint"]}, "code": "from transformers import Trainer\ntrainer = Trainer(... )\ntrainer.train(resume_from_checkpoint=True)\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n조기 종료 콜백 추가\n\n[CONTEXT]\n<<<FILE examples/ai571/callbacks.py>>>\nfrom transformers import TrainerCallback\n\nclass MyCB(TrainerCallback):\n    pass\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai571/callbacks.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["EarlyStoppingCallback"]}, "code": "from transformers import TrainerCallback, EarlyStoppingCallback\n\nclass MyCB(TrainerCallback):\n    pass\n\nCALLBACKS = [EarlyStoppingCallback(early_stopping_patience=2)]\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n학습 후 어댑터 저장 추가\n\n[CONTEXT]\n<<<FILE examples/ai516/save_after_train.py>>>\nfrom peft import PeftModel\n# ... 학습 후\n# BUG: 저장 누락\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai516/save_after_train.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["save_pretrained"]}, "code": "from peft import PeftModel\n# ... 학습 후\nmodel.save_pretrained(\"training/qlora-out/export\")\n"}]}}
{"instruction": "Propose a minimal JSON plan for code changes. Output PLAN JSON only.", "input": "[INTENT]\n학습 안정화(AMP+훈련 설정)\n\n[CONTEXT]\n<<<FILE examples/ai647/train_utils.py>>>\ndef get_dtype():\n    return torch.float16\n\n<<<END>>>\n<<<FILE examples/ai647/training_args.py>>>\nargs = TrainingArguments(output_dir='out', per_device_train_batch_size=1, num_train_epochs=1)\n\n<<<END>>>", "output": {"files": [{"path": "examples/ai647/train_utils.py", "reason": "dtype가 고정 float16", "strategy": "regex", "tests": ["bf16 또는 fp16 반환"]}, {"path": "examples/ai647/training_args.py", "reason": "로깅/평가/GA 설정 누락", "strategy": "regex", "tests": ["eval_strategy=='steps'"]}], "notes": "bf16 지원 시 우선"}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nAMP 적용 + bf16 우선\n\n[CONTEXT]\n<<<FILE examples/ai63/train_step.py>>>\nimport torch\n\n# BUG: autocast 미사용\ndef train_step(model, batch):\n    x, y = batch\n    out = model(x)\n    loss = out.loss if hasattr(out, 'loss') else out.mean()\n    loss.backward()\n    return loss.item()\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai63/train_step.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["autocast"]}, "code": "import torch\n\n@torch.no_grad()\ndef _bf16_supported():\n    return torch.cuda.is_available() and torch.cuda.is_bf16_supported()\n\n\ndef train_step(model, batch):\n    x, y = batch\n    amp_dtype = torch.bfloat16 if _bf16_supported() else torch.float16\n    scaler = torch.cuda.amp.GradScaler(enabled=True)\n    with torch.cuda.amp.autocast(enabled=True, dtype=amp_dtype):\n        out = model(x)\n        loss = out.loss if hasattr(out, 'loss') else out.mean()\n    scaler.scale(loss).backward()\n    scaler.step(torch.optim.Adam(model.parameters(), lr=1e-4))\n    scaler.update()\n    model.zero_grad(set_to_none=True)\n    return float(loss.detach().item())\n"}]}}
{"instruction": "Propose a minimal JSON plan for code changes. Output PLAN JSON only.", "input": "[INTENT]\n입출력/런타임 안전성 개선\n\n[CONTEXT]\n<<<FILE examples/ai769/dataloader_main.py>>>\nloader = DataLoader(...); for b in loader: pass\n\n<<<END>>>\n<<<FILE examples/ai769/ddp_main.py>>>\ntrainer()\n\n<<<END>>>", "output": {"files": [{"path": "examples/ai769/dataloader_main.py", "reason": "Windows freeze 가능", "strategy": "regex", "tests": ["__main__ 가드"]}, {"path": "examples/ai769/ddp_main.py", "reason": "DDP 가드 없음", "strategy": "regex", "tests": ["__main__"]}], "notes": "spawn 보호"}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n8bit 옵티마이저 + 코사인 스케줄러\n\n[CONTEXT]\n<<<FILE examples/ai358/optim.py>>>\nimport torch\nopt = torch.optim.Adam([], lr=3e-4)\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai358/optim.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["PagedAdamW8bit"]}, "code": "from transformers import get_scheduler\nfrom bitsandbytes.optim import PagedAdamW8bit\nopt = PagedAdamW8bit([], lr=2e-4)\nsched = get_scheduler(\"cosine\", optimizer=opt, num_warmup_steps=100, num_training_steps=1000)\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nWindows에서 DataLoader 프리즈 방지 + pin_memory/persistent_workers\n\n[CONTEXT]\n<<<FILE examples/ai451/dataloader_main.py>>>\nimport torch, torch.utils.data as tud\n\nclass Dummy(torch.utils.data.Dataset):\n    def __len__(self): return 8\n    def __getitem__(self, i): return i\n\n# BUG: 모듈 임포트만으로 워커가 스폰되어 Windows에서 프리즈\nloader = tud.DataLoader(Dummy(), batch_size=2, num_workers=4)\nfor b in loader:\n    pass\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai451/dataloader_main.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["if __name__ == \"__main__\":"]}, "code": "import torch, torch.utils.data as tud\n\nclass Dummy(torch.utils.data.Dataset):\n    def __len__(self): return 8\n    def __getitem__(self, i): return i\n\n\ndef main():\n    loader = tud.DataLoader(Dummy(), batch_size=2, num_workers=4, pin_memory=True, persistent_workers=True)\n    for _ in loader:\n        pass\n\n\nif __name__ == \"__main__\":\n    main()\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nGPU bf16 지원 시 dtype 선택 개선\n\n[CONTEXT]\n<<<FILE examples/ai225/train_utils.py>>>\nimport torch\n\n# BUG: dtype가 고정되어 혼합정밀에 불리\ndef get_dtype():\n    return torch.float16\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai225/train_utils.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["torch.bfloat16"]}, "code": "import torch\n\n\ndef get_dtype():\n    \"\"\"GPU가 bf16 지원 시 bfloat16, 아니면 float16 반환\"\"\"\n    if torch.cuda.is_available() and torch.cuda.is_bf16_supported():\n        return torch.bfloat16\n    return torch.float16\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nAMP 적용 + bf16 우선\n\n[CONTEXT]\n<<<FILE examples/ai148/train_step.py>>>\nimport torch\n\n# BUG: autocast 미사용\ndef train_step(model, batch):\n    x, y = batch\n    out = model(x)\n    loss = out.loss if hasattr(out, 'loss') else out.mean()\n    loss.backward()\n    return loss.item()\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai148/train_step.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["autocast"]}, "code": "import torch\n\n@torch.no_grad()\ndef _bf16_supported():\n    return torch.cuda.is_available() and torch.cuda.is_bf16_supported()\n\n\ndef train_step(model, batch):\n    x, y = batch\n    amp_dtype = torch.bfloat16 if _bf16_supported() else torch.float16\n    scaler = torch.cuda.amp.GradScaler(enabled=True)\n    with torch.cuda.amp.autocast(enabled=True, dtype=amp_dtype):\n        out = model(x)\n        loss = out.loss if hasattr(out, 'loss') else out.mean()\n    scaler.scale(loss).backward()\n    scaler.step(torch.optim.Adam(model.parameters(), lr=1e-4))\n    scaler.update()\n    model.zero_grad(set_to_none=True)\n    return float(loss.detach().item())\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nAMP 적용 + bf16 우선\n\n[CONTEXT]\n<<<FILE examples/ai475/train_step.py>>>\nimport torch\n\n# BUG: autocast 미사용\ndef train_step(model, batch):\n    x, y = batch\n    out = model(x)\n    loss = out.loss if hasattr(out, 'loss') else out.mean()\n    loss.backward()\n    return loss.item()\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai475/train_step.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["autocast"]}, "code": "import torch\n\n@torch.no_grad()\ndef _bf16_supported():\n    return torch.cuda.is_available() and torch.cuda.is_bf16_supported()\n\n\ndef train_step(model, batch):\n    x, y = batch\n    amp_dtype = torch.bfloat16 if _bf16_supported() else torch.float16\n    scaler = torch.cuda.amp.GradScaler(enabled=True)\n    with torch.cuda.amp.autocast(enabled=True, dtype=amp_dtype):\n        out = model(x)\n        loss = out.loss if hasattr(out, 'loss') else out.mean()\n    scaler.scale(loss).backward()\n    scaler.step(torch.optim.Adam(model.parameters(), lr=1e-4))\n    scaler.update()\n    model.zero_grad(set_to_none=True)\n    return float(loss.detach().item())\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nflash 미지원 시 sdpa 안전 경로\n\n[CONTEXT]\n<<<FILE examples/ai68/model_load.py>>>\nfrom transformers import AutoModelForCausalLM\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"Qwen/Qwen3-8B\",\n)\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai68/model_load.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["attn_implementation"]}, "code": "from transformers import AutoModelForCausalLM\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"Qwen/Qwen3-8B\",\n    attn_implementation=\"sdpa\",\n)\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nWindows에서 DataLoader 프리즈 방지 + pin_memory/persistent_workers\n\n[CONTEXT]\n<<<FILE examples/ai282/dataloader_main.py>>>\nimport torch, torch.utils.data as tud\n\nclass Dummy(torch.utils.data.Dataset):\n    def __len__(self): return 8\n    def __getitem__(self, i): return i\n\n# BUG: 모듈 임포트만으로 워커가 스폰되어 Windows에서 프리즈\nloader = tud.DataLoader(Dummy(), batch_size=2, num_workers=4)\nfor b in loader:\n    pass\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai282/dataloader_main.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["if __name__ == \"__main__\":"]}, "code": "import torch, torch.utils.data as tud\n\nclass Dummy(torch.utils.data.Dataset):\n    def __len__(self): return 8\n    def __getitem__(self, i): return i\n\n\ndef main():\n    loader = tud.DataLoader(Dummy(), batch_size=2, num_workers=4, pin_memory=True, persistent_workers=True)\n    for _ in loader:\n        pass\n\n\nif __name__ == \"__main__\":\n    main()\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n필수 import 추가\n\n[CONTEXT]\n<<<FILE examples/ai137/imports.py>>>\n# BUG: numpy 미사용이지만 추후 사용 예정\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai137/imports.py", "loc": {"type": "anchor", "before": "\\A"}, "action": "insert_after", "once": true, "pre": {"must_not_contain": ["import numpy as np"]}, "code": "import numpy as np\n"}]}}
{"instruction": "Propose a minimal JSON plan for code changes. Output PLAN JSON only.", "input": "[INTENT]\n학습 안정화(AMP+훈련 설정)\n\n[CONTEXT]\n<<<FILE examples/ai768/train_utils.py>>>\ndef get_dtype():\n    return torch.float16\n\n<<<END>>>\n<<<FILE examples/ai768/training_args.py>>>\nargs = TrainingArguments(output_dir='out', per_device_train_batch_size=1, num_train_epochs=1)\n\n<<<END>>>", "output": {"files": [{"path": "examples/ai768/train_utils.py", "reason": "dtype가 고정 float16", "strategy": "regex", "tests": ["bf16 또는 fp16 반환"]}, {"path": "examples/ai768/training_args.py", "reason": "로깅/평가/GA 설정 누락", "strategy": "regex", "tests": ["eval_strategy=='steps'"]}], "notes": "bf16 지원 시 우선"}}
{"instruction": "Propose a minimal JSON plan for code changes. Output PLAN JSON only.", "input": "[INTENT]\n입출력/런타임 안전성 개선\n\n[CONTEXT]\n<<<FILE examples/ai759/dataloader_main.py>>>\nloader = DataLoader(...); for b in loader: pass\n\n<<<END>>>\n<<<FILE examples/ai759/ddp_main.py>>>\ntrainer()\n\n<<<END>>>", "output": {"files": [{"path": "examples/ai759/dataloader_main.py", "reason": "Windows freeze 가능", "strategy": "regex", "tests": ["__main__ 가드"]}, {"path": "examples/ai759/ddp_main.py", "reason": "DDP 가드 없음", "strategy": "regex", "tests": ["__main__"]}], "notes": "spawn 보호"}}
{"instruction": "Propose a minimal JSON plan for code changes. Output PLAN JSON only.", "input": "[INTENT]\n입출력/런타임 안전성 개선\n\n[CONTEXT]\n<<<FILE examples/ai731/dataloader_main.py>>>\nloader = DataLoader(...); for b in loader: pass\n\n<<<END>>>\n<<<FILE examples/ai731/ddp_main.py>>>\ntrainer()\n\n<<<END>>>", "output": {"files": [{"path": "examples/ai731/dataloader_main.py", "reason": "Windows freeze 가능", "strategy": "regex", "tests": ["__main__ 가드"]}, {"path": "examples/ai731/ddp_main.py", "reason": "DDP 가드 없음", "strategy": "regex", "tests": ["__main__"]}], "notes": "spawn 보호"}}
{"instruction": "Propose a minimal JSON plan for code changes. Output PLAN JSON only.", "input": "[INTENT]\n입출력/런타임 안전성 개선\n\n[CONTEXT]\n<<<FILE examples/ai688/dataloader_main.py>>>\nloader = DataLoader(...); for b in loader: pass\n\n<<<END>>>\n<<<FILE examples/ai688/ddp_main.py>>>\ntrainer()\n\n<<<END>>>", "output": {"files": [{"path": "examples/ai688/dataloader_main.py", "reason": "Windows freeze 가능", "strategy": "regex", "tests": ["__main__ 가드"]}, {"path": "examples/ai688/ddp_main.py", "reason": "DDP 가드 없음", "strategy": "regex", "tests": ["__main__"]}], "notes": "spawn 보호"}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n필수 import 추가\n\n[CONTEXT]\n<<<FILE examples/ai321/imports.py>>>\n# BUG: numpy 미사용이지만 추후 사용 예정\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai321/imports.py", "loc": {"type": "anchor", "before": "\\A"}, "action": "insert_after", "once": true, "pre": {"must_not_contain": ["import numpy as np"]}, "code": "import numpy as np\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nAMP 적용 + bf16 우선\n\n[CONTEXT]\n<<<FILE examples/ai256/train_step.py>>>\nimport torch\n\n# BUG: autocast 미사용\ndef train_step(model, batch):\n    x, y = batch\n    out = model(x)\n    loss = out.loss if hasattr(out, 'loss') else out.mean()\n    loss.backward()\n    return loss.item()\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai256/train_step.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["autocast"]}, "code": "import torch\n\n@torch.no_grad()\ndef _bf16_supported():\n    return torch.cuda.is_available() and torch.cuda.is_bf16_supported()\n\n\ndef train_step(model, batch):\n    x, y = batch\n    amp_dtype = torch.bfloat16 if _bf16_supported() else torch.float16\n    scaler = torch.cuda.amp.GradScaler(enabled=True)\n    with torch.cuda.amp.autocast(enabled=True, dtype=amp_dtype):\n        out = model(x)\n        loss = out.loss if hasattr(out, 'loss') else out.mean()\n    scaler.scale(loss).backward()\n    scaler.step(torch.optim.Adam(model.parameters(), lr=1e-4))\n    scaler.update()\n    model.zero_grad(set_to_none=True)\n    return float(loss.detach().item())\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nLoRA target_modules 확장\n\n[CONTEXT]\n<<<FILE examples/ai388/lora_cfg.py>>>\nfrom peft import LoraConfig\n\nLC = LoraConfig(r=8, target_modules=[\n    \"q_proj\", \"v_proj\"  # BUG: 부족\n])\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai388/lora_cfg.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["k_proj", "o_proj"]}, "code": "from peft import LoraConfig\n\nLC = LoraConfig(r=8, target_modules=[\n    \"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"\n])\n"}]}}
{"instruction": "Propose a minimal JSON plan for code changes. Output PLAN JSON only.", "input": "[INTENT]\n입출력/런타임 안전성 개선\n\n[CONTEXT]\n<<<FILE examples/ai784/dataloader_main.py>>>\nloader = DataLoader(...); for b in loader: pass\n\n<<<END>>>\n<<<FILE examples/ai784/ddp_main.py>>>\ntrainer()\n\n<<<END>>>", "output": {"files": [{"path": "examples/ai784/dataloader_main.py", "reason": "Windows freeze 가능", "strategy": "regex", "tests": ["__main__ 가드"]}, {"path": "examples/ai784/ddp_main.py", "reason": "DDP 가드 없음", "strategy": "regex", "tests": ["__main__"]}], "notes": "spawn 보호"}}
{"instruction": "Propose a minimal JSON plan for code changes. Output PLAN JSON only.", "input": "[INTENT]\n입출력/런타임 안전성 개선\n\n[CONTEXT]\n<<<FILE examples/ai739/dataloader_main.py>>>\nloader = DataLoader(...); for b in loader: pass\n\n<<<END>>>\n<<<FILE examples/ai739/ddp_main.py>>>\ntrainer()\n\n<<<END>>>", "output": {"files": [{"path": "examples/ai739/dataloader_main.py", "reason": "Windows freeze 가능", "strategy": "regex", "tests": ["__main__ 가드"]}, {"path": "examples/ai739/ddp_main.py", "reason": "DDP 가드 없음", "strategy": "regex", "tests": ["__main__"]}], "notes": "spawn 보호"}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\npad 토큰 ID 정확히 설정\n\n[CONTEXT]\n<<<FILE examples/ai266/label_cfg.py>>>\nfrom transformers import AutoTokenizer\nfrom dataclasses import dataclass\n\n@dataclass\nclass Cfg:\n    pad_id: int = -100  # BUG\n\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai266/label_cfg.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["CFG = Cfg"]}, "code": "from transformers import AutoTokenizer\nfrom dataclasses import dataclass\n\n@dataclass\nclass Cfg:\n    pad_id: int | None = None\n\n# tokenizer 로드 후 설정\ntok = AutoTokenizer.from_pretrained(\"Qwen/Qwen3-8B\", use_fast=True)\nCFG = Cfg(pad_id=tok.pad_token_id)\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nLoRA target_modules 확장\n\n[CONTEXT]\n<<<FILE examples/ai97/lora_cfg.py>>>\nfrom peft import LoraConfig\n\nLC = LoraConfig(r=8, target_modules=[\n    \"q_proj\", \"v_proj\"  # BUG: 부족\n])\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai97/lora_cfg.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["k_proj", "o_proj"]}, "code": "from peft import LoraConfig\n\nLC = LoraConfig(r=8, target_modules=[\n    \"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"\n])\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n필수 import 추가\n\n[CONTEXT]\n<<<FILE examples/ai28/imports.py>>>\n# BUG: numpy 미사용이지만 추후 사용 예정\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai28/imports.py", "loc": {"type": "anchor", "before": "\\A"}, "action": "insert_after", "once": true, "pre": {"must_not_contain": ["import numpy as np"]}, "code": "import numpy as np\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n체크포인트 재개 지원\n\n[CONTEXT]\n<<<FILE examples/ai212/trainer_resume.py>>>\nfrom transformers import Trainer\ntrainer = Trainer(... )\n# BUG: 재시작 미지원\ntrainer.train()\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai212/trainer_resume.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["resume_from_checkpoint"]}, "code": "from transformers import Trainer\ntrainer = Trainer(... )\ntrainer.train(resume_from_checkpoint=True)\n"}]}}
{"instruction": "Propose a minimal JSON plan for code changes. Output PLAN JSON only.", "input": "[INTENT]\n입출력/런타임 안전성 개선\n\n[CONTEXT]\n<<<FILE examples/ai669/dataloader_main.py>>>\nloader = DataLoader(...); for b in loader: pass\n\n<<<END>>>\n<<<FILE examples/ai669/ddp_main.py>>>\ntrainer()\n\n<<<END>>>", "output": {"files": [{"path": "examples/ai669/dataloader_main.py", "reason": "Windows freeze 가능", "strategy": "regex", "tests": ["__main__ 가드"]}, {"path": "examples/ai669/ddp_main.py", "reason": "DDP 가드 없음", "strategy": "regex", "tests": ["__main__"]}], "notes": "spawn 보호"}}
{"instruction": "Propose a minimal JSON plan for code changes. Output PLAN JSON only.", "input": "[INTENT]\n학습 안정화(AMP+훈련 설정)\n\n[CONTEXT]\n<<<FILE examples/ai672/train_utils.py>>>\ndef get_dtype():\n    return torch.float16\n\n<<<END>>>\n<<<FILE examples/ai672/training_args.py>>>\nargs = TrainingArguments(output_dir='out', per_device_train_batch_size=1, num_train_epochs=1)\n\n<<<END>>>", "output": {"files": [{"path": "examples/ai672/train_utils.py", "reason": "dtype가 고정 float16", "strategy": "regex", "tests": ["bf16 또는 fp16 반환"]}, {"path": "examples/ai672/training_args.py", "reason": "로깅/평가/GA 설정 누락", "strategy": "regex", "tests": ["eval_strategy=='steps'"]}], "notes": "bf16 지원 시 우선"}}
{"instruction": "Propose a minimal JSON plan for code changes. Output PLAN JSON only.", "input": "[INTENT]\n학습 안정화(AMP+훈련 설정)\n\n[CONTEXT]\n<<<FILE examples/ai741/train_utils.py>>>\ndef get_dtype():\n    return torch.float16\n\n<<<END>>>\n<<<FILE examples/ai741/training_args.py>>>\nargs = TrainingArguments(output_dir='out', per_device_train_batch_size=1, num_train_epochs=1)\n\n<<<END>>>", "output": {"files": [{"path": "examples/ai741/train_utils.py", "reason": "dtype가 고정 float16", "strategy": "regex", "tests": ["bf16 또는 fp16 반환"]}, {"path": "examples/ai741/training_args.py", "reason": "로깅/평가/GA 설정 누락", "strategy": "regex", "tests": ["eval_strategy=='steps'"]}], "notes": "bf16 지원 시 우선"}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n훈련 안정화: GA steps/로깅/평가/체크포인팅/스케줄러\n\n[CONTEXT]\n<<<FILE examples/ai337/training_args.py>>>\nfrom transformers import TrainingArguments\n\nargs = TrainingArguments(\n    output_dir=\"out\",\n    per_device_train_batch_size=1,\n    num_train_epochs=1,\n)\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai337/training_args.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["gradient_accumulation_steps"]}, "code": "from transformers import TrainingArguments\n\nargs = TrainingArguments(\n    output_dir=\"out\",\n    per_device_train_batch_size=1,\n    gradient_accumulation_steps=8,\n    num_train_epochs=1,\n    logging_steps=10,\n    eval_strategy=\"steps\",\n    eval_steps=50,\n    save_steps=50,\n    lr_scheduler_type=\"cosine\",\n    warmup_ratio=0.03,\n    fp16=False, bf16=True,\n    gradient_checkpointing=True,\n    max_grad_norm=1.0,\n)\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n필수 import 추가\n\n[CONTEXT]\n<<<FILE examples/ai1/imports.py>>>\n# BUG: numpy 미사용이지만 추후 사용 예정\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai1/imports.py", "loc": {"type": "anchor", "before": "\\A"}, "action": "insert_after", "once": true, "pre": {"must_not_contain": ["import numpy as np"]}, "code": "import numpy as np\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n훈련 안정화: GA steps/로깅/평가/체크포인팅/스케줄러\n\n[CONTEXT]\n<<<FILE examples/ai554/training_args.py>>>\nfrom transformers import TrainingArguments\n\nargs = TrainingArguments(\n    output_dir=\"out\",\n    per_device_train_batch_size=1,\n    num_train_epochs=1,\n)\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai554/training_args.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["gradient_accumulation_steps"]}, "code": "from transformers import TrainingArguments\n\nargs = TrainingArguments(\n    output_dir=\"out\",\n    per_device_train_batch_size=1,\n    gradient_accumulation_steps=8,\n    num_train_epochs=1,\n    logging_steps=10,\n    eval_strategy=\"steps\",\n    eval_steps=50,\n    save_steps=50,\n    lr_scheduler_type=\"cosine\",\n    warmup_ratio=0.03,\n    fp16=False, bf16=True,\n    gradient_checkpointing=True,\n    max_grad_norm=1.0,\n)\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nWindows에서 DataLoader 프리즈 방지 + pin_memory/persistent_workers\n\n[CONTEXT]\n<<<FILE examples/ai423/dataloader_main.py>>>\nimport torch, torch.utils.data as tud\n\nclass Dummy(torch.utils.data.Dataset):\n    def __len__(self): return 8\n    def __getitem__(self, i): return i\n\n# BUG: 모듈 임포트만으로 워커가 스폰되어 Windows에서 프리즈\nloader = tud.DataLoader(Dummy(), batch_size=2, num_workers=4)\nfor b in loader:\n    pass\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai423/dataloader_main.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["if __name__ == \"__main__\":"]}, "code": "import torch, torch.utils.data as tud\n\nclass Dummy(torch.utils.data.Dataset):\n    def __len__(self): return 8\n    def __getitem__(self, i): return i\n\n\ndef main():\n    loader = tud.DataLoader(Dummy(), batch_size=2, num_workers=4, pin_memory=True, persistent_workers=True)\n    for _ in loader:\n        pass\n\n\nif __name__ == \"__main__\":\n    main()\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n필수 import 추가\n\n[CONTEXT]\n<<<FILE examples/ai264/imports.py>>>\n# BUG: numpy 미사용이지만 추후 사용 예정\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai264/imports.py", "loc": {"type": "anchor", "before": "\\A"}, "action": "insert_after", "once": true, "pre": {"must_not_contain": ["import numpy as np"]}, "code": "import numpy as np\n"}]}}
{"instruction": "Propose a minimal JSON plan for code changes. Output PLAN JSON only.", "input": "[INTENT]\n입출력/런타임 안전성 개선\n\n[CONTEXT]\n<<<FILE examples/ai753/dataloader_main.py>>>\nloader = DataLoader(...); for b in loader: pass\n\n<<<END>>>\n<<<FILE examples/ai753/ddp_main.py>>>\ntrainer()\n\n<<<END>>>", "output": {"files": [{"path": "examples/ai753/dataloader_main.py", "reason": "Windows freeze 가능", "strategy": "regex", "tests": ["__main__ 가드"]}, {"path": "examples/ai753/ddp_main.py", "reason": "DDP 가드 없음", "strategy": "regex", "tests": ["__main__"]}], "notes": "spawn 보호"}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nLoRA target_modules 확장\n\n[CONTEXT]\n<<<FILE examples/ai414/lora_cfg.py>>>\nfrom peft import LoraConfig\n\nLC = LoraConfig(r=8, target_modules=[\n    \"q_proj\", \"v_proj\"  # BUG: 부족\n])\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai414/lora_cfg.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["k_proj", "o_proj"]}, "code": "from peft import LoraConfig\n\nLC = LoraConfig(r=8, target_modules=[\n    \"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"\n])\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\npad 토큰 ID 정확히 설정\n\n[CONTEXT]\n<<<FILE examples/ai119/label_cfg.py>>>\nfrom transformers import AutoTokenizer\nfrom dataclasses import dataclass\n\n@dataclass\nclass Cfg:\n    pad_id: int = -100  # BUG\n\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai119/label_cfg.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["CFG = Cfg"]}, "code": "from transformers import AutoTokenizer\nfrom dataclasses import dataclass\n\n@dataclass\nclass Cfg:\n    pad_id: int | None = None\n\n# tokenizer 로드 후 설정\ntok = AutoTokenizer.from_pretrained(\"Qwen/Qwen3-8B\", use_fast=True)\nCFG = Cfg(pad_id=tok.pad_token_id)\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nLoRA target_modules 확장\n\n[CONTEXT]\n<<<FILE examples/ai332/lora_cfg.py>>>\nfrom peft import LoraConfig\n\nLC = LoraConfig(r=8, target_modules=[\n    \"q_proj\", \"v_proj\"  # BUG: 부족\n])\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai332/lora_cfg.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["k_proj", "o_proj"]}, "code": "from peft import LoraConfig\n\nLC = LoraConfig(r=8, target_modules=[\n    \"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"\n])\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n재현성: 시드 함수 추가\n\n[CONTEXT]\n<<<FILE examples/ai627/seed_utils.py>>>\n# BUG: 시드 미설정\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai627/seed_utils.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["set_seed("]}, "code": "import os, random, numpy as np, torch\n\ndef set_seed(seed: int = 42):\n    random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n필수 import 추가\n\n[CONTEXT]\n<<<FILE examples/ai323/imports.py>>>\n# BUG: numpy 미사용이지만 추후 사용 예정\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai323/imports.py", "loc": {"type": "anchor", "before": "\\A"}, "action": "insert_after", "once": true, "pre": {"must_not_contain": ["import numpy as np"]}, "code": "import numpy as np\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n필수 import 추가\n\n[CONTEXT]\n<<<FILE examples/ai91/imports.py>>>\n# BUG: numpy 미사용이지만 추후 사용 예정\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai91/imports.py", "loc": {"type": "anchor", "before": "\\A"}, "action": "insert_after", "once": true, "pre": {"must_not_contain": ["import numpy as np"]}, "code": "import numpy as np\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n조기 종료 콜백 추가\n\n[CONTEXT]\n<<<FILE examples/ai466/callbacks.py>>>\nfrom transformers import TrainerCallback\n\nclass MyCB(TrainerCallback):\n    pass\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai466/callbacks.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["EarlyStoppingCallback"]}, "code": "from transformers import TrainerCallback, EarlyStoppingCallback\n\nclass MyCB(TrainerCallback):\n    pass\n\nCALLBACKS = [EarlyStoppingCallback(early_stopping_patience=2)]\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nGPU bf16 지원 시 dtype 선택 개선\n\n[CONTEXT]\n<<<FILE examples/ai38/train_utils.py>>>\nimport torch\n\n# BUG: dtype가 고정되어 혼합정밀에 불리\ndef get_dtype():\n    return torch.float16\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai38/train_utils.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["torch.bfloat16"]}, "code": "import torch\n\n\ndef get_dtype():\n    \"\"\"GPU가 bf16 지원 시 bfloat16, 아니면 float16 반환\"\"\"\n    if torch.cuda.is_available() and torch.cuda.is_bf16_supported():\n        return torch.bfloat16\n    return torch.float16\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\npad 토큰 ID 정확히 설정\n\n[CONTEXT]\n<<<FILE examples/ai623/label_cfg.py>>>\nfrom transformers import AutoTokenizer\nfrom dataclasses import dataclass\n\n@dataclass\nclass Cfg:\n    pad_id: int = -100  # BUG\n\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai623/label_cfg.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["CFG = Cfg"]}, "code": "from transformers import AutoTokenizer\nfrom dataclasses import dataclass\n\n@dataclass\nclass Cfg:\n    pad_id: int | None = None\n\n# tokenizer 로드 후 설정\ntok = AutoTokenizer.from_pretrained(\"Qwen/Qwen3-8B\", use_fast=True)\nCFG = Cfg(pad_id=tok.pad_token_id)\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nGPU bf16 지원 시 dtype 선택 개선\n\n[CONTEXT]\n<<<FILE examples/ai562/train_utils.py>>>\nimport torch\n\n# BUG: dtype가 고정되어 혼합정밀에 불리\ndef get_dtype():\n    return torch.float16\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai562/train_utils.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["torch.bfloat16"]}, "code": "import torch\n\n\ndef get_dtype():\n    \"\"\"GPU가 bf16 지원 시 bfloat16, 아니면 float16 반환\"\"\"\n    if torch.cuda.is_available() and torch.cuda.is_bf16_supported():\n        return torch.bfloat16\n    return torch.float16\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nGPU bf16 지원 시 dtype 선택 개선\n\n[CONTEXT]\n<<<FILE examples/ai508/train_utils.py>>>\nimport torch\n\n# BUG: dtype가 고정되어 혼합정밀에 불리\ndef get_dtype():\n    return torch.float16\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai508/train_utils.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["torch.bfloat16"]}, "code": "import torch\n\n\ndef get_dtype():\n    \"\"\"GPU가 bf16 지원 시 bfloat16, 아니면 float16 반환\"\"\"\n    if torch.cuda.is_available() and torch.cuda.is_bf16_supported():\n        return torch.bfloat16\n    return torch.float16\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n체크포인트 재개 지원\n\n[CONTEXT]\n<<<FILE examples/ai422/trainer_resume.py>>>\nfrom transformers import Trainer\ntrainer = Trainer(... )\n# BUG: 재시작 미지원\ntrainer.train()\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai422/trainer_resume.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["resume_from_checkpoint"]}, "code": "from transformers import Trainer\ntrainer = Trainer(... )\ntrainer.train(resume_from_checkpoint=True)\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n재현성: 시드 함수 추가\n\n[CONTEXT]\n<<<FILE examples/ai338/seed_utils.py>>>\n# BUG: 시드 미설정\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai338/seed_utils.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["set_seed("]}, "code": "import os, random, numpy as np, torch\n\ndef set_seed(seed: int = 42):\n    random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n훈련 안정화: GA steps/로깅/평가/체크포인팅/스케줄러\n\n[CONTEXT]\n<<<FILE examples/ai165/training_args.py>>>\nfrom transformers import TrainingArguments\n\nargs = TrainingArguments(\n    output_dir=\"out\",\n    per_device_train_batch_size=1,\n    num_train_epochs=1,\n)\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai165/training_args.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["gradient_accumulation_steps"]}, "code": "from transformers import TrainingArguments\n\nargs = TrainingArguments(\n    output_dir=\"out\",\n    per_device_train_batch_size=1,\n    gradient_accumulation_steps=8,\n    num_train_epochs=1,\n    logging_steps=10,\n    eval_strategy=\"steps\",\n    eval_steps=50,\n    save_steps=50,\n    lr_scheduler_type=\"cosine\",\n    warmup_ratio=0.03,\n    fp16=False, bf16=True,\n    gradient_checkpointing=True,\n    max_grad_norm=1.0,\n)\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n조기 종료 콜백 추가\n\n[CONTEXT]\n<<<FILE examples/ai108/callbacks.py>>>\nfrom transformers import TrainerCallback\n\nclass MyCB(TrainerCallback):\n    pass\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai108/callbacks.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["EarlyStoppingCallback"]}, "code": "from transformers import TrainerCallback, EarlyStoppingCallback\n\nclass MyCB(TrainerCallback):\n    pass\n\nCALLBACKS = [EarlyStoppingCallback(early_stopping_patience=2)]\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nDDP/torchrun 안전 가드\n\n[CONTEXT]\n<<<FILE examples/ai329/ddp_main.py>>>\nimport torch\n\n# BUG: torchrun에서 main 가드 없음\ntrainer()\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai329/ddp_main.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["__main__"]}, "code": "import torch\n\n\ndef main():\n    trainer()\n\n\nif __name__ == \"__main__\":\n    main()\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nLoRA target_modules 확장\n\n[CONTEXT]\n<<<FILE examples/ai342/lora_cfg.py>>>\nfrom peft import LoraConfig\n\nLC = LoraConfig(r=8, target_modules=[\n    \"q_proj\", \"v_proj\"  # BUG: 부족\n])\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai342/lora_cfg.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["k_proj", "o_proj"]}, "code": "from peft import LoraConfig\n\nLC = LoraConfig(r=8, target_modules=[\n    \"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"\n])\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n8bit 옵티마이저 + 코사인 스케줄러\n\n[CONTEXT]\n<<<FILE examples/ai586/optim.py>>>\nimport torch\nopt = torch.optim.Adam([], lr=3e-4)\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai586/optim.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["PagedAdamW8bit"]}, "code": "from transformers import get_scheduler\nfrom bitsandbytes.optim import PagedAdamW8bit\nopt = PagedAdamW8bit([], lr=2e-4)\nsched = get_scheduler(\"cosine\", optimizer=opt, num_warmup_steps=100, num_training_steps=1000)\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nDDP/torchrun 안전 가드\n\n[CONTEXT]\n<<<FILE examples/ai357/ddp_main.py>>>\nimport torch\n\n# BUG: torchrun에서 main 가드 없음\ntrainer()\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai357/ddp_main.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["__main__"]}, "code": "import torch\n\n\ndef main():\n    trainer()\n\n\nif __name__ == \"__main__\":\n    main()\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n8bit 옵티마이저 + 코사인 스케줄러\n\n[CONTEXT]\n<<<FILE examples/ai539/optim.py>>>\nimport torch\nopt = torch.optim.Adam([], lr=3e-4)\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai539/optim.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["PagedAdamW8bit"]}, "code": "from transformers import get_scheduler\nfrom bitsandbytes.optim import PagedAdamW8bit\nopt = PagedAdamW8bit([], lr=2e-4)\nsched = get_scheduler(\"cosine\", optimizer=opt, num_warmup_steps=100, num_training_steps=1000)\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n필수 import 추가\n\n[CONTEXT]\n<<<FILE examples/ai545/imports.py>>>\n# BUG: numpy 미사용이지만 추후 사용 예정\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai545/imports.py", "loc": {"type": "anchor", "before": "\\A"}, "action": "insert_after", "once": true, "pre": {"must_not_contain": ["import numpy as np"]}, "code": "import numpy as np\n"}]}}
{"instruction": "Propose a minimal JSON plan for code changes. Output PLAN JSON only.", "input": "[INTENT]\n입출력/런타임 안전성 개선\n\n[CONTEXT]\n<<<FILE examples/ai789/dataloader_main.py>>>\nloader = DataLoader(...); for b in loader: pass\n\n<<<END>>>\n<<<FILE examples/ai789/ddp_main.py>>>\ntrainer()\n\n<<<END>>>", "output": {"files": [{"path": "examples/ai789/dataloader_main.py", "reason": "Windows freeze 가능", "strategy": "regex", "tests": ["__main__ 가드"]}, {"path": "examples/ai789/ddp_main.py", "reason": "DDP 가드 없음", "strategy": "regex", "tests": ["__main__"]}], "notes": "spawn 보호"}}
{"instruction": "Propose a minimal JSON plan for code changes. Output PLAN JSON only.", "input": "[INTENT]\n학습 안정화(AMP+훈련 설정)\n\n[CONTEXT]\n<<<FILE examples/ai732/train_utils.py>>>\ndef get_dtype():\n    return torch.float16\n\n<<<END>>>\n<<<FILE examples/ai732/training_args.py>>>\nargs = TrainingArguments(output_dir='out', per_device_train_batch_size=1, num_train_epochs=1)\n\n<<<END>>>", "output": {"files": [{"path": "examples/ai732/train_utils.py", "reason": "dtype가 고정 float16", "strategy": "regex", "tests": ["bf16 또는 fp16 반환"]}, {"path": "examples/ai732/training_args.py", "reason": "로깅/평가/GA 설정 누락", "strategy": "regex", "tests": ["eval_strategy=='steps'"]}], "notes": "bf16 지원 시 우선"}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nAMP 적용 + bf16 우선\n\n[CONTEXT]\n<<<FILE examples/ai186/train_step.py>>>\nimport torch\n\n# BUG: autocast 미사용\ndef train_step(model, batch):\n    x, y = batch\n    out = model(x)\n    loss = out.loss if hasattr(out, 'loss') else out.mean()\n    loss.backward()\n    return loss.item()\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai186/train_step.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["autocast"]}, "code": "import torch\n\n@torch.no_grad()\ndef _bf16_supported():\n    return torch.cuda.is_available() and torch.cuda.is_bf16_supported()\n\n\ndef train_step(model, batch):\n    x, y = batch\n    amp_dtype = torch.bfloat16 if _bf16_supported() else torch.float16\n    scaler = torch.cuda.amp.GradScaler(enabled=True)\n    with torch.cuda.amp.autocast(enabled=True, dtype=amp_dtype):\n        out = model(x)\n        loss = out.loss if hasattr(out, 'loss') else out.mean()\n    scaler.scale(loss).backward()\n    scaler.step(torch.optim.Adam(model.parameters(), lr=1e-4))\n    scaler.update()\n    model.zero_grad(set_to_none=True)\n    return float(loss.detach().item())\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nGPU bf16 지원 시 dtype 선택 개선\n\n[CONTEXT]\n<<<FILE examples/ai285/train_utils.py>>>\nimport torch\n\n# BUG: dtype가 고정되어 혼합정밀에 불리\ndef get_dtype():\n    return torch.float16\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai285/train_utils.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["torch.bfloat16"]}, "code": "import torch\n\n\ndef get_dtype():\n    \"\"\"GPU가 bf16 지원 시 bfloat16, 아니면 float16 반환\"\"\"\n    if torch.cuda.is_available() and torch.cuda.is_bf16_supported():\n        return torch.bfloat16\n    return torch.float16\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nLoRA target_modules 확장\n\n[CONTEXT]\n<<<FILE examples/ai453/lora_cfg.py>>>\nfrom peft import LoraConfig\n\nLC = LoraConfig(r=8, target_modules=[\n    \"q_proj\", \"v_proj\"  # BUG: 부족\n])\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai453/lora_cfg.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["k_proj", "o_proj"]}, "code": "from peft import LoraConfig\n\nLC = LoraConfig(r=8, target_modules=[\n    \"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"\n])\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n훈련 안정화: GA steps/로깅/평가/체크포인팅/스케줄러\n\n[CONTEXT]\n<<<FILE examples/ai532/training_args.py>>>\nfrom transformers import TrainingArguments\n\nargs = TrainingArguments(\n    output_dir=\"out\",\n    per_device_train_batch_size=1,\n    num_train_epochs=1,\n)\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai532/training_args.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["gradient_accumulation_steps"]}, "code": "from transformers import TrainingArguments\n\nargs = TrainingArguments(\n    output_dir=\"out\",\n    per_device_train_batch_size=1,\n    gradient_accumulation_steps=8,\n    num_train_epochs=1,\n    logging_steps=10,\n    eval_strategy=\"steps\",\n    eval_steps=50,\n    save_steps=50,\n    lr_scheduler_type=\"cosine\",\n    warmup_ratio=0.03,\n    fp16=False, bf16=True,\n    gradient_checkpointing=True,\n    max_grad_norm=1.0,\n)\n"}]}}
{"instruction": "Propose a minimal JSON plan for code changes. Output PLAN JSON only.", "input": "[INTENT]\n입출력/런타임 안전성 개선\n\n[CONTEXT]\n<<<FILE examples/ai722/dataloader_main.py>>>\nloader = DataLoader(...); for b in loader: pass\n\n<<<END>>>\n<<<FILE examples/ai722/ddp_main.py>>>\ntrainer()\n\n<<<END>>>", "output": {"files": [{"path": "examples/ai722/dataloader_main.py", "reason": "Windows freeze 가능", "strategy": "regex", "tests": ["__main__ 가드"]}, {"path": "examples/ai722/ddp_main.py", "reason": "DDP 가드 없음", "strategy": "regex", "tests": ["__main__"]}], "notes": "spawn 보호"}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n8bit 옵티마이저 + 코사인 스케줄러\n\n[CONTEXT]\n<<<FILE examples/ai180/optim.py>>>\nimport torch\nopt = torch.optim.Adam([], lr=3e-4)\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai180/optim.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["PagedAdamW8bit"]}, "code": "from transformers import get_scheduler\nfrom bitsandbytes.optim import PagedAdamW8bit\nopt = PagedAdamW8bit([], lr=2e-4)\nsched = get_scheduler(\"cosine\", optimizer=opt, num_warmup_steps=100, num_training_steps=1000)\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n조기 종료 콜백 추가\n\n[CONTEXT]\n<<<FILE examples/ai42/callbacks.py>>>\nfrom transformers import TrainerCallback\n\nclass MyCB(TrainerCallback):\n    pass\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai42/callbacks.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["EarlyStoppingCallback"]}, "code": "from transformers import TrainerCallback, EarlyStoppingCallback\n\nclass MyCB(TrainerCallback):\n    pass\n\nCALLBACKS = [EarlyStoppingCallback(early_stopping_patience=2)]\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n체크포인트 재개 지원\n\n[CONTEXT]\n<<<FILE examples/ai276/trainer_resume.py>>>\nfrom transformers import Trainer\ntrainer = Trainer(... )\n# BUG: 재시작 미지원\ntrainer.train()\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai276/trainer_resume.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["resume_from_checkpoint"]}, "code": "from transformers import Trainer\ntrainer = Trainer(... )\ntrainer.train(resume_from_checkpoint=True)\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nAMP 적용 + bf16 우선\n\n[CONTEXT]\n<<<FILE examples/ai22/train_step.py>>>\nimport torch\n\n# BUG: autocast 미사용\ndef train_step(model, batch):\n    x, y = batch\n    out = model(x)\n    loss = out.loss if hasattr(out, 'loss') else out.mean()\n    loss.backward()\n    return loss.item()\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai22/train_step.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["autocast"]}, "code": "import torch\n\n@torch.no_grad()\ndef _bf16_supported():\n    return torch.cuda.is_available() and torch.cuda.is_bf16_supported()\n\n\ndef train_step(model, batch):\n    x, y = batch\n    amp_dtype = torch.bfloat16 if _bf16_supported() else torch.float16\n    scaler = torch.cuda.amp.GradScaler(enabled=True)\n    with torch.cuda.amp.autocast(enabled=True, dtype=amp_dtype):\n        out = model(x)\n        loss = out.loss if hasattr(out, 'loss') else out.mean()\n    scaler.scale(loss).backward()\n    scaler.step(torch.optim.Adam(model.parameters(), lr=1e-4))\n    scaler.update()\n    model.zero_grad(set_to_none=True)\n    return float(loss.detach().item())\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nWindows에서 DataLoader 프리즈 방지 + pin_memory/persistent_workers\n\n[CONTEXT]\n<<<FILE examples/ai235/dataloader_main.py>>>\nimport torch, torch.utils.data as tud\n\nclass Dummy(torch.utils.data.Dataset):\n    def __len__(self): return 8\n    def __getitem__(self, i): return i\n\n# BUG: 모듈 임포트만으로 워커가 스폰되어 Windows에서 프리즈\nloader = tud.DataLoader(Dummy(), batch_size=2, num_workers=4)\nfor b in loader:\n    pass\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai235/dataloader_main.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["if __name__ == \"__main__\":"]}, "code": "import torch, torch.utils.data as tud\n\nclass Dummy(torch.utils.data.Dataset):\n    def __len__(self): return 8\n    def __getitem__(self, i): return i\n\n\ndef main():\n    loader = tud.DataLoader(Dummy(), batch_size=2, num_workers=4, pin_memory=True, persistent_workers=True)\n    for _ in loader:\n        pass\n\n\nif __name__ == \"__main__\":\n    main()\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n훈련 안정화: GA steps/로깅/평가/체크포인팅/스케줄러\n\n[CONTEXT]\n<<<FILE examples/ai94/training_args.py>>>\nfrom transformers import TrainingArguments\n\nargs = TrainingArguments(\n    output_dir=\"out\",\n    per_device_train_batch_size=1,\n    num_train_epochs=1,\n)\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai94/training_args.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["gradient_accumulation_steps"]}, "code": "from transformers import TrainingArguments\n\nargs = TrainingArguments(\n    output_dir=\"out\",\n    per_device_train_batch_size=1,\n    gradient_accumulation_steps=8,\n    num_train_epochs=1,\n    logging_steps=10,\n    eval_strategy=\"steps\",\n    eval_steps=50,\n    save_steps=50,\n    lr_scheduler_type=\"cosine\",\n    warmup_ratio=0.03,\n    fp16=False, bf16=True,\n    gradient_checkpointing=True,\n    max_grad_norm=1.0,\n)\n"}]}}
{"instruction": "Propose a minimal JSON plan for code changes. Output PLAN JSON only.", "input": "[INTENT]\n입출력/런타임 안전성 개선\n\n[CONTEXT]\n<<<FILE examples/ai781/dataloader_main.py>>>\nloader = DataLoader(...); for b in loader: pass\n\n<<<END>>>\n<<<FILE examples/ai781/ddp_main.py>>>\ntrainer()\n\n<<<END>>>", "output": {"files": [{"path": "examples/ai781/dataloader_main.py", "reason": "Windows freeze 가능", "strategy": "regex", "tests": ["__main__ 가드"]}, {"path": "examples/ai781/ddp_main.py", "reason": "DDP 가드 없음", "strategy": "regex", "tests": ["__main__"]}], "notes": "spawn 보호"}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nDDP/torchrun 안전 가드\n\n[CONTEXT]\n<<<FILE examples/ai257/ddp_main.py>>>\nimport torch\n\n# BUG: torchrun에서 main 가드 없음\ntrainer()\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai257/ddp_main.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["__main__"]}, "code": "import torch\n\n\ndef main():\n    trainer()\n\n\nif __name__ == \"__main__\":\n    main()\n"}]}}
{"instruction": "Propose a minimal JSON plan for code changes. Output PLAN JSON only.", "input": "[INTENT]\n학습 안정화(AMP+훈련 설정)\n\n[CONTEXT]\n<<<FILE examples/ai729/train_utils.py>>>\ndef get_dtype():\n    return torch.float16\n\n<<<END>>>\n<<<FILE examples/ai729/training_args.py>>>\nargs = TrainingArguments(output_dir='out', per_device_train_batch_size=1, num_train_epochs=1)\n\n<<<END>>>", "output": {"files": [{"path": "examples/ai729/train_utils.py", "reason": "dtype가 고정 float16", "strategy": "regex", "tests": ["bf16 또는 fp16 반환"]}, {"path": "examples/ai729/training_args.py", "reason": "로깅/평가/GA 설정 누락", "strategy": "regex", "tests": ["eval_strategy=='steps'"]}], "notes": "bf16 지원 시 우선"}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\npad 토큰 ID 정확히 설정\n\n[CONTEXT]\n<<<FILE examples/ai443/label_cfg.py>>>\nfrom transformers import AutoTokenizer\nfrom dataclasses import dataclass\n\n@dataclass\nclass Cfg:\n    pad_id: int = -100  # BUG\n\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai443/label_cfg.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["CFG = Cfg"]}, "code": "from transformers import AutoTokenizer\nfrom dataclasses import dataclass\n\n@dataclass\nclass Cfg:\n    pad_id: int | None = None\n\n# tokenizer 로드 후 설정\ntok = AutoTokenizer.from_pretrained(\"Qwen/Qwen3-8B\", use_fast=True)\nCFG = Cfg(pad_id=tok.pad_token_id)\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n훈련 안정화: GA steps/로깅/평가/체크포인팅/스케줄러\n\n[CONTEXT]\n<<<FILE examples/ai41/training_args.py>>>\nfrom transformers import TrainingArguments\n\nargs = TrainingArguments(\n    output_dir=\"out\",\n    per_device_train_batch_size=1,\n    num_train_epochs=1,\n)\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai41/training_args.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["gradient_accumulation_steps"]}, "code": "from transformers import TrainingArguments\n\nargs = TrainingArguments(\n    output_dir=\"out\",\n    per_device_train_batch_size=1,\n    gradient_accumulation_steps=8,\n    num_train_epochs=1,\n    logging_steps=10,\n    eval_strategy=\"steps\",\n    eval_steps=50,\n    save_steps=50,\n    lr_scheduler_type=\"cosine\",\n    warmup_ratio=0.03,\n    fp16=False, bf16=True,\n    gradient_checkpointing=True,\n    max_grad_norm=1.0,\n)\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nWindows에서 DataLoader 프리즈 방지 + pin_memory/persistent_workers\n\n[CONTEXT]\n<<<FILE examples/ai83/dataloader_main.py>>>\nimport torch, torch.utils.data as tud\n\nclass Dummy(torch.utils.data.Dataset):\n    def __len__(self): return 8\n    def __getitem__(self, i): return i\n\n# BUG: 모듈 임포트만으로 워커가 스폰되어 Windows에서 프리즈\nloader = tud.DataLoader(Dummy(), batch_size=2, num_workers=4)\nfor b in loader:\n    pass\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai83/dataloader_main.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["if __name__ == \"__main__\":"]}, "code": "import torch, torch.utils.data as tud\n\nclass Dummy(torch.utils.data.Dataset):\n    def __len__(self): return 8\n    def __getitem__(self, i): return i\n\n\ndef main():\n    loader = tud.DataLoader(Dummy(), batch_size=2, num_workers=4, pin_memory=True, persistent_workers=True)\n    for _ in loader:\n        pass\n\n\nif __name__ == \"__main__\":\n    main()\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n체크포인트 재개 지원\n\n[CONTEXT]\n<<<FILE examples/ai449/trainer_resume.py>>>\nfrom transformers import Trainer\ntrainer = Trainer(... )\n# BUG: 재시작 미지원\ntrainer.train()\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai449/trainer_resume.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["resume_from_checkpoint"]}, "code": "from transformers import Trainer\ntrainer = Trainer(... )\ntrainer.train(resume_from_checkpoint=True)\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nflash 미지원 시 sdpa 안전 경로\n\n[CONTEXT]\n<<<FILE examples/ai506/model_load.py>>>\nfrom transformers import AutoModelForCausalLM\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"Qwen/Qwen3-8B\",\n)\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai506/model_load.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["attn_implementation"]}, "code": "from transformers import AutoModelForCausalLM\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"Qwen/Qwen3-8B\",\n    attn_implementation=\"sdpa\",\n)\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n조기 종료 콜백 추가\n\n[CONTEXT]\n<<<FILE examples/ai553/callbacks.py>>>\nfrom transformers import TrainerCallback\n\nclass MyCB(TrainerCallback):\n    pass\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai553/callbacks.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["EarlyStoppingCallback"]}, "code": "from transformers import TrainerCallback, EarlyStoppingCallback\n\nclass MyCB(TrainerCallback):\n    pass\n\nCALLBACKS = [EarlyStoppingCallback(early_stopping_patience=2)]\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nflash 미지원 시 sdpa 안전 경로\n\n[CONTEXT]\n<<<FILE examples/ai513/model_load.py>>>\nfrom transformers import AutoModelForCausalLM\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"Qwen/Qwen3-8B\",\n)\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai513/model_load.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["attn_implementation"]}, "code": "from transformers import AutoModelForCausalLM\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"Qwen/Qwen3-8B\",\n    attn_implementation=\"sdpa\",\n)\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n체크포인트 재개 지원\n\n[CONTEXT]\n<<<FILE examples/ai279/trainer_resume.py>>>\nfrom transformers import Trainer\ntrainer = Trainer(... )\n# BUG: 재시작 미지원\ntrainer.train()\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai279/trainer_resume.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["resume_from_checkpoint"]}, "code": "from transformers import Trainer\ntrainer = Trainer(... )\ntrainer.train(resume_from_checkpoint=True)\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n8bit 옵티마이저 + 코사인 스케줄러\n\n[CONTEXT]\n<<<FILE examples/ai413/optim.py>>>\nimport torch\nopt = torch.optim.Adam([], lr=3e-4)\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai413/optim.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["PagedAdamW8bit"]}, "code": "from transformers import get_scheduler\nfrom bitsandbytes.optim import PagedAdamW8bit\nopt = PagedAdamW8bit([], lr=2e-4)\nsched = get_scheduler(\"cosine\", optimizer=opt, num_warmup_steps=100, num_training_steps=1000)\n"}]}}
{"instruction": "Propose a minimal JSON plan for code changes. Output PLAN JSON only.", "input": "[INTENT]\n학습 안정화(AMP+훈련 설정)\n\n[CONTEXT]\n<<<FILE examples/ai764/train_utils.py>>>\ndef get_dtype():\n    return torch.float16\n\n<<<END>>>\n<<<FILE examples/ai764/training_args.py>>>\nargs = TrainingArguments(output_dir='out', per_device_train_batch_size=1, num_train_epochs=1)\n\n<<<END>>>", "output": {"files": [{"path": "examples/ai764/train_utils.py", "reason": "dtype가 고정 float16", "strategy": "regex", "tests": ["bf16 또는 fp16 반환"]}, {"path": "examples/ai764/training_args.py", "reason": "로깅/평가/GA 설정 누락", "strategy": "regex", "tests": ["eval_strategy=='steps'"]}], "notes": "bf16 지원 시 우선"}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nflash 미지원 시 sdpa 안전 경로\n\n[CONTEXT]\n<<<FILE examples/ai480/model_load.py>>>\nfrom transformers import AutoModelForCausalLM\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"Qwen/Qwen3-8B\",\n)\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai480/model_load.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["attn_implementation"]}, "code": "from transformers import AutoModelForCausalLM\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"Qwen/Qwen3-8B\",\n    attn_implementation=\"sdpa\",\n)\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\npad 토큰 ID 정확히 설정\n\n[CONTEXT]\n<<<FILE examples/ai128/label_cfg.py>>>\nfrom transformers import AutoTokenizer\nfrom dataclasses import dataclass\n\n@dataclass\nclass Cfg:\n    pad_id: int = -100  # BUG\n\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai128/label_cfg.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["CFG = Cfg"]}, "code": "from transformers import AutoTokenizer\nfrom dataclasses import dataclass\n\n@dataclass\nclass Cfg:\n    pad_id: int | None = None\n\n# tokenizer 로드 후 설정\ntok = AutoTokenizer.from_pretrained(\"Qwen/Qwen3-8B\", use_fast=True)\nCFG = Cfg(pad_id=tok.pad_token_id)\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nWindows에서 DataLoader 프리즈 방지 + pin_memory/persistent_workers\n\n[CONTEXT]\n<<<FILE examples/ai450/dataloader_main.py>>>\nimport torch, torch.utils.data as tud\n\nclass Dummy(torch.utils.data.Dataset):\n    def __len__(self): return 8\n    def __getitem__(self, i): return i\n\n# BUG: 모듈 임포트만으로 워커가 스폰되어 Windows에서 프리즈\nloader = tud.DataLoader(Dummy(), batch_size=2, num_workers=4)\nfor b in loader:\n    pass\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai450/dataloader_main.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["if __name__ == \"__main__\":"]}, "code": "import torch, torch.utils.data as tud\n\nclass Dummy(torch.utils.data.Dataset):\n    def __len__(self): return 8\n    def __getitem__(self, i): return i\n\n\ndef main():\n    loader = tud.DataLoader(Dummy(), batch_size=2, num_workers=4, pin_memory=True, persistent_workers=True)\n    for _ in loader:\n        pass\n\n\nif __name__ == \"__main__\":\n    main()\n"}]}}
{"instruction": "Propose a minimal JSON plan for code changes. Output PLAN JSON only.", "input": "[INTENT]\n학습 안정화(AMP+훈련 설정)\n\n[CONTEXT]\n<<<FILE examples/ai762/train_utils.py>>>\ndef get_dtype():\n    return torch.float16\n\n<<<END>>>\n<<<FILE examples/ai762/training_args.py>>>\nargs = TrainingArguments(output_dir='out', per_device_train_batch_size=1, num_train_epochs=1)\n\n<<<END>>>", "output": {"files": [{"path": "examples/ai762/train_utils.py", "reason": "dtype가 고정 float16", "strategy": "regex", "tests": ["bf16 또는 fp16 반환"]}, {"path": "examples/ai762/training_args.py", "reason": "로깅/평가/GA 설정 누락", "strategy": "regex", "tests": ["eval_strategy=='steps'"]}], "notes": "bf16 지원 시 우선"}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n재현성: 시드 함수 추가\n\n[CONTEXT]\n<<<FILE examples/ai315/seed_utils.py>>>\n# BUG: 시드 미설정\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai315/seed_utils.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["set_seed("]}, "code": "import os, random, numpy as np, torch\n\ndef set_seed(seed: int = 42):\n    random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n재현성: 시드 함수 추가\n\n[CONTEXT]\n<<<FILE examples/ai170/seed_utils.py>>>\n# BUG: 시드 미설정\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai170/seed_utils.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["set_seed("]}, "code": "import os, random, numpy as np, torch\n\ndef set_seed(seed: int = 42):\n    random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\npad 토큰 ID 정확히 설정\n\n[CONTEXT]\n<<<FILE examples/ai599/label_cfg.py>>>\nfrom transformers import AutoTokenizer\nfrom dataclasses import dataclass\n\n@dataclass\nclass Cfg:\n    pad_id: int = -100  # BUG\n\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai599/label_cfg.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["CFG = Cfg"]}, "code": "from transformers import AutoTokenizer\nfrom dataclasses import dataclass\n\n@dataclass\nclass Cfg:\n    pad_id: int | None = None\n\n# tokenizer 로드 후 설정\ntok = AutoTokenizer.from_pretrained(\"Qwen/Qwen3-8B\", use_fast=True)\nCFG = Cfg(pad_id=tok.pad_token_id)\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n체크포인트 재개 지원\n\n[CONTEXT]\n<<<FILE examples/ai74/trainer_resume.py>>>\nfrom transformers import Trainer\ntrainer = Trainer(... )\n# BUG: 재시작 미지원\ntrainer.train()\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai74/trainer_resume.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["resume_from_checkpoint"]}, "code": "from transformers import Trainer\ntrainer = Trainer(... )\ntrainer.train(resume_from_checkpoint=True)\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n조기 종료 콜백 추가\n\n[CONTEXT]\n<<<FILE examples/ai146/callbacks.py>>>\nfrom transformers import TrainerCallback\n\nclass MyCB(TrainerCallback):\n    pass\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai146/callbacks.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["EarlyStoppingCallback"]}, "code": "from transformers import TrainerCallback, EarlyStoppingCallback\n\nclass MyCB(TrainerCallback):\n    pass\n\nCALLBACKS = [EarlyStoppingCallback(early_stopping_patience=2)]\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nWindows에서 DataLoader 프리즈 방지 + pin_memory/persistent_workers\n\n[CONTEXT]\n<<<FILE examples/ai273/dataloader_main.py>>>\nimport torch, torch.utils.data as tud\n\nclass Dummy(torch.utils.data.Dataset):\n    def __len__(self): return 8\n    def __getitem__(self, i): return i\n\n# BUG: 모듈 임포트만으로 워커가 스폰되어 Windows에서 프리즈\nloader = tud.DataLoader(Dummy(), batch_size=2, num_workers=4)\nfor b in loader:\n    pass\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai273/dataloader_main.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["if __name__ == \"__main__\":"]}, "code": "import torch, torch.utils.data as tud\n\nclass Dummy(torch.utils.data.Dataset):\n    def __len__(self): return 8\n    def __getitem__(self, i): return i\n\n\ndef main():\n    loader = tud.DataLoader(Dummy(), batch_size=2, num_workers=4, pin_memory=True, persistent_workers=True)\n    for _ in loader:\n        pass\n\n\nif __name__ == \"__main__\":\n    main()\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n체크포인트 재개 지원\n\n[CONTEXT]\n<<<FILE examples/ai617/trainer_resume.py>>>\nfrom transformers import Trainer\ntrainer = Trainer(... )\n# BUG: 재시작 미지원\ntrainer.train()\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai617/trainer_resume.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["resume_from_checkpoint"]}, "code": "from transformers import Trainer\ntrainer = Trainer(... )\ntrainer.train(resume_from_checkpoint=True)\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nLoRA target_modules 확장\n\n[CONTEXT]\n<<<FILE examples/ai231/lora_cfg.py>>>\nfrom peft import LoraConfig\n\nLC = LoraConfig(r=8, target_modules=[\n    \"q_proj\", \"v_proj\"  # BUG: 부족\n])\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai231/lora_cfg.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["k_proj", "o_proj"]}, "code": "from peft import LoraConfig\n\nLC = LoraConfig(r=8, target_modules=[\n    \"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"\n])\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nDDP/torchrun 안전 가드\n\n[CONTEXT]\n<<<FILE examples/ai558/ddp_main.py>>>\nimport torch\n\n# BUG: torchrun에서 main 가드 없음\ntrainer()\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai558/ddp_main.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["__main__"]}, "code": "import torch\n\n\ndef main():\n    trainer()\n\n\nif __name__ == \"__main__\":\n    main()\n"}]}}
{"instruction": "Propose a minimal JSON plan for code changes. Output PLAN JSON only.", "input": "[INTENT]\n학습 안정화(AMP+훈련 설정)\n\n[CONTEXT]\n<<<FILE examples/ai793/train_utils.py>>>\ndef get_dtype():\n    return torch.float16\n\n<<<END>>>\n<<<FILE examples/ai793/training_args.py>>>\nargs = TrainingArguments(output_dir='out', per_device_train_batch_size=1, num_train_epochs=1)\n\n<<<END>>>", "output": {"files": [{"path": "examples/ai793/train_utils.py", "reason": "dtype가 고정 float16", "strategy": "regex", "tests": ["bf16 또는 fp16 반환"]}, {"path": "examples/ai793/training_args.py", "reason": "로깅/평가/GA 설정 누락", "strategy": "regex", "tests": ["eval_strategy=='steps'"]}], "notes": "bf16 지원 시 우선"}}
{"instruction": "Propose a minimal JSON plan for code changes. Output PLAN JSON only.", "input": "[INTENT]\n학습 안정화(AMP+훈련 설정)\n\n[CONTEXT]\n<<<FILE examples/ai677/train_utils.py>>>\ndef get_dtype():\n    return torch.float16\n\n<<<END>>>\n<<<FILE examples/ai677/training_args.py>>>\nargs = TrainingArguments(output_dir='out', per_device_train_batch_size=1, num_train_epochs=1)\n\n<<<END>>>", "output": {"files": [{"path": "examples/ai677/train_utils.py", "reason": "dtype가 고정 float16", "strategy": "regex", "tests": ["bf16 또는 fp16 반환"]}, {"path": "examples/ai677/training_args.py", "reason": "로깅/평가/GA 설정 누락", "strategy": "regex", "tests": ["eval_strategy=='steps'"]}], "notes": "bf16 지원 시 우선"}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n훈련 안정화: GA steps/로깅/평가/체크포인팅/스케줄러\n\n[CONTEXT]\n<<<FILE examples/ai203/training_args.py>>>\nfrom transformers import TrainingArguments\n\nargs = TrainingArguments(\n    output_dir=\"out\",\n    per_device_train_batch_size=1,\n    num_train_epochs=1,\n)\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai203/training_args.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["gradient_accumulation_steps"]}, "code": "from transformers import TrainingArguments\n\nargs = TrainingArguments(\n    output_dir=\"out\",\n    per_device_train_batch_size=1,\n    gradient_accumulation_steps=8,\n    num_train_epochs=1,\n    logging_steps=10,\n    eval_strategy=\"steps\",\n    eval_steps=50,\n    save_steps=50,\n    lr_scheduler_type=\"cosine\",\n    warmup_ratio=0.03,\n    fp16=False, bf16=True,\n    gradient_checkpointing=True,\n    max_grad_norm=1.0,\n)\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nGPU bf16 지원 시 dtype 선택 개선\n\n[CONTEXT]\n<<<FILE examples/ai595/train_utils.py>>>\nimport torch\n\n# BUG: dtype가 고정되어 혼합정밀에 불리\ndef get_dtype():\n    return torch.float16\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai595/train_utils.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["torch.bfloat16"]}, "code": "import torch\n\n\ndef get_dtype():\n    \"\"\"GPU가 bf16 지원 시 bfloat16, 아니면 float16 반환\"\"\"\n    if torch.cuda.is_available() and torch.cuda.is_bf16_supported():\n        return torch.bfloat16\n    return torch.float16\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nWindows에서 DataLoader 프리즈 방지 + pin_memory/persistent_workers\n\n[CONTEXT]\n<<<FILE examples/ai240/dataloader_main.py>>>\nimport torch, torch.utils.data as tud\n\nclass Dummy(torch.utils.data.Dataset):\n    def __len__(self): return 8\n    def __getitem__(self, i): return i\n\n# BUG: 모듈 임포트만으로 워커가 스폰되어 Windows에서 프리즈\nloader = tud.DataLoader(Dummy(), batch_size=2, num_workers=4)\nfor b in loader:\n    pass\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai240/dataloader_main.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["if __name__ == \"__main__\":"]}, "code": "import torch, torch.utils.data as tud\n\nclass Dummy(torch.utils.data.Dataset):\n    def __len__(self): return 8\n    def __getitem__(self, i): return i\n\n\ndef main():\n    loader = tud.DataLoader(Dummy(), batch_size=2, num_workers=4, pin_memory=True, persistent_workers=True)\n    for _ in loader:\n        pass\n\n\nif __name__ == \"__main__\":\n    main()\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\npad 토큰 ID 정확히 설정\n\n[CONTEXT]\n<<<FILE examples/ai73/label_cfg.py>>>\nfrom transformers import AutoTokenizer\nfrom dataclasses import dataclass\n\n@dataclass\nclass Cfg:\n    pad_id: int = -100  # BUG\n\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai73/label_cfg.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["CFG = Cfg"]}, "code": "from transformers import AutoTokenizer\nfrom dataclasses import dataclass\n\n@dataclass\nclass Cfg:\n    pad_id: int | None = None\n\n# tokenizer 로드 후 설정\ntok = AutoTokenizer.from_pretrained(\"Qwen/Qwen3-8B\", use_fast=True)\nCFG = Cfg(pad_id=tok.pad_token_id)\n"}]}}
{"instruction": "Propose a minimal JSON plan for code changes. Output PLAN JSON only.", "input": "[INTENT]\n학습 안정화(AMP+훈련 설정)\n\n[CONTEXT]\n<<<FILE examples/ai673/train_utils.py>>>\ndef get_dtype():\n    return torch.float16\n\n<<<END>>>\n<<<FILE examples/ai673/training_args.py>>>\nargs = TrainingArguments(output_dir='out', per_device_train_batch_size=1, num_train_epochs=1)\n\n<<<END>>>", "output": {"files": [{"path": "examples/ai673/train_utils.py", "reason": "dtype가 고정 float16", "strategy": "regex", "tests": ["bf16 또는 fp16 반환"]}, {"path": "examples/ai673/training_args.py", "reason": "로깅/평가/GA 설정 누락", "strategy": "regex", "tests": ["eval_strategy=='steps'"]}], "notes": "bf16 지원 시 우선"}}
{"instruction": "Propose a minimal JSON plan for code changes. Output PLAN JSON only.", "input": "[INTENT]\n학습 안정화(AMP+훈련 설정)\n\n[CONTEXT]\n<<<FILE examples/ai655/train_utils.py>>>\ndef get_dtype():\n    return torch.float16\n\n<<<END>>>\n<<<FILE examples/ai655/training_args.py>>>\nargs = TrainingArguments(output_dir='out', per_device_train_batch_size=1, num_train_epochs=1)\n\n<<<END>>>", "output": {"files": [{"path": "examples/ai655/train_utils.py", "reason": "dtype가 고정 float16", "strategy": "regex", "tests": ["bf16 또는 fp16 반환"]}, {"path": "examples/ai655/training_args.py", "reason": "로깅/평가/GA 설정 누락", "strategy": "regex", "tests": ["eval_strategy=='steps'"]}], "notes": "bf16 지원 시 우선"}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n학습 후 어댑터 저장 추가\n\n[CONTEXT]\n<<<FILE examples/ai296/save_after_train.py>>>\nfrom peft import PeftModel\n# ... 학습 후\n# BUG: 저장 누락\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai296/save_after_train.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["save_pretrained"]}, "code": "from peft import PeftModel\n# ... 학습 후\nmodel.save_pretrained(\"training/qlora-out/export\")\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n학습 후 어댑터 저장 추가\n\n[CONTEXT]\n<<<FILE examples/ai238/save_after_train.py>>>\nfrom peft import PeftModel\n# ... 학습 후\n# BUG: 저장 누락\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai238/save_after_train.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["save_pretrained"]}, "code": "from peft import PeftModel\n# ... 학습 후\nmodel.save_pretrained(\"training/qlora-out/export\")\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n체크포인트 재개 지원\n\n[CONTEXT]\n<<<FILE examples/ai34/trainer_resume.py>>>\nfrom transformers import Trainer\ntrainer = Trainer(... )\n# BUG: 재시작 미지원\ntrainer.train()\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai34/trainer_resume.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["resume_from_checkpoint"]}, "code": "from transformers import Trainer\ntrainer = Trainer(... )\ntrainer.train(resume_from_checkpoint=True)\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n훈련 안정화: GA steps/로깅/평가/체크포인팅/스케줄러\n\n[CONTEXT]\n<<<FILE examples/ai299/training_args.py>>>\nfrom transformers import TrainingArguments\n\nargs = TrainingArguments(\n    output_dir=\"out\",\n    per_device_train_batch_size=1,\n    num_train_epochs=1,\n)\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai299/training_args.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["gradient_accumulation_steps"]}, "code": "from transformers import TrainingArguments\n\nargs = TrainingArguments(\n    output_dir=\"out\",\n    per_device_train_batch_size=1,\n    gradient_accumulation_steps=8,\n    num_train_epochs=1,\n    logging_steps=10,\n    eval_strategy=\"steps\",\n    eval_steps=50,\n    save_steps=50,\n    lr_scheduler_type=\"cosine\",\n    warmup_ratio=0.03,\n    fp16=False, bf16=True,\n    gradient_checkpointing=True,\n    max_grad_norm=1.0,\n)\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nWindows에서 DataLoader 프리즈 방지 + pin_memory/persistent_workers\n\n[CONTEXT]\n<<<FILE examples/ai482/dataloader_main.py>>>\nimport torch, torch.utils.data as tud\n\nclass Dummy(torch.utils.data.Dataset):\n    def __len__(self): return 8\n    def __getitem__(self, i): return i\n\n# BUG: 모듈 임포트만으로 워커가 스폰되어 Windows에서 프리즈\nloader = tud.DataLoader(Dummy(), batch_size=2, num_workers=4)\nfor b in loader:\n    pass\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai482/dataloader_main.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["if __name__ == \"__main__\":"]}, "code": "import torch, torch.utils.data as tud\n\nclass Dummy(torch.utils.data.Dataset):\n    def __len__(self): return 8\n    def __getitem__(self, i): return i\n\n\ndef main():\n    loader = tud.DataLoader(Dummy(), batch_size=2, num_workers=4, pin_memory=True, persistent_workers=True)\n    for _ in loader:\n        pass\n\n\nif __name__ == \"__main__\":\n    main()\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n필수 import 추가\n\n[CONTEXT]\n<<<FILE examples/ai606/imports.py>>>\n# BUG: numpy 미사용이지만 추후 사용 예정\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai606/imports.py", "loc": {"type": "anchor", "before": "\\A"}, "action": "insert_after", "once": true, "pre": {"must_not_contain": ["import numpy as np"]}, "code": "import numpy as np\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n체크포인트 재개 지원\n\n[CONTEXT]\n<<<FILE examples/ai419/trainer_resume.py>>>\nfrom transformers import Trainer\ntrainer = Trainer(... )\n# BUG: 재시작 미지원\ntrainer.train()\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai419/trainer_resume.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["resume_from_checkpoint"]}, "code": "from transformers import Trainer\ntrainer = Trainer(... )\ntrainer.train(resume_from_checkpoint=True)\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nGPU bf16 지원 시 dtype 선택 개선\n\n[CONTEXT]\n<<<FILE examples/ai3/train_utils.py>>>\nimport torch\n\n# BUG: dtype가 고정되어 혼합정밀에 불리\ndef get_dtype():\n    return torch.float16\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai3/train_utils.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["torch.bfloat16"]}, "code": "import torch\n\n\ndef get_dtype():\n    \"\"\"GPU가 bf16 지원 시 bfloat16, 아니면 float16 반환\"\"\"\n    if torch.cuda.is_available() and torch.cuda.is_bf16_supported():\n        return torch.bfloat16\n    return torch.float16\n"}]}}
{"instruction": "Propose a minimal JSON plan for code changes. Output PLAN JSON only.", "input": "[INTENT]\n입출력/런타임 안전성 개선\n\n[CONTEXT]\n<<<FILE examples/ai782/dataloader_main.py>>>\nloader = DataLoader(...); for b in loader: pass\n\n<<<END>>>\n<<<FILE examples/ai782/ddp_main.py>>>\ntrainer()\n\n<<<END>>>", "output": {"files": [{"path": "examples/ai782/dataloader_main.py", "reason": "Windows freeze 가능", "strategy": "regex", "tests": ["__main__ 가드"]}, {"path": "examples/ai782/ddp_main.py", "reason": "DDP 가드 없음", "strategy": "regex", "tests": ["__main__"]}], "notes": "spawn 보호"}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nWindows에서 DataLoader 프리즈 방지 + pin_memory/persistent_workers\n\n[CONTEXT]\n<<<FILE examples/ai77/dataloader_main.py>>>\nimport torch, torch.utils.data as tud\n\nclass Dummy(torch.utils.data.Dataset):\n    def __len__(self): return 8\n    def __getitem__(self, i): return i\n\n# BUG: 모듈 임포트만으로 워커가 스폰되어 Windows에서 프리즈\nloader = tud.DataLoader(Dummy(), batch_size=2, num_workers=4)\nfor b in loader:\n    pass\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai77/dataloader_main.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["if __name__ == \"__main__\":"]}, "code": "import torch, torch.utils.data as tud\n\nclass Dummy(torch.utils.data.Dataset):\n    def __len__(self): return 8\n    def __getitem__(self, i): return i\n\n\ndef main():\n    loader = tud.DataLoader(Dummy(), batch_size=2, num_workers=4, pin_memory=True, persistent_workers=True)\n    for _ in loader:\n        pass\n\n\nif __name__ == \"__main__\":\n    main()\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nDDP/torchrun 안전 가드\n\n[CONTEXT]\n<<<FILE examples/ai566/ddp_main.py>>>\nimport torch\n\n# BUG: torchrun에서 main 가드 없음\ntrainer()\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai566/ddp_main.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["__main__"]}, "code": "import torch\n\n\ndef main():\n    trainer()\n\n\nif __name__ == \"__main__\":\n    main()\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nflash 미지원 시 sdpa 안전 경로\n\n[CONTEXT]\n<<<FILE examples/ai178/model_load.py>>>\nfrom transformers import AutoModelForCausalLM\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"Qwen/Qwen3-8B\",\n)\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai178/model_load.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["attn_implementation"]}, "code": "from transformers import AutoModelForCausalLM\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"Qwen/Qwen3-8B\",\n    attn_implementation=\"sdpa\",\n)\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n필수 import 추가\n\n[CONTEXT]\n<<<FILE examples/ai246/imports.py>>>\n# BUG: numpy 미사용이지만 추후 사용 예정\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai246/imports.py", "loc": {"type": "anchor", "before": "\\A"}, "action": "insert_after", "once": true, "pre": {"must_not_contain": ["import numpy as np"]}, "code": "import numpy as np\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nDDP/torchrun 안전 가드\n\n[CONTEXT]\n<<<FILE examples/ai161/ddp_main.py>>>\nimport torch\n\n# BUG: torchrun에서 main 가드 없음\ntrainer()\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai161/ddp_main.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["__main__"]}, "code": "import torch\n\n\ndef main():\n    trainer()\n\n\nif __name__ == \"__main__\":\n    main()\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n8bit 옵티마이저 + 코사인 스케줄러\n\n[CONTEXT]\n<<<FILE examples/ai84/optim.py>>>\nimport torch\nopt = torch.optim.Adam([], lr=3e-4)\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai84/optim.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["PagedAdamW8bit"]}, "code": "from transformers import get_scheduler\nfrom bitsandbytes.optim import PagedAdamW8bit\nopt = PagedAdamW8bit([], lr=2e-4)\nsched = get_scheduler(\"cosine\", optimizer=opt, num_warmup_steps=100, num_training_steps=1000)\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nLoRA target_modules 확장\n\n[CONTEXT]\n<<<FILE examples/ai271/lora_cfg.py>>>\nfrom peft import LoraConfig\n\nLC = LoraConfig(r=8, target_modules=[\n    \"q_proj\", \"v_proj\"  # BUG: 부족\n])\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai271/lora_cfg.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["k_proj", "o_proj"]}, "code": "from peft import LoraConfig\n\nLC = LoraConfig(r=8, target_modules=[\n    \"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"\n])\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n훈련 안정화: GA steps/로깅/평가/체크포인팅/스케줄러\n\n[CONTEXT]\n<<<FILE examples/ai217/training_args.py>>>\nfrom transformers import TrainingArguments\n\nargs = TrainingArguments(\n    output_dir=\"out\",\n    per_device_train_batch_size=1,\n    num_train_epochs=1,\n)\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai217/training_args.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["gradient_accumulation_steps"]}, "code": "from transformers import TrainingArguments\n\nargs = TrainingArguments(\n    output_dir=\"out\",\n    per_device_train_batch_size=1,\n    gradient_accumulation_steps=8,\n    num_train_epochs=1,\n    logging_steps=10,\n    eval_strategy=\"steps\",\n    eval_steps=50,\n    save_steps=50,\n    lr_scheduler_type=\"cosine\",\n    warmup_ratio=0.03,\n    fp16=False, bf16=True,\n    gradient_checkpointing=True,\n    max_grad_norm=1.0,\n)\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nLoRA target_modules 확장\n\n[CONTEXT]\n<<<FILE examples/ai213/lora_cfg.py>>>\nfrom peft import LoraConfig\n\nLC = LoraConfig(r=8, target_modules=[\n    \"q_proj\", \"v_proj\"  # BUG: 부족\n])\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai213/lora_cfg.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["k_proj", "o_proj"]}, "code": "from peft import LoraConfig\n\nLC = LoraConfig(r=8, target_modules=[\n    \"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"\n])\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n필수 import 추가\n\n[CONTEXT]\n<<<FILE examples/ai102/imports.py>>>\n# BUG: numpy 미사용이지만 추후 사용 예정\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai102/imports.py", "loc": {"type": "anchor", "before": "\\A"}, "action": "insert_after", "once": true, "pre": {"must_not_contain": ["import numpy as np"]}, "code": "import numpy as np\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n학습 후 어댑터 저장 추가\n\n[CONTEXT]\n<<<FILE examples/ai62/save_after_train.py>>>\nfrom peft import PeftModel\n# ... 학습 후\n# BUG: 저장 누락\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai62/save_after_train.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["save_pretrained"]}, "code": "from peft import PeftModel\n# ... 학습 후\nmodel.save_pretrained(\"training/qlora-out/export\")\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n학습 후 어댑터 저장 추가\n\n[CONTEXT]\n<<<FILE examples/ai583/save_after_train.py>>>\nfrom peft import PeftModel\n# ... 학습 후\n# BUG: 저장 누락\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai583/save_after_train.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["save_pretrained"]}, "code": "from peft import PeftModel\n# ... 학습 후\nmodel.save_pretrained(\"training/qlora-out/export\")\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n재현성: 시드 함수 추가\n\n[CONTEXT]\n<<<FILE examples/ai313/seed_utils.py>>>\n# BUG: 시드 미설정\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai313/seed_utils.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["set_seed("]}, "code": "import os, random, numpy as np, torch\n\ndef set_seed(seed: int = 42):\n    random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n필수 import 추가\n\n[CONTEXT]\n<<<FILE examples/ai440/imports.py>>>\n# BUG: numpy 미사용이지만 추후 사용 예정\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai440/imports.py", "loc": {"type": "anchor", "before": "\\A"}, "action": "insert_after", "once": true, "pre": {"must_not_contain": ["import numpy as np"]}, "code": "import numpy as np\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nDDP/torchrun 안전 가드\n\n[CONTEXT]\n<<<FILE examples/ai603/ddp_main.py>>>\nimport torch\n\n# BUG: torchrun에서 main 가드 없음\ntrainer()\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai603/ddp_main.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["__main__"]}, "code": "import torch\n\n\ndef main():\n    trainer()\n\n\nif __name__ == \"__main__\":\n    main()\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nDDP/torchrun 안전 가드\n\n[CONTEXT]\n<<<FILE examples/ai305/ddp_main.py>>>\nimport torch\n\n# BUG: torchrun에서 main 가드 없음\ntrainer()\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai305/ddp_main.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["__main__"]}, "code": "import torch\n\n\ndef main():\n    trainer()\n\n\nif __name__ == \"__main__\":\n    main()\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nAMP 적용 + bf16 우선\n\n[CONTEXT]\n<<<FILE examples/ai454/train_step.py>>>\nimport torch\n\n# BUG: autocast 미사용\ndef train_step(model, batch):\n    x, y = batch\n    out = model(x)\n    loss = out.loss if hasattr(out, 'loss') else out.mean()\n    loss.backward()\n    return loss.item()\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai454/train_step.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["autocast"]}, "code": "import torch\n\n@torch.no_grad()\ndef _bf16_supported():\n    return torch.cuda.is_available() and torch.cuda.is_bf16_supported()\n\n\ndef train_step(model, batch):\n    x, y = batch\n    amp_dtype = torch.bfloat16 if _bf16_supported() else torch.float16\n    scaler = torch.cuda.amp.GradScaler(enabled=True)\n    with torch.cuda.amp.autocast(enabled=True, dtype=amp_dtype):\n        out = model(x)\n        loss = out.loss if hasattr(out, 'loss') else out.mean()\n    scaler.scale(loss).backward()\n    scaler.step(torch.optim.Adam(model.parameters(), lr=1e-4))\n    scaler.update()\n    model.zero_grad(set_to_none=True)\n    return float(loss.detach().item())\n"}]}}
{"instruction": "Propose a minimal JSON plan for code changes. Output PLAN JSON only.", "input": "[INTENT]\n입출력/런타임 안전성 개선\n\n[CONTEXT]\n<<<FILE examples/ai743/dataloader_main.py>>>\nloader = DataLoader(...); for b in loader: pass\n\n<<<END>>>\n<<<FILE examples/ai743/ddp_main.py>>>\ntrainer()\n\n<<<END>>>", "output": {"files": [{"path": "examples/ai743/dataloader_main.py", "reason": "Windows freeze 가능", "strategy": "regex", "tests": ["__main__ 가드"]}, {"path": "examples/ai743/ddp_main.py", "reason": "DDP 가드 없음", "strategy": "regex", "tests": ["__main__"]}], "notes": "spawn 보호"}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n체크포인트 재개 지원\n\n[CONTEXT]\n<<<FILE examples/ai390/trainer_resume.py>>>\nfrom transformers import Trainer\ntrainer = Trainer(... )\n# BUG: 재시작 미지원\ntrainer.train()\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai390/trainer_resume.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["resume_from_checkpoint"]}, "code": "from transformers import Trainer\ntrainer = Trainer(... )\ntrainer.train(resume_from_checkpoint=True)\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nflash 미지원 시 sdpa 안전 경로\n\n[CONTEXT]\n<<<FILE examples/ai536/model_load.py>>>\nfrom transformers import AutoModelForCausalLM\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"Qwen/Qwen3-8B\",\n)\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai536/model_load.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["attn_implementation"]}, "code": "from transformers import AutoModelForCausalLM\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"Qwen/Qwen3-8B\",\n    attn_implementation=\"sdpa\",\n)\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nGPU bf16 지원 시 dtype 선택 개선\n\n[CONTEXT]\n<<<FILE examples/ai232/train_utils.py>>>\nimport torch\n\n# BUG: dtype가 고정되어 혼합정밀에 불리\ndef get_dtype():\n    return torch.float16\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai232/train_utils.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["torch.bfloat16"]}, "code": "import torch\n\n\ndef get_dtype():\n    \"\"\"GPU가 bf16 지원 시 bfloat16, 아니면 float16 반환\"\"\"\n    if torch.cuda.is_available() and torch.cuda.is_bf16_supported():\n        return torch.bfloat16\n    return torch.float16\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n체크포인트 재개 지원\n\n[CONTEXT]\n<<<FILE examples/ai427/trainer_resume.py>>>\nfrom transformers import Trainer\ntrainer = Trainer(... )\n# BUG: 재시작 미지원\ntrainer.train()\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai427/trainer_resume.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["resume_from_checkpoint"]}, "code": "from transformers import Trainer\ntrainer = Trainer(... )\ntrainer.train(resume_from_checkpoint=True)\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nDDP/torchrun 안전 가드\n\n[CONTEXT]\n<<<FILE examples/ai572/ddp_main.py>>>\nimport torch\n\n# BUG: torchrun에서 main 가드 없음\ntrainer()\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai572/ddp_main.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["__main__"]}, "code": "import torch\n\n\ndef main():\n    trainer()\n\n\nif __name__ == \"__main__\":\n    main()\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nDDP/torchrun 안전 가드\n\n[CONTEXT]\n<<<FILE examples/ai122/ddp_main.py>>>\nimport torch\n\n# BUG: torchrun에서 main 가드 없음\ntrainer()\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai122/ddp_main.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["__main__"]}, "code": "import torch\n\n\ndef main():\n    trainer()\n\n\nif __name__ == \"__main__\":\n    main()\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nLoRA target_modules 확장\n\n[CONTEXT]\n<<<FILE examples/ai255/lora_cfg.py>>>\nfrom peft import LoraConfig\n\nLC = LoraConfig(r=8, target_modules=[\n    \"q_proj\", \"v_proj\"  # BUG: 부족\n])\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai255/lora_cfg.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["k_proj", "o_proj"]}, "code": "from peft import LoraConfig\n\nLC = LoraConfig(r=8, target_modules=[\n    \"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"\n])\n"}]}}
{"instruction": "Propose a minimal JSON plan for code changes. Output PLAN JSON only.", "input": "[INTENT]\n학습 안정화(AMP+훈련 설정)\n\n[CONTEXT]\n<<<FILE examples/ai765/train_utils.py>>>\ndef get_dtype():\n    return torch.float16\n\n<<<END>>>\n<<<FILE examples/ai765/training_args.py>>>\nargs = TrainingArguments(output_dir='out', per_device_train_batch_size=1, num_train_epochs=1)\n\n<<<END>>>", "output": {"files": [{"path": "examples/ai765/train_utils.py", "reason": "dtype가 고정 float16", "strategy": "regex", "tests": ["bf16 또는 fp16 반환"]}, {"path": "examples/ai765/training_args.py", "reason": "로깅/평가/GA 설정 누락", "strategy": "regex", "tests": ["eval_strategy=='steps'"]}], "notes": "bf16 지원 시 우선"}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n필수 import 추가\n\n[CONTEXT]\n<<<FILE examples/ai312/imports.py>>>\n# BUG: numpy 미사용이지만 추후 사용 예정\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai312/imports.py", "loc": {"type": "anchor", "before": "\\A"}, "action": "insert_after", "once": true, "pre": {"must_not_contain": ["import numpy as np"]}, "code": "import numpy as np\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n조기 종료 콜백 추가\n\n[CONTEXT]\n<<<FILE examples/ai577/callbacks.py>>>\nfrom transformers import TrainerCallback\n\nclass MyCB(TrainerCallback):\n    pass\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai577/callbacks.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["EarlyStoppingCallback"]}, "code": "from transformers import TrainerCallback, EarlyStoppingCallback\n\nclass MyCB(TrainerCallback):\n    pass\n\nCALLBACKS = [EarlyStoppingCallback(early_stopping_patience=2)]\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n체크포인트 재개 지원\n\n[CONTEXT]\n<<<FILE examples/ai153/trainer_resume.py>>>\nfrom transformers import Trainer\ntrainer = Trainer(... )\n# BUG: 재시작 미지원\ntrainer.train()\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai153/trainer_resume.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["resume_from_checkpoint"]}, "code": "from transformers import Trainer\ntrainer = Trainer(... )\ntrainer.train(resume_from_checkpoint=True)\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nLoRA target_modules 확장\n\n[CONTEXT]\n<<<FILE examples/ai156/lora_cfg.py>>>\nfrom peft import LoraConfig\n\nLC = LoraConfig(r=8, target_modules=[\n    \"q_proj\", \"v_proj\"  # BUG: 부족\n])\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai156/lora_cfg.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["k_proj", "o_proj"]}, "code": "from peft import LoraConfig\n\nLC = LoraConfig(r=8, target_modules=[\n    \"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"\n])\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n8bit 옵티마이저 + 코사인 스케줄러\n\n[CONTEXT]\n<<<FILE examples/ai55/optim.py>>>\nimport torch\nopt = torch.optim.Adam([], lr=3e-4)\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai55/optim.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["PagedAdamW8bit"]}, "code": "from transformers import get_scheduler\nfrom bitsandbytes.optim import PagedAdamW8bit\nopt = PagedAdamW8bit([], lr=2e-4)\nsched = get_scheduler(\"cosine\", optimizer=opt, num_warmup_steps=100, num_training_steps=1000)\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nDDP/torchrun 안전 가드\n\n[CONTEXT]\n<<<FILE examples/ai591/ddp_main.py>>>\nimport torch\n\n# BUG: torchrun에서 main 가드 없음\ntrainer()\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai591/ddp_main.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["__main__"]}, "code": "import torch\n\n\ndef main():\n    trainer()\n\n\nif __name__ == \"__main__\":\n    main()\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n조기 종료 콜백 추가\n\n[CONTEXT]\n<<<FILE examples/ai76/callbacks.py>>>\nfrom transformers import TrainerCallback\n\nclass MyCB(TrainerCallback):\n    pass\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai76/callbacks.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["EarlyStoppingCallback"]}, "code": "from transformers import TrainerCallback, EarlyStoppingCallback\n\nclass MyCB(TrainerCallback):\n    pass\n\nCALLBACKS = [EarlyStoppingCallback(early_stopping_patience=2)]\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n재현성: 시드 함수 추가\n\n[CONTEXT]\n<<<FILE examples/ai504/seed_utils.py>>>\n# BUG: 시드 미설정\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai504/seed_utils.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["set_seed("]}, "code": "import os, random, numpy as np, torch\n\ndef set_seed(seed: int = 42):\n    random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nWindows에서 DataLoader 프리즈 방지 + pin_memory/persistent_workers\n\n[CONTEXT]\n<<<FILE examples/ai411/dataloader_main.py>>>\nimport torch, torch.utils.data as tud\n\nclass Dummy(torch.utils.data.Dataset):\n    def __len__(self): return 8\n    def __getitem__(self, i): return i\n\n# BUG: 모듈 임포트만으로 워커가 스폰되어 Windows에서 프리즈\nloader = tud.DataLoader(Dummy(), batch_size=2, num_workers=4)\nfor b in loader:\n    pass\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai411/dataloader_main.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["if __name__ == \"__main__\":"]}, "code": "import torch, torch.utils.data as tud\n\nclass Dummy(torch.utils.data.Dataset):\n    def __len__(self): return 8\n    def __getitem__(self, i): return i\n\n\ndef main():\n    loader = tud.DataLoader(Dummy(), batch_size=2, num_workers=4, pin_memory=True, persistent_workers=True)\n    for _ in loader:\n        pass\n\n\nif __name__ == \"__main__\":\n    main()\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nAMP 적용 + bf16 우선\n\n[CONTEXT]\n<<<FILE examples/ai67/train_step.py>>>\nimport torch\n\n# BUG: autocast 미사용\ndef train_step(model, batch):\n    x, y = batch\n    out = model(x)\n    loss = out.loss if hasattr(out, 'loss') else out.mean()\n    loss.backward()\n    return loss.item()\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai67/train_step.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["autocast"]}, "code": "import torch\n\n@torch.no_grad()\ndef _bf16_supported():\n    return torch.cuda.is_available() and torch.cuda.is_bf16_supported()\n\n\ndef train_step(model, batch):\n    x, y = batch\n    amp_dtype = torch.bfloat16 if _bf16_supported() else torch.float16\n    scaler = torch.cuda.amp.GradScaler(enabled=True)\n    with torch.cuda.amp.autocast(enabled=True, dtype=amp_dtype):\n        out = model(x)\n        loss = out.loss if hasattr(out, 'loss') else out.mean()\n    scaler.scale(loss).backward()\n    scaler.step(torch.optim.Adam(model.parameters(), lr=1e-4))\n    scaler.update()\n    model.zero_grad(set_to_none=True)\n    return float(loss.detach().item())\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nLoRA target_modules 확장\n\n[CONTEXT]\n<<<FILE examples/ai117/lora_cfg.py>>>\nfrom peft import LoraConfig\n\nLC = LoraConfig(r=8, target_modules=[\n    \"q_proj\", \"v_proj\"  # BUG: 부족\n])\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai117/lora_cfg.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["k_proj", "o_proj"]}, "code": "from peft import LoraConfig\n\nLC = LoraConfig(r=8, target_modules=[\n    \"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"\n])\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n조기 종료 콜백 추가\n\n[CONTEXT]\n<<<FILE examples/ai490/callbacks.py>>>\nfrom transformers import TrainerCallback\n\nclass MyCB(TrainerCallback):\n    pass\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai490/callbacks.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["EarlyStoppingCallback"]}, "code": "from transformers import TrainerCallback, EarlyStoppingCallback\n\nclass MyCB(TrainerCallback):\n    pass\n\nCALLBACKS = [EarlyStoppingCallback(early_stopping_patience=2)]\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n조기 종료 콜백 추가\n\n[CONTEXT]\n<<<FILE examples/ai259/callbacks.py>>>\nfrom transformers import TrainerCallback\n\nclass MyCB(TrainerCallback):\n    pass\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai259/callbacks.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["EarlyStoppingCallback"]}, "code": "from transformers import TrainerCallback, EarlyStoppingCallback\n\nclass MyCB(TrainerCallback):\n    pass\n\nCALLBACKS = [EarlyStoppingCallback(early_stopping_patience=2)]\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nWindows에서 DataLoader 프리즈 방지 + pin_memory/persistent_workers\n\n[CONTEXT]\n<<<FILE examples/ai53/dataloader_main.py>>>\nimport torch, torch.utils.data as tud\n\nclass Dummy(torch.utils.data.Dataset):\n    def __len__(self): return 8\n    def __getitem__(self, i): return i\n\n# BUG: 모듈 임포트만으로 워커가 스폰되어 Windows에서 프리즈\nloader = tud.DataLoader(Dummy(), batch_size=2, num_workers=4)\nfor b in loader:\n    pass\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai53/dataloader_main.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["if __name__ == \"__main__\":"]}, "code": "import torch, torch.utils.data as tud\n\nclass Dummy(torch.utils.data.Dataset):\n    def __len__(self): return 8\n    def __getitem__(self, i): return i\n\n\ndef main():\n    loader = tud.DataLoader(Dummy(), batch_size=2, num_workers=4, pin_memory=True, persistent_workers=True)\n    for _ in loader:\n        pass\n\n\nif __name__ == \"__main__\":\n    main()\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nDDP/torchrun 안전 가드\n\n[CONTEXT]\n<<<FILE examples/ai189/ddp_main.py>>>\nimport torch\n\n# BUG: torchrun에서 main 가드 없음\ntrainer()\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai189/ddp_main.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["__main__"]}, "code": "import torch\n\n\ndef main():\n    trainer()\n\n\nif __name__ == \"__main__\":\n    main()\n"}]}}
{"instruction": "Propose a minimal JSON plan for code changes. Output PLAN JSON only.", "input": "[INTENT]\n입출력/런타임 안전성 개선\n\n[CONTEXT]\n<<<FILE examples/ai709/dataloader_main.py>>>\nloader = DataLoader(...); for b in loader: pass\n\n<<<END>>>\n<<<FILE examples/ai709/ddp_main.py>>>\ntrainer()\n\n<<<END>>>", "output": {"files": [{"path": "examples/ai709/dataloader_main.py", "reason": "Windows freeze 가능", "strategy": "regex", "tests": ["__main__ 가드"]}, {"path": "examples/ai709/ddp_main.py", "reason": "DDP 가드 없음", "strategy": "regex", "tests": ["__main__"]}], "notes": "spawn 보호"}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nAMP 적용 + bf16 우선\n\n[CONTEXT]\n<<<FILE examples/ai452/train_step.py>>>\nimport torch\n\n# BUG: autocast 미사용\ndef train_step(model, batch):\n    x, y = batch\n    out = model(x)\n    loss = out.loss if hasattr(out, 'loss') else out.mean()\n    loss.backward()\n    return loss.item()\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai452/train_step.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["autocast"]}, "code": "import torch\n\n@torch.no_grad()\ndef _bf16_supported():\n    return torch.cuda.is_available() and torch.cuda.is_bf16_supported()\n\n\ndef train_step(model, batch):\n    x, y = batch\n    amp_dtype = torch.bfloat16 if _bf16_supported() else torch.float16\n    scaler = torch.cuda.amp.GradScaler(enabled=True)\n    with torch.cuda.amp.autocast(enabled=True, dtype=amp_dtype):\n        out = model(x)\n        loss = out.loss if hasattr(out, 'loss') else out.mean()\n    scaler.scale(loss).backward()\n    scaler.step(torch.optim.Adam(model.parameters(), lr=1e-4))\n    scaler.update()\n    model.zero_grad(set_to_none=True)\n    return float(loss.detach().item())\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\npad 토큰 ID 정확히 설정\n\n[CONTEXT]\n<<<FILE examples/ai491/label_cfg.py>>>\nfrom transformers import AutoTokenizer\nfrom dataclasses import dataclass\n\n@dataclass\nclass Cfg:\n    pad_id: int = -100  # BUG\n\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai491/label_cfg.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["CFG = Cfg"]}, "code": "from transformers import AutoTokenizer\nfrom dataclasses import dataclass\n\n@dataclass\nclass Cfg:\n    pad_id: int | None = None\n\n# tokenizer 로드 후 설정\ntok = AutoTokenizer.from_pretrained(\"Qwen/Qwen3-8B\", use_fast=True)\nCFG = Cfg(pad_id=tok.pad_token_id)\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nWindows에서 DataLoader 프리즈 방지 + pin_memory/persistent_workers\n\n[CONTEXT]\n<<<FILE examples/ai481/dataloader_main.py>>>\nimport torch, torch.utils.data as tud\n\nclass Dummy(torch.utils.data.Dataset):\n    def __len__(self): return 8\n    def __getitem__(self, i): return i\n\n# BUG: 모듈 임포트만으로 워커가 스폰되어 Windows에서 프리즈\nloader = tud.DataLoader(Dummy(), batch_size=2, num_workers=4)\nfor b in loader:\n    pass\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai481/dataloader_main.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["if __name__ == \"__main__\":"]}, "code": "import torch, torch.utils.data as tud\n\nclass Dummy(torch.utils.data.Dataset):\n    def __len__(self): return 8\n    def __getitem__(self, i): return i\n\n\ndef main():\n    loader = tud.DataLoader(Dummy(), batch_size=2, num_workers=4, pin_memory=True, persistent_workers=True)\n    for _ in loader:\n        pass\n\n\nif __name__ == \"__main__\":\n    main()\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nAMP 적용 + bf16 우선\n\n[CONTEXT]\n<<<FILE examples/ai292/train_step.py>>>\nimport torch\n\n# BUG: autocast 미사용\ndef train_step(model, batch):\n    x, y = batch\n    out = model(x)\n    loss = out.loss if hasattr(out, 'loss') else out.mean()\n    loss.backward()\n    return loss.item()\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai292/train_step.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["autocast"]}, "code": "import torch\n\n@torch.no_grad()\ndef _bf16_supported():\n    return torch.cuda.is_available() and torch.cuda.is_bf16_supported()\n\n\ndef train_step(model, batch):\n    x, y = batch\n    amp_dtype = torch.bfloat16 if _bf16_supported() else torch.float16\n    scaler = torch.cuda.amp.GradScaler(enabled=True)\n    with torch.cuda.amp.autocast(enabled=True, dtype=amp_dtype):\n        out = model(x)\n        loss = out.loss if hasattr(out, 'loss') else out.mean()\n    scaler.scale(loss).backward()\n    scaler.step(torch.optim.Adam(model.parameters(), lr=1e-4))\n    scaler.update()\n    model.zero_grad(set_to_none=True)\n    return float(loss.detach().item())\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\npad 토큰 ID 정확히 설정\n\n[CONTEXT]\n<<<FILE examples/ai588/label_cfg.py>>>\nfrom transformers import AutoTokenizer\nfrom dataclasses import dataclass\n\n@dataclass\nclass Cfg:\n    pad_id: int = -100  # BUG\n\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai588/label_cfg.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["CFG = Cfg"]}, "code": "from transformers import AutoTokenizer\nfrom dataclasses import dataclass\n\n@dataclass\nclass Cfg:\n    pad_id: int | None = None\n\n# tokenizer 로드 후 설정\ntok = AutoTokenizer.from_pretrained(\"Qwen/Qwen3-8B\", use_fast=True)\nCFG = Cfg(pad_id=tok.pad_token_id)\n"}]}}
{"instruction": "Propose a minimal JSON plan for code changes. Output PLAN JSON only.", "input": "[INTENT]\n입출력/런타임 안전성 개선\n\n[CONTEXT]\n<<<FILE examples/ai751/dataloader_main.py>>>\nloader = DataLoader(...); for b in loader: pass\n\n<<<END>>>\n<<<FILE examples/ai751/ddp_main.py>>>\ntrainer()\n\n<<<END>>>", "output": {"files": [{"path": "examples/ai751/dataloader_main.py", "reason": "Windows freeze 가능", "strategy": "regex", "tests": ["__main__ 가드"]}, {"path": "examples/ai751/ddp_main.py", "reason": "DDP 가드 없음", "strategy": "regex", "tests": ["__main__"]}], "notes": "spawn 보호"}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n훈련 안정화: GA steps/로깅/평가/체크포인팅/스케줄러\n\n[CONTEXT]\n<<<FILE examples/ai106/training_args.py>>>\nfrom transformers import TrainingArguments\n\nargs = TrainingArguments(\n    output_dir=\"out\",\n    per_device_train_batch_size=1,\n    num_train_epochs=1,\n)\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai106/training_args.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["gradient_accumulation_steps"]}, "code": "from transformers import TrainingArguments\n\nargs = TrainingArguments(\n    output_dir=\"out\",\n    per_device_train_batch_size=1,\n    gradient_accumulation_steps=8,\n    num_train_epochs=1,\n    logging_steps=10,\n    eval_strategy=\"steps\",\n    eval_steps=50,\n    save_steps=50,\n    lr_scheduler_type=\"cosine\",\n    warmup_ratio=0.03,\n    fp16=False, bf16=True,\n    gradient_checkpointing=True,\n    max_grad_norm=1.0,\n)\n"}]}}
{"instruction": "Propose a minimal JSON plan for code changes. Output PLAN JSON only.", "input": "[INTENT]\n학습 안정화(AMP+훈련 설정)\n\n[CONTEXT]\n<<<FILE examples/ai693/train_utils.py>>>\ndef get_dtype():\n    return torch.float16\n\n<<<END>>>\n<<<FILE examples/ai693/training_args.py>>>\nargs = TrainingArguments(output_dir='out', per_device_train_batch_size=1, num_train_epochs=1)\n\n<<<END>>>", "output": {"files": [{"path": "examples/ai693/train_utils.py", "reason": "dtype가 고정 float16", "strategy": "regex", "tests": ["bf16 또는 fp16 반환"]}, {"path": "examples/ai693/training_args.py", "reason": "로깅/평가/GA 설정 누락", "strategy": "regex", "tests": ["eval_strategy=='steps'"]}], "notes": "bf16 지원 시 우선"}}
{"instruction": "Propose a minimal JSON plan for code changes. Output PLAN JSON only.", "input": "[INTENT]\n입출력/런타임 안전성 개선\n\n[CONTEXT]\n<<<FILE examples/ai645/dataloader_main.py>>>\nloader = DataLoader(...); for b in loader: pass\n\n<<<END>>>\n<<<FILE examples/ai645/ddp_main.py>>>\ntrainer()\n\n<<<END>>>", "output": {"files": [{"path": "examples/ai645/dataloader_main.py", "reason": "Windows freeze 가능", "strategy": "regex", "tests": ["__main__ 가드"]}, {"path": "examples/ai645/ddp_main.py", "reason": "DDP 가드 없음", "strategy": "regex", "tests": ["__main__"]}], "notes": "spawn 보호"}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n조기 종료 콜백 추가\n\n[CONTEXT]\n<<<FILE examples/ai300/callbacks.py>>>\nfrom transformers import TrainerCallback\n\nclass MyCB(TrainerCallback):\n    pass\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai300/callbacks.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["EarlyStoppingCallback"]}, "code": "from transformers import TrainerCallback, EarlyStoppingCallback\n\nclass MyCB(TrainerCallback):\n    pass\n\nCALLBACKS = [EarlyStoppingCallback(early_stopping_patience=2)]\n"}]}}
{"instruction": "Propose a minimal JSON plan for code changes. Output PLAN JSON only.", "input": "[INTENT]\n학습 안정화(AMP+훈련 설정)\n\n[CONTEXT]\n<<<FILE examples/ai684/train_utils.py>>>\ndef get_dtype():\n    return torch.float16\n\n<<<END>>>\n<<<FILE examples/ai684/training_args.py>>>\nargs = TrainingArguments(output_dir='out', per_device_train_batch_size=1, num_train_epochs=1)\n\n<<<END>>>", "output": {"files": [{"path": "examples/ai684/train_utils.py", "reason": "dtype가 고정 float16", "strategy": "regex", "tests": ["bf16 또는 fp16 반환"]}, {"path": "examples/ai684/training_args.py", "reason": "로깅/평가/GA 설정 누락", "strategy": "regex", "tests": ["eval_strategy=='steps'"]}], "notes": "bf16 지원 시 우선"}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\npad 토큰 ID 정확히 설정\n\n[CONTEXT]\n<<<FILE examples/ai56/label_cfg.py>>>\nfrom transformers import AutoTokenizer\nfrom dataclasses import dataclass\n\n@dataclass\nclass Cfg:\n    pad_id: int = -100  # BUG\n\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai56/label_cfg.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["CFG = Cfg"]}, "code": "from transformers import AutoTokenizer\nfrom dataclasses import dataclass\n\n@dataclass\nclass Cfg:\n    pad_id: int | None = None\n\n# tokenizer 로드 후 설정\ntok = AutoTokenizer.from_pretrained(\"Qwen/Qwen3-8B\", use_fast=True)\nCFG = Cfg(pad_id=tok.pad_token_id)\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nLoRA target_modules 확장\n\n[CONTEXT]\n<<<FILE examples/ai47/lora_cfg.py>>>\nfrom peft import LoraConfig\n\nLC = LoraConfig(r=8, target_modules=[\n    \"q_proj\", \"v_proj\"  # BUG: 부족\n])\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai47/lora_cfg.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["k_proj", "o_proj"]}, "code": "from peft import LoraConfig\n\nLC = LoraConfig(r=8, target_modules=[\n    \"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"\n])\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nGPU bf16 지원 시 dtype 선택 개선\n\n[CONTEXT]\n<<<FILE examples/ai281/train_utils.py>>>\nimport torch\n\n# BUG: dtype가 고정되어 혼합정밀에 불리\ndef get_dtype():\n    return torch.float16\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai281/train_utils.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["torch.bfloat16"]}, "code": "import torch\n\n\ndef get_dtype():\n    \"\"\"GPU가 bf16 지원 시 bfloat16, 아니면 float16 반환\"\"\"\n    if torch.cuda.is_available() and torch.cuda.is_bf16_supported():\n        return torch.bfloat16\n    return torch.float16\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nflash 미지원 시 sdpa 안전 경로\n\n[CONTEXT]\n<<<FILE examples/ai130/model_load.py>>>\nfrom transformers import AutoModelForCausalLM\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"Qwen/Qwen3-8B\",\n)\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai130/model_load.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["attn_implementation"]}, "code": "from transformers import AutoModelForCausalLM\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"Qwen/Qwen3-8B\",\n    attn_implementation=\"sdpa\",\n)\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\npad 토큰 ID 정확히 설정\n\n[CONTEXT]\n<<<FILE examples/ai547/label_cfg.py>>>\nfrom transformers import AutoTokenizer\nfrom dataclasses import dataclass\n\n@dataclass\nclass Cfg:\n    pad_id: int = -100  # BUG\n\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai547/label_cfg.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["CFG = Cfg"]}, "code": "from transformers import AutoTokenizer\nfrom dataclasses import dataclass\n\n@dataclass\nclass Cfg:\n    pad_id: int | None = None\n\n# tokenizer 로드 후 설정\ntok = AutoTokenizer.from_pretrained(\"Qwen/Qwen3-8B\", use_fast=True)\nCFG = Cfg(pad_id=tok.pad_token_id)\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n체크포인트 재개 지원\n\n[CONTEXT]\n<<<FILE examples/ai15/trainer_resume.py>>>\nfrom transformers import Trainer\ntrainer = Trainer(... )\n# BUG: 재시작 미지원\ntrainer.train()\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai15/trainer_resume.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["resume_from_checkpoint"]}, "code": "from transformers import Trainer\ntrainer = Trainer(... )\ntrainer.train(resume_from_checkpoint=True)\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nWindows에서 DataLoader 프리즈 방지 + pin_memory/persistent_workers\n\n[CONTEXT]\n<<<FILE examples/ai310/dataloader_main.py>>>\nimport torch, torch.utils.data as tud\n\nclass Dummy(torch.utils.data.Dataset):\n    def __len__(self): return 8\n    def __getitem__(self, i): return i\n\n# BUG: 모듈 임포트만으로 워커가 스폰되어 Windows에서 프리즈\nloader = tud.DataLoader(Dummy(), batch_size=2, num_workers=4)\nfor b in loader:\n    pass\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai310/dataloader_main.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["if __name__ == \"__main__\":"]}, "code": "import torch, torch.utils.data as tud\n\nclass Dummy(torch.utils.data.Dataset):\n    def __len__(self): return 8\n    def __getitem__(self, i): return i\n\n\ndef main():\n    loader = tud.DataLoader(Dummy(), batch_size=2, num_workers=4, pin_memory=True, persistent_workers=True)\n    for _ in loader:\n        pass\n\n\nif __name__ == \"__main__\":\n    main()\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nWindows에서 DataLoader 프리즈 방지 + pin_memory/persistent_workers\n\n[CONTEXT]\n<<<FILE examples/ai184/dataloader_main.py>>>\nimport torch, torch.utils.data as tud\n\nclass Dummy(torch.utils.data.Dataset):\n    def __len__(self): return 8\n    def __getitem__(self, i): return i\n\n# BUG: 모듈 임포트만으로 워커가 스폰되어 Windows에서 프리즈\nloader = tud.DataLoader(Dummy(), batch_size=2, num_workers=4)\nfor b in loader:\n    pass\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai184/dataloader_main.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["if __name__ == \"__main__\":"]}, "code": "import torch, torch.utils.data as tud\n\nclass Dummy(torch.utils.data.Dataset):\n    def __len__(self): return 8\n    def __getitem__(self, i): return i\n\n\ndef main():\n    loader = tud.DataLoader(Dummy(), batch_size=2, num_workers=4, pin_memory=True, persistent_workers=True)\n    for _ in loader:\n        pass\n\n\nif __name__ == \"__main__\":\n    main()\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nGPU bf16 지원 시 dtype 선택 개선\n\n[CONTEXT]\n<<<FILE examples/ai370/train_utils.py>>>\nimport torch\n\n# BUG: dtype가 고정되어 혼합정밀에 불리\ndef get_dtype():\n    return torch.float16\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai370/train_utils.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["torch.bfloat16"]}, "code": "import torch\n\n\ndef get_dtype():\n    \"\"\"GPU가 bf16 지원 시 bfloat16, 아니면 float16 반환\"\"\"\n    if torch.cuda.is_available() and torch.cuda.is_bf16_supported():\n        return torch.bfloat16\n    return torch.float16\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nLoRA target_modules 확장\n\n[CONTEXT]\n<<<FILE examples/ai233/lora_cfg.py>>>\nfrom peft import LoraConfig\n\nLC = LoraConfig(r=8, target_modules=[\n    \"q_proj\", \"v_proj\"  # BUG: 부족\n])\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai233/lora_cfg.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["k_proj", "o_proj"]}, "code": "from peft import LoraConfig\n\nLC = LoraConfig(r=8, target_modules=[\n    \"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"\n])\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nLoRA target_modules 확장\n\n[CONTEXT]\n<<<FILE examples/ai304/lora_cfg.py>>>\nfrom peft import LoraConfig\n\nLC = LoraConfig(r=8, target_modules=[\n    \"q_proj\", \"v_proj\"  # BUG: 부족\n])\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai304/lora_cfg.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["k_proj", "o_proj"]}, "code": "from peft import LoraConfig\n\nLC = LoraConfig(r=8, target_modules=[\n    \"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"\n])\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n조기 종료 콜백 추가\n\n[CONTEXT]\n<<<FILE examples/ai199/callbacks.py>>>\nfrom transformers import TrainerCallback\n\nclass MyCB(TrainerCallback):\n    pass\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai199/callbacks.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["EarlyStoppingCallback"]}, "code": "from transformers import TrainerCallback, EarlyStoppingCallback\n\nclass MyCB(TrainerCallback):\n    pass\n\nCALLBACKS = [EarlyStoppingCallback(early_stopping_patience=2)]\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nWindows에서 DataLoader 프리즈 방지 + pin_memory/persistent_workers\n\n[CONTEXT]\n<<<FILE examples/ai103/dataloader_main.py>>>\nimport torch, torch.utils.data as tud\n\nclass Dummy(torch.utils.data.Dataset):\n    def __len__(self): return 8\n    def __getitem__(self, i): return i\n\n# BUG: 모듈 임포트만으로 워커가 스폰되어 Windows에서 프리즈\nloader = tud.DataLoader(Dummy(), batch_size=2, num_workers=4)\nfor b in loader:\n    pass\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai103/dataloader_main.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["if __name__ == \"__main__\":"]}, "code": "import torch, torch.utils.data as tud\n\nclass Dummy(torch.utils.data.Dataset):\n    def __len__(self): return 8\n    def __getitem__(self, i): return i\n\n\ndef main():\n    loader = tud.DataLoader(Dummy(), batch_size=2, num_workers=4, pin_memory=True, persistent_workers=True)\n    for _ in loader:\n        pass\n\n\nif __name__ == \"__main__\":\n    main()\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\npad 토큰 ID 정확히 설정\n\n[CONTEXT]\n<<<FILE examples/ai290/label_cfg.py>>>\nfrom transformers import AutoTokenizer\nfrom dataclasses import dataclass\n\n@dataclass\nclass Cfg:\n    pad_id: int = -100  # BUG\n\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai290/label_cfg.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["CFG = Cfg"]}, "code": "from transformers import AutoTokenizer\nfrom dataclasses import dataclass\n\n@dataclass\nclass Cfg:\n    pad_id: int | None = None\n\n# tokenizer 로드 후 설정\ntok = AutoTokenizer.from_pretrained(\"Qwen/Qwen3-8B\", use_fast=True)\nCFG = Cfg(pad_id=tok.pad_token_id)\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\npad 토큰 ID 정확히 설정\n\n[CONTEXT]\n<<<FILE examples/ai612/label_cfg.py>>>\nfrom transformers import AutoTokenizer\nfrom dataclasses import dataclass\n\n@dataclass\nclass Cfg:\n    pad_id: int = -100  # BUG\n\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai612/label_cfg.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["CFG = Cfg"]}, "code": "from transformers import AutoTokenizer\nfrom dataclasses import dataclass\n\n@dataclass\nclass Cfg:\n    pad_id: int | None = None\n\n# tokenizer 로드 후 설정\ntok = AutoTokenizer.from_pretrained(\"Qwen/Qwen3-8B\", use_fast=True)\nCFG = Cfg(pad_id=tok.pad_token_id)\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nDDP/torchrun 안전 가드\n\n[CONTEXT]\n<<<FILE examples/ai538/ddp_main.py>>>\nimport torch\n\n# BUG: torchrun에서 main 가드 없음\ntrainer()\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai538/ddp_main.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["__main__"]}, "code": "import torch\n\n\ndef main():\n    trainer()\n\n\nif __name__ == \"__main__\":\n    main()\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nflash 미지원 시 sdpa 안전 경로\n\n[CONTEXT]\n<<<FILE examples/ai604/model_load.py>>>\nfrom transformers import AutoModelForCausalLM\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"Qwen/Qwen3-8B\",\n)\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai604/model_load.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["attn_implementation"]}, "code": "from transformers import AutoModelForCausalLM\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"Qwen/Qwen3-8B\",\n    attn_implementation=\"sdpa\",\n)\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n필수 import 추가\n\n[CONTEXT]\n<<<FILE examples/ai284/imports.py>>>\n# BUG: numpy 미사용이지만 추후 사용 예정\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai284/imports.py", "loc": {"type": "anchor", "before": "\\A"}, "action": "insert_after", "once": true, "pre": {"must_not_contain": ["import numpy as np"]}, "code": "import numpy as np\n"}]}}
{"instruction": "Propose a minimal JSON plan for code changes. Output PLAN JSON only.", "input": "[INTENT]\n학습 안정화(AMP+훈련 설정)\n\n[CONTEXT]\n<<<FILE examples/ai734/train_utils.py>>>\ndef get_dtype():\n    return torch.float16\n\n<<<END>>>\n<<<FILE examples/ai734/training_args.py>>>\nargs = TrainingArguments(output_dir='out', per_device_train_batch_size=1, num_train_epochs=1)\n\n<<<END>>>", "output": {"files": [{"path": "examples/ai734/train_utils.py", "reason": "dtype가 고정 float16", "strategy": "regex", "tests": ["bf16 또는 fp16 반환"]}, {"path": "examples/ai734/training_args.py", "reason": "로깅/평가/GA 설정 누락", "strategy": "regex", "tests": ["eval_strategy=='steps'"]}], "notes": "bf16 지원 시 우선"}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n체크포인트 재개 지원\n\n[CONTEXT]\n<<<FILE examples/ai222/trainer_resume.py>>>\nfrom transformers import Trainer\ntrainer = Trainer(... )\n# BUG: 재시작 미지원\ntrainer.train()\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai222/trainer_resume.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["resume_from_checkpoint"]}, "code": "from transformers import Trainer\ntrainer = Trainer(... )\ntrainer.train(resume_from_checkpoint=True)\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nGPU bf16 지원 시 dtype 선택 개선\n\n[CONTEXT]\n<<<FILE examples/ai197/train_utils.py>>>\nimport torch\n\n# BUG: dtype가 고정되어 혼합정밀에 불리\ndef get_dtype():\n    return torch.float16\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai197/train_utils.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["torch.bfloat16"]}, "code": "import torch\n\n\ndef get_dtype():\n    \"\"\"GPU가 bf16 지원 시 bfloat16, 아니면 float16 반환\"\"\"\n    if torch.cuda.is_available() and torch.cuda.is_bf16_supported():\n        return torch.bfloat16\n    return torch.float16\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n필수 import 추가\n\n[CONTEXT]\n<<<FILE examples/ai190/imports.py>>>\n# BUG: numpy 미사용이지만 추후 사용 예정\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai190/imports.py", "loc": {"type": "anchor", "before": "\\A"}, "action": "insert_after", "once": true, "pre": {"must_not_contain": ["import numpy as np"]}, "code": "import numpy as np\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nAMP 적용 + bf16 우선\n\n[CONTEXT]\n<<<FILE examples/ai549/train_step.py>>>\nimport torch\n\n# BUG: autocast 미사용\ndef train_step(model, batch):\n    x, y = batch\n    out = model(x)\n    loss = out.loss if hasattr(out, 'loss') else out.mean()\n    loss.backward()\n    return loss.item()\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai549/train_step.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["autocast"]}, "code": "import torch\n\n@torch.no_grad()\ndef _bf16_supported():\n    return torch.cuda.is_available() and torch.cuda.is_bf16_supported()\n\n\ndef train_step(model, batch):\n    x, y = batch\n    amp_dtype = torch.bfloat16 if _bf16_supported() else torch.float16\n    scaler = torch.cuda.amp.GradScaler(enabled=True)\n    with torch.cuda.amp.autocast(enabled=True, dtype=amp_dtype):\n        out = model(x)\n        loss = out.loss if hasattr(out, 'loss') else out.mean()\n    scaler.scale(loss).backward()\n    scaler.step(torch.optim.Adam(model.parameters(), lr=1e-4))\n    scaler.update()\n    model.zero_grad(set_to_none=True)\n    return float(loss.detach().item())\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nGPU bf16 지원 시 dtype 선택 개선\n\n[CONTEXT]\n<<<FILE examples/ai123/train_utils.py>>>\nimport torch\n\n# BUG: dtype가 고정되어 혼합정밀에 불리\ndef get_dtype():\n    return torch.float16\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai123/train_utils.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["torch.bfloat16"]}, "code": "import torch\n\n\ndef get_dtype():\n    \"\"\"GPU가 bf16 지원 시 bfloat16, 아니면 float16 반환\"\"\"\n    if torch.cuda.is_available() and torch.cuda.is_bf16_supported():\n        return torch.bfloat16\n    return torch.float16\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nWindows에서 DataLoader 프리즈 방지 + pin_memory/persistent_workers\n\n[CONTEXT]\n<<<FILE examples/ai324/dataloader_main.py>>>\nimport torch, torch.utils.data as tud\n\nclass Dummy(torch.utils.data.Dataset):\n    def __len__(self): return 8\n    def __getitem__(self, i): return i\n\n# BUG: 모듈 임포트만으로 워커가 스폰되어 Windows에서 프리즈\nloader = tud.DataLoader(Dummy(), batch_size=2, num_workers=4)\nfor b in loader:\n    pass\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai324/dataloader_main.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["if __name__ == \"__main__\":"]}, "code": "import torch, torch.utils.data as tud\n\nclass Dummy(torch.utils.data.Dataset):\n    def __len__(self): return 8\n    def __getitem__(self, i): return i\n\n\ndef main():\n    loader = tud.DataLoader(Dummy(), batch_size=2, num_workers=4, pin_memory=True, persistent_workers=True)\n    for _ in loader:\n        pass\n\n\nif __name__ == \"__main__\":\n    main()\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n조기 종료 콜백 추가\n\n[CONTEXT]\n<<<FILE examples/ai198/callbacks.py>>>\nfrom transformers import TrainerCallback\n\nclass MyCB(TrainerCallback):\n    pass\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai198/callbacks.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["EarlyStoppingCallback"]}, "code": "from transformers import TrainerCallback, EarlyStoppingCallback\n\nclass MyCB(TrainerCallback):\n    pass\n\nCALLBACKS = [EarlyStoppingCallback(early_stopping_patience=2)]\n"}]}}
{"instruction": "Propose a minimal JSON plan for code changes. Output PLAN JSON only.", "input": "[INTENT]\n입출력/런타임 안전성 개선\n\n[CONTEXT]\n<<<FILE examples/ai763/dataloader_main.py>>>\nloader = DataLoader(...); for b in loader: pass\n\n<<<END>>>\n<<<FILE examples/ai763/ddp_main.py>>>\ntrainer()\n\n<<<END>>>", "output": {"files": [{"path": "examples/ai763/dataloader_main.py", "reason": "Windows freeze 가능", "strategy": "regex", "tests": ["__main__ 가드"]}, {"path": "examples/ai763/ddp_main.py", "reason": "DDP 가드 없음", "strategy": "regex", "tests": ["__main__"]}], "notes": "spawn 보호"}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n조기 종료 콜백 추가\n\n[CONTEXT]\n<<<FILE examples/ai237/callbacks.py>>>\nfrom transformers import TrainerCallback\n\nclass MyCB(TrainerCallback):\n    pass\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai237/callbacks.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["EarlyStoppingCallback"]}, "code": "from transformers import TrainerCallback, EarlyStoppingCallback\n\nclass MyCB(TrainerCallback):\n    pass\n\nCALLBACKS = [EarlyStoppingCallback(early_stopping_patience=2)]\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n조기 종료 콜백 추가\n\n[CONTEXT]\n<<<FILE examples/ai258/callbacks.py>>>\nfrom transformers import TrainerCallback\n\nclass MyCB(TrainerCallback):\n    pass\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai258/callbacks.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["EarlyStoppingCallback"]}, "code": "from transformers import TrainerCallback, EarlyStoppingCallback\n\nclass MyCB(TrainerCallback):\n    pass\n\nCALLBACKS = [EarlyStoppingCallback(early_stopping_patience=2)]\n"}]}}
{"instruction": "Propose a minimal JSON plan for code changes. Output PLAN JSON only.", "input": "[INTENT]\n입출력/런타임 안전성 개선\n\n[CONTEXT]\n<<<FILE examples/ai720/dataloader_main.py>>>\nloader = DataLoader(...); for b in loader: pass\n\n<<<END>>>\n<<<FILE examples/ai720/ddp_main.py>>>\ntrainer()\n\n<<<END>>>", "output": {"files": [{"path": "examples/ai720/dataloader_main.py", "reason": "Windows freeze 가능", "strategy": "regex", "tests": ["__main__ 가드"]}, {"path": "examples/ai720/ddp_main.py", "reason": "DDP 가드 없음", "strategy": "regex", "tests": ["__main__"]}], "notes": "spawn 보호"}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nLoRA target_modules 확장\n\n[CONTEXT]\n<<<FILE examples/ai278/lora_cfg.py>>>\nfrom peft import LoraConfig\n\nLC = LoraConfig(r=8, target_modules=[\n    \"q_proj\", \"v_proj\"  # BUG: 부족\n])\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai278/lora_cfg.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["k_proj", "o_proj"]}, "code": "from peft import LoraConfig\n\nLC = LoraConfig(r=8, target_modules=[\n    \"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"\n])\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nAMP 적용 + bf16 우선\n\n[CONTEXT]\n<<<FILE examples/ai465/train_step.py>>>\nimport torch\n\n# BUG: autocast 미사용\ndef train_step(model, batch):\n    x, y = batch\n    out = model(x)\n    loss = out.loss if hasattr(out, 'loss') else out.mean()\n    loss.backward()\n    return loss.item()\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai465/train_step.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["autocast"]}, "code": "import torch\n\n@torch.no_grad()\ndef _bf16_supported():\n    return torch.cuda.is_available() and torch.cuda.is_bf16_supported()\n\n\ndef train_step(model, batch):\n    x, y = batch\n    amp_dtype = torch.bfloat16 if _bf16_supported() else torch.float16\n    scaler = torch.cuda.amp.GradScaler(enabled=True)\n    with torch.cuda.amp.autocast(enabled=True, dtype=amp_dtype):\n        out = model(x)\n        loss = out.loss if hasattr(out, 'loss') else out.mean()\n    scaler.scale(loss).backward()\n    scaler.step(torch.optim.Adam(model.parameters(), lr=1e-4))\n    scaler.update()\n    model.zero_grad(set_to_none=True)\n    return float(loss.detach().item())\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n재현성: 시드 함수 추가\n\n[CONTEXT]\n<<<FILE examples/ai361/seed_utils.py>>>\n# BUG: 시드 미설정\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai361/seed_utils.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["set_seed("]}, "code": "import os, random, numpy as np, torch\n\ndef set_seed(seed: int = 42):\n    random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n훈련 안정화: GA steps/로깅/평가/체크포인팅/스케줄러\n\n[CONTEXT]\n<<<FILE examples/ai339/training_args.py>>>\nfrom transformers import TrainingArguments\n\nargs = TrainingArguments(\n    output_dir=\"out\",\n    per_device_train_batch_size=1,\n    num_train_epochs=1,\n)\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai339/training_args.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["gradient_accumulation_steps"]}, "code": "from transformers import TrainingArguments\n\nargs = TrainingArguments(\n    output_dir=\"out\",\n    per_device_train_batch_size=1,\n    gradient_accumulation_steps=8,\n    num_train_epochs=1,\n    logging_steps=10,\n    eval_strategy=\"steps\",\n    eval_steps=50,\n    save_steps=50,\n    lr_scheduler_type=\"cosine\",\n    warmup_ratio=0.03,\n    fp16=False, bf16=True,\n    gradient_checkpointing=True,\n    max_grad_norm=1.0,\n)\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n조기 종료 콜백 추가\n\n[CONTEXT]\n<<<FILE examples/ai564/callbacks.py>>>\nfrom transformers import TrainerCallback\n\nclass MyCB(TrainerCallback):\n    pass\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai564/callbacks.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["EarlyStoppingCallback"]}, "code": "from transformers import TrainerCallback, EarlyStoppingCallback\n\nclass MyCB(TrainerCallback):\n    pass\n\nCALLBACKS = [EarlyStoppingCallback(early_stopping_patience=2)]\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nflash 미지원 시 sdpa 안전 경로\n\n[CONTEXT]\n<<<FILE examples/ai436/model_load.py>>>\nfrom transformers import AutoModelForCausalLM\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"Qwen/Qwen3-8B\",\n)\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai436/model_load.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["attn_implementation"]}, "code": "from transformers import AutoModelForCausalLM\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"Qwen/Qwen3-8B\",\n    attn_implementation=\"sdpa\",\n)\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nAMP 적용 + bf16 우선\n\n[CONTEXT]\n<<<FILE examples/ai353/train_step.py>>>\nimport torch\n\n# BUG: autocast 미사용\ndef train_step(model, batch):\n    x, y = batch\n    out = model(x)\n    loss = out.loss if hasattr(out, 'loss') else out.mean()\n    loss.backward()\n    return loss.item()\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai353/train_step.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["autocast"]}, "code": "import torch\n\n@torch.no_grad()\ndef _bf16_supported():\n    return torch.cuda.is_available() and torch.cuda.is_bf16_supported()\n\n\ndef train_step(model, batch):\n    x, y = batch\n    amp_dtype = torch.bfloat16 if _bf16_supported() else torch.float16\n    scaler = torch.cuda.amp.GradScaler(enabled=True)\n    with torch.cuda.amp.autocast(enabled=True, dtype=amp_dtype):\n        out = model(x)\n        loss = out.loss if hasattr(out, 'loss') else out.mean()\n    scaler.scale(loss).backward()\n    scaler.step(torch.optim.Adam(model.parameters(), lr=1e-4))\n    scaler.update()\n    model.zero_grad(set_to_none=True)\n    return float(loss.detach().item())\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nLoRA target_modules 확장\n\n[CONTEXT]\n<<<FILE examples/ai544/lora_cfg.py>>>\nfrom peft import LoraConfig\n\nLC = LoraConfig(r=8, target_modules=[\n    \"q_proj\", \"v_proj\"  # BUG: 부족\n])\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai544/lora_cfg.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["k_proj", "o_proj"]}, "code": "from peft import LoraConfig\n\nLC = LoraConfig(r=8, target_modules=[\n    \"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"\n])\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n재현성: 시드 함수 추가\n\n[CONTEXT]\n<<<FILE examples/ai567/seed_utils.py>>>\n# BUG: 시드 미설정\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai567/seed_utils.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["set_seed("]}, "code": "import os, random, numpy as np, torch\n\ndef set_seed(seed: int = 42):\n    random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n훈련 안정화: GA steps/로깅/평가/체크포인팅/스케줄러\n\n[CONTEXT]\n<<<FILE examples/ai485/training_args.py>>>\nfrom transformers import TrainingArguments\n\nargs = TrainingArguments(\n    output_dir=\"out\",\n    per_device_train_batch_size=1,\n    num_train_epochs=1,\n)\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai485/training_args.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["gradient_accumulation_steps"]}, "code": "from transformers import TrainingArguments\n\nargs = TrainingArguments(\n    output_dir=\"out\",\n    per_device_train_batch_size=1,\n    gradient_accumulation_steps=8,\n    num_train_epochs=1,\n    logging_steps=10,\n    eval_strategy=\"steps\",\n    eval_steps=50,\n    save_steps=50,\n    lr_scheduler_type=\"cosine\",\n    warmup_ratio=0.03,\n    fp16=False, bf16=True,\n    gradient_checkpointing=True,\n    max_grad_norm=1.0,\n)\n"}]}}
{"instruction": "Propose a minimal JSON plan for code changes. Output PLAN JSON only.", "input": "[INTENT]\n학습 안정화(AMP+훈련 설정)\n\n[CONTEXT]\n<<<FILE examples/ai792/train_utils.py>>>\ndef get_dtype():\n    return torch.float16\n\n<<<END>>>\n<<<FILE examples/ai792/training_args.py>>>\nargs = TrainingArguments(output_dir='out', per_device_train_batch_size=1, num_train_epochs=1)\n\n<<<END>>>", "output": {"files": [{"path": "examples/ai792/train_utils.py", "reason": "dtype가 고정 float16", "strategy": "regex", "tests": ["bf16 또는 fp16 반환"]}, {"path": "examples/ai792/training_args.py", "reason": "로깅/평가/GA 설정 누락", "strategy": "regex", "tests": ["eval_strategy=='steps'"]}], "notes": "bf16 지원 시 우선"}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n필수 import 추가\n\n[CONTEXT]\n<<<FILE examples/ai596/imports.py>>>\n# BUG: numpy 미사용이지만 추후 사용 예정\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai596/imports.py", "loc": {"type": "anchor", "before": "\\A"}, "action": "insert_after", "once": true, "pre": {"must_not_contain": ["import numpy as np"]}, "code": "import numpy as np\n"}]}}
{"instruction": "Propose a minimal JSON plan for code changes. Output PLAN JSON only.", "input": "[INTENT]\n학습 안정화(AMP+훈련 설정)\n\n[CONTEXT]\n<<<FILE examples/ai680/train_utils.py>>>\ndef get_dtype():\n    return torch.float16\n\n<<<END>>>\n<<<FILE examples/ai680/training_args.py>>>\nargs = TrainingArguments(output_dir='out', per_device_train_batch_size=1, num_train_epochs=1)\n\n<<<END>>>", "output": {"files": [{"path": "examples/ai680/train_utils.py", "reason": "dtype가 고정 float16", "strategy": "regex", "tests": ["bf16 또는 fp16 반환"]}, {"path": "examples/ai680/training_args.py", "reason": "로깅/평가/GA 설정 누락", "strategy": "regex", "tests": ["eval_strategy=='steps'"]}], "notes": "bf16 지원 시 우선"}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\npad 토큰 ID 정확히 설정\n\n[CONTEXT]\n<<<FILE examples/ai314/label_cfg.py>>>\nfrom transformers import AutoTokenizer\nfrom dataclasses import dataclass\n\n@dataclass\nclass Cfg:\n    pad_id: int = -100  # BUG\n\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai314/label_cfg.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["CFG = Cfg"]}, "code": "from transformers import AutoTokenizer\nfrom dataclasses import dataclass\n\n@dataclass\nclass Cfg:\n    pad_id: int | None = None\n\n# tokenizer 로드 후 설정\ntok = AutoTokenizer.from_pretrained(\"Qwen/Qwen3-8B\", use_fast=True)\nCFG = Cfg(pad_id=tok.pad_token_id)\n"}]}}
{"instruction": "Propose a minimal JSON plan for code changes. Output PLAN JSON only.", "input": "[INTENT]\n입출력/런타임 안전성 개선\n\n[CONTEXT]\n<<<FILE examples/ai724/dataloader_main.py>>>\nloader = DataLoader(...); for b in loader: pass\n\n<<<END>>>\n<<<FILE examples/ai724/ddp_main.py>>>\ntrainer()\n\n<<<END>>>", "output": {"files": [{"path": "examples/ai724/dataloader_main.py", "reason": "Windows freeze 가능", "strategy": "regex", "tests": ["__main__ 가드"]}, {"path": "examples/ai724/ddp_main.py", "reason": "DDP 가드 없음", "strategy": "regex", "tests": ["__main__"]}], "notes": "spawn 보호"}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n재현성: 시드 함수 추가\n\n[CONTEXT]\n<<<FILE examples/ai415/seed_utils.py>>>\n# BUG: 시드 미설정\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai415/seed_utils.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["set_seed("]}, "code": "import os, random, numpy as np, torch\n\ndef set_seed(seed: int = 42):\n    random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\npad 토큰 ID 정확히 설정\n\n[CONTEXT]\n<<<FILE examples/ai227/label_cfg.py>>>\nfrom transformers import AutoTokenizer\nfrom dataclasses import dataclass\n\n@dataclass\nclass Cfg:\n    pad_id: int = -100  # BUG\n\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai227/label_cfg.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["CFG = Cfg"]}, "code": "from transformers import AutoTokenizer\nfrom dataclasses import dataclass\n\n@dataclass\nclass Cfg:\n    pad_id: int | None = None\n\n# tokenizer 로드 후 설정\ntok = AutoTokenizer.from_pretrained(\"Qwen/Qwen3-8B\", use_fast=True)\nCFG = Cfg(pad_id=tok.pad_token_id)\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n체크포인트 재개 지원\n\n[CONTEXT]\n<<<FILE examples/ai302/trainer_resume.py>>>\nfrom transformers import Trainer\ntrainer = Trainer(... )\n# BUG: 재시작 미지원\ntrainer.train()\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai302/trainer_resume.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["resume_from_checkpoint"]}, "code": "from transformers import Trainer\ntrainer = Trainer(... )\ntrainer.train(resume_from_checkpoint=True)\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\npad 토큰 ID 정확히 설정\n\n[CONTEXT]\n<<<FILE examples/ai96/label_cfg.py>>>\nfrom transformers import AutoTokenizer\nfrom dataclasses import dataclass\n\n@dataclass\nclass Cfg:\n    pad_id: int = -100  # BUG\n\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai96/label_cfg.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["CFG = Cfg"]}, "code": "from transformers import AutoTokenizer\nfrom dataclasses import dataclass\n\n@dataclass\nclass Cfg:\n    pad_id: int | None = None\n\n# tokenizer 로드 후 설정\ntok = AutoTokenizer.from_pretrained(\"Qwen/Qwen3-8B\", use_fast=True)\nCFG = Cfg(pad_id=tok.pad_token_id)\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nGPU bf16 지원 시 dtype 선택 개선\n\n[CONTEXT]\n<<<FILE examples/ai18/train_utils.py>>>\nimport torch\n\n# BUG: dtype가 고정되어 혼합정밀에 불리\ndef get_dtype():\n    return torch.float16\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai18/train_utils.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["torch.bfloat16"]}, "code": "import torch\n\n\ndef get_dtype():\n    \"\"\"GPU가 bf16 지원 시 bfloat16, 아니면 float16 반환\"\"\"\n    if torch.cuda.is_available() and torch.cuda.is_bf16_supported():\n        return torch.bfloat16\n    return torch.float16\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nLoRA target_modules 확장\n\n[CONTEXT]\n<<<FILE examples/ai459/lora_cfg.py>>>\nfrom peft import LoraConfig\n\nLC = LoraConfig(r=8, target_modules=[\n    \"q_proj\", \"v_proj\"  # BUG: 부족\n])\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai459/lora_cfg.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["k_proj", "o_proj"]}, "code": "from peft import LoraConfig\n\nLC = LoraConfig(r=8, target_modules=[\n    \"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"\n])\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n필수 import 추가\n\n[CONTEXT]\n<<<FILE examples/ai503/imports.py>>>\n# BUG: numpy 미사용이지만 추후 사용 예정\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai503/imports.py", "loc": {"type": "anchor", "before": "\\A"}, "action": "insert_after", "once": true, "pre": {"must_not_contain": ["import numpy as np"]}, "code": "import numpy as np\n"}]}}
{"instruction": "Propose a minimal JSON plan for code changes. Output PLAN JSON only.", "input": "[INTENT]\n입출력/런타임 안전성 개선\n\n[CONTEXT]\n<<<FILE examples/ai662/dataloader_main.py>>>\nloader = DataLoader(...); for b in loader: pass\n\n<<<END>>>\n<<<FILE examples/ai662/ddp_main.py>>>\ntrainer()\n\n<<<END>>>", "output": {"files": [{"path": "examples/ai662/dataloader_main.py", "reason": "Windows freeze 가능", "strategy": "regex", "tests": ["__main__ 가드"]}, {"path": "examples/ai662/ddp_main.py", "reason": "DDP 가드 없음", "strategy": "regex", "tests": ["__main__"]}], "notes": "spawn 보호"}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n체크포인트 재개 지원\n\n[CONTEXT]\n<<<FILE examples/ai488/trainer_resume.py>>>\nfrom transformers import Trainer\ntrainer = Trainer(... )\n# BUG: 재시작 미지원\ntrainer.train()\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai488/trainer_resume.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["resume_from_checkpoint"]}, "code": "from transformers import Trainer\ntrainer = Trainer(... )\ntrainer.train(resume_from_checkpoint=True)\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n학습 후 어댑터 저장 추가\n\n[CONTEXT]\n<<<FILE examples/ai247/save_after_train.py>>>\nfrom peft import PeftModel\n# ... 학습 후\n# BUG: 저장 누락\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai247/save_after_train.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["save_pretrained"]}, "code": "from peft import PeftModel\n# ... 학습 후\nmodel.save_pretrained(\"training/qlora-out/export\")\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n필수 import 추가\n\n[CONTEXT]\n<<<FILE examples/ai569/imports.py>>>\n# BUG: numpy 미사용이지만 추후 사용 예정\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai569/imports.py", "loc": {"type": "anchor", "before": "\\A"}, "action": "insert_after", "once": true, "pre": {"must_not_contain": ["import numpy as np"]}, "code": "import numpy as np\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nGPU bf16 지원 시 dtype 선택 개선\n\n[CONTEXT]\n<<<FILE examples/ai501/train_utils.py>>>\nimport torch\n\n# BUG: dtype가 고정되어 혼합정밀에 불리\ndef get_dtype():\n    return torch.float16\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai501/train_utils.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["torch.bfloat16"]}, "code": "import torch\n\n\ndef get_dtype():\n    \"\"\"GPU가 bf16 지원 시 bfloat16, 아니면 float16 반환\"\"\"\n    if torch.cuda.is_available() and torch.cuda.is_bf16_supported():\n        return torch.bfloat16\n    return torch.float16\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n체크포인트 재개 지원\n\n[CONTEXT]\n<<<FILE examples/ai417/trainer_resume.py>>>\nfrom transformers import Trainer\ntrainer = Trainer(... )\n# BUG: 재시작 미지원\ntrainer.train()\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai417/trainer_resume.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["resume_from_checkpoint"]}, "code": "from transformers import Trainer\ntrainer = Trainer(... )\ntrainer.train(resume_from_checkpoint=True)\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nAMP 적용 + bf16 우선\n\n[CONTEXT]\n<<<FILE examples/ai248/train_step.py>>>\nimport torch\n\n# BUG: autocast 미사용\ndef train_step(model, batch):\n    x, y = batch\n    out = model(x)\n    loss = out.loss if hasattr(out, 'loss') else out.mean()\n    loss.backward()\n    return loss.item()\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai248/train_step.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["autocast"]}, "code": "import torch\n\n@torch.no_grad()\ndef _bf16_supported():\n    return torch.cuda.is_available() and torch.cuda.is_bf16_supported()\n\n\ndef train_step(model, batch):\n    x, y = batch\n    amp_dtype = torch.bfloat16 if _bf16_supported() else torch.float16\n    scaler = torch.cuda.amp.GradScaler(enabled=True)\n    with torch.cuda.amp.autocast(enabled=True, dtype=amp_dtype):\n        out = model(x)\n        loss = out.loss if hasattr(out, 'loss') else out.mean()\n    scaler.scale(loss).backward()\n    scaler.step(torch.optim.Adam(model.parameters(), lr=1e-4))\n    scaler.update()\n    model.zero_grad(set_to_none=True)\n    return float(loss.detach().item())\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n재현성: 시드 함수 추가\n\n[CONTEXT]\n<<<FILE examples/ai88/seed_utils.py>>>\n# BUG: 시드 미설정\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai88/seed_utils.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["set_seed("]}, "code": "import os, random, numpy as np, torch\n\ndef set_seed(seed: int = 42):\n    random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n학습 후 어댑터 저장 추가\n\n[CONTEXT]\n<<<FILE examples/ai196/save_after_train.py>>>\nfrom peft import PeftModel\n# ... 학습 후\n# BUG: 저장 누락\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai196/save_after_train.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["save_pretrained"]}, "code": "from peft import PeftModel\n# ... 학습 후\nmodel.save_pretrained(\"training/qlora-out/export\")\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n체크포인트 재개 지원\n\n[CONTEXT]\n<<<FILE examples/ai530/trainer_resume.py>>>\nfrom transformers import Trainer\ntrainer = Trainer(... )\n# BUG: 재시작 미지원\ntrainer.train()\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai530/trainer_resume.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["resume_from_checkpoint"]}, "code": "from transformers import Trainer\ntrainer = Trainer(... )\ntrainer.train(resume_from_checkpoint=True)\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n필수 import 추가\n\n[CONTEXT]\n<<<FILE examples/ai11/imports.py>>>\n# BUG: numpy 미사용이지만 추후 사용 예정\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai11/imports.py", "loc": {"type": "anchor", "before": "\\A"}, "action": "insert_after", "once": true, "pre": {"must_not_contain": ["import numpy as np"]}, "code": "import numpy as np\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n8bit 옵티마이저 + 코사인 스케줄러\n\n[CONTEXT]\n<<<FILE examples/ai570/optim.py>>>\nimport torch\nopt = torch.optim.Adam([], lr=3e-4)\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai570/optim.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["PagedAdamW8bit"]}, "code": "from transformers import get_scheduler\nfrom bitsandbytes.optim import PagedAdamW8bit\nopt = PagedAdamW8bit([], lr=2e-4)\nsched = get_scheduler(\"cosine\", optimizer=opt, num_warmup_steps=100, num_training_steps=1000)\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n조기 종료 콜백 추가\n\n[CONTEXT]\n<<<FILE examples/ai283/callbacks.py>>>\nfrom transformers import TrainerCallback\n\nclass MyCB(TrainerCallback):\n    pass\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai283/callbacks.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["EarlyStoppingCallback"]}, "code": "from transformers import TrainerCallback, EarlyStoppingCallback\n\nclass MyCB(TrainerCallback):\n    pass\n\nCALLBACKS = [EarlyStoppingCallback(early_stopping_patience=2)]\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nAMP 적용 + bf16 우선\n\n[CONTEXT]\n<<<FILE examples/ai611/train_step.py>>>\nimport torch\n\n# BUG: autocast 미사용\ndef train_step(model, batch):\n    x, y = batch\n    out = model(x)\n    loss = out.loss if hasattr(out, 'loss') else out.mean()\n    loss.backward()\n    return loss.item()\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai611/train_step.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["autocast"]}, "code": "import torch\n\n@torch.no_grad()\ndef _bf16_supported():\n    return torch.cuda.is_available() and torch.cuda.is_bf16_supported()\n\n\ndef train_step(model, batch):\n    x, y = batch\n    amp_dtype = torch.bfloat16 if _bf16_supported() else torch.float16\n    scaler = torch.cuda.amp.GradScaler(enabled=True)\n    with torch.cuda.amp.autocast(enabled=True, dtype=amp_dtype):\n        out = model(x)\n        loss = out.loss if hasattr(out, 'loss') else out.mean()\n    scaler.scale(loss).backward()\n    scaler.step(torch.optim.Adam(model.parameters(), lr=1e-4))\n    scaler.update()\n    model.zero_grad(set_to_none=True)\n    return float(loss.detach().item())\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nWindows에서 DataLoader 프리즈 방지 + pin_memory/persistent_workers\n\n[CONTEXT]\n<<<FILE examples/ai287/dataloader_main.py>>>\nimport torch, torch.utils.data as tud\n\nclass Dummy(torch.utils.data.Dataset):\n    def __len__(self): return 8\n    def __getitem__(self, i): return i\n\n# BUG: 모듈 임포트만으로 워커가 스폰되어 Windows에서 프리즈\nloader = tud.DataLoader(Dummy(), batch_size=2, num_workers=4)\nfor b in loader:\n    pass\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai287/dataloader_main.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["if __name__ == \"__main__\":"]}, "code": "import torch, torch.utils.data as tud\n\nclass Dummy(torch.utils.data.Dataset):\n    def __len__(self): return 8\n    def __getitem__(self, i): return i\n\n\ndef main():\n    loader = tud.DataLoader(Dummy(), batch_size=2, num_workers=4, pin_memory=True, persistent_workers=True)\n    for _ in loader:\n        pass\n\n\nif __name__ == \"__main__\":\n    main()\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nAMP 적용 + bf16 우선\n\n[CONTEXT]\n<<<FILE examples/ai349/train_step.py>>>\nimport torch\n\n# BUG: autocast 미사용\ndef train_step(model, batch):\n    x, y = batch\n    out = model(x)\n    loss = out.loss if hasattr(out, 'loss') else out.mean()\n    loss.backward()\n    return loss.item()\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai349/train_step.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["autocast"]}, "code": "import torch\n\n@torch.no_grad()\ndef _bf16_supported():\n    return torch.cuda.is_available() and torch.cuda.is_bf16_supported()\n\n\ndef train_step(model, batch):\n    x, y = batch\n    amp_dtype = torch.bfloat16 if _bf16_supported() else torch.float16\n    scaler = torch.cuda.amp.GradScaler(enabled=True)\n    with torch.cuda.amp.autocast(enabled=True, dtype=amp_dtype):\n        out = model(x)\n        loss = out.loss if hasattr(out, 'loss') else out.mean()\n    scaler.scale(loss).backward()\n    scaler.step(torch.optim.Adam(model.parameters(), lr=1e-4))\n    scaler.update()\n    model.zero_grad(set_to_none=True)\n    return float(loss.detach().item())\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n필수 import 추가\n\n[CONTEXT]\n<<<FILE examples/ai265/imports.py>>>\n# BUG: numpy 미사용이지만 추후 사용 예정\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai265/imports.py", "loc": {"type": "anchor", "before": "\\A"}, "action": "insert_after", "once": true, "pre": {"must_not_contain": ["import numpy as np"]}, "code": "import numpy as np\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nLoRA target_modules 확장\n\n[CONTEXT]\n<<<FILE examples/ai364/lora_cfg.py>>>\nfrom peft import LoraConfig\n\nLC = LoraConfig(r=8, target_modules=[\n    \"q_proj\", \"v_proj\"  # BUG: 부족\n])\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai364/lora_cfg.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["k_proj", "o_proj"]}, "code": "from peft import LoraConfig\n\nLC = LoraConfig(r=8, target_modules=[\n    \"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"\n])\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nAMP 적용 + bf16 우선\n\n[CONTEXT]\n<<<FILE examples/ai218/train_step.py>>>\nimport torch\n\n# BUG: autocast 미사용\ndef train_step(model, batch):\n    x, y = batch\n    out = model(x)\n    loss = out.loss if hasattr(out, 'loss') else out.mean()\n    loss.backward()\n    return loss.item()\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai218/train_step.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["autocast"]}, "code": "import torch\n\n@torch.no_grad()\ndef _bf16_supported():\n    return torch.cuda.is_available() and torch.cuda.is_bf16_supported()\n\n\ndef train_step(model, batch):\n    x, y = batch\n    amp_dtype = torch.bfloat16 if _bf16_supported() else torch.float16\n    scaler = torch.cuda.amp.GradScaler(enabled=True)\n    with torch.cuda.amp.autocast(enabled=True, dtype=amp_dtype):\n        out = model(x)\n        loss = out.loss if hasattr(out, 'loss') else out.mean()\n    scaler.scale(loss).backward()\n    scaler.step(torch.optim.Adam(model.parameters(), lr=1e-4))\n    scaler.update()\n    model.zero_grad(set_to_none=True)\n    return float(loss.detach().item())\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n8bit 옵티마이저 + 코사인 스케줄러\n\n[CONTEXT]\n<<<FILE examples/ai500/optim.py>>>\nimport torch\nopt = torch.optim.Adam([], lr=3e-4)\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai500/optim.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["PagedAdamW8bit"]}, "code": "from transformers import get_scheduler\nfrom bitsandbytes.optim import PagedAdamW8bit\nopt = PagedAdamW8bit([], lr=2e-4)\nsched = get_scheduler(\"cosine\", optimizer=opt, num_warmup_steps=100, num_training_steps=1000)\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nDDP/torchrun 안전 가드\n\n[CONTEXT]\n<<<FILE examples/ai187/ddp_main.py>>>\nimport torch\n\n# BUG: torchrun에서 main 가드 없음\ntrainer()\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai187/ddp_main.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["__main__"]}, "code": "import torch\n\n\ndef main():\n    trainer()\n\n\nif __name__ == \"__main__\":\n    main()\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nLoRA target_modules 확장\n\n[CONTEXT]\n<<<FILE examples/ai333/lora_cfg.py>>>\nfrom peft import LoraConfig\n\nLC = LoraConfig(r=8, target_modules=[\n    \"q_proj\", \"v_proj\"  # BUG: 부족\n])\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai333/lora_cfg.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["k_proj", "o_proj"]}, "code": "from peft import LoraConfig\n\nLC = LoraConfig(r=8, target_modules=[\n    \"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"\n])\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nGPU bf16 지원 시 dtype 선택 개선\n\n[CONTEXT]\n<<<FILE examples/ai507/train_utils.py>>>\nimport torch\n\n# BUG: dtype가 고정되어 혼합정밀에 불리\ndef get_dtype():\n    return torch.float16\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai507/train_utils.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["torch.bfloat16"]}, "code": "import torch\n\n\ndef get_dtype():\n    \"\"\"GPU가 bf16 지원 시 bfloat16, 아니면 float16 반환\"\"\"\n    if torch.cuda.is_available() and torch.cuda.is_bf16_supported():\n        return torch.bfloat16\n    return torch.float16\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nAMP 적용 + bf16 우선\n\n[CONTEXT]\n<<<FILE examples/ai30/train_step.py>>>\nimport torch\n\n# BUG: autocast 미사용\ndef train_step(model, batch):\n    x, y = batch\n    out = model(x)\n    loss = out.loss if hasattr(out, 'loss') else out.mean()\n    loss.backward()\n    return loss.item()\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai30/train_step.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["autocast"]}, "code": "import torch\n\n@torch.no_grad()\ndef _bf16_supported():\n    return torch.cuda.is_available() and torch.cuda.is_bf16_supported()\n\n\ndef train_step(model, batch):\n    x, y = batch\n    amp_dtype = torch.bfloat16 if _bf16_supported() else torch.float16\n    scaler = torch.cuda.amp.GradScaler(enabled=True)\n    with torch.cuda.amp.autocast(enabled=True, dtype=amp_dtype):\n        out = model(x)\n        loss = out.loss if hasattr(out, 'loss') else out.mean()\n    scaler.scale(loss).backward()\n    scaler.step(torch.optim.Adam(model.parameters(), lr=1e-4))\n    scaler.update()\n    model.zero_grad(set_to_none=True)\n    return float(loss.detach().item())\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n학습 후 어댑터 저장 추가\n\n[CONTEXT]\n<<<FILE examples/ai345/save_after_train.py>>>\nfrom peft import PeftModel\n# ... 학습 후\n# BUG: 저장 누락\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai345/save_after_train.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["save_pretrained"]}, "code": "from peft import PeftModel\n# ... 학습 후\nmodel.save_pretrained(\"training/qlora-out/export\")\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nGPU bf16 지원 시 dtype 선택 개선\n\n[CONTEXT]\n<<<FILE examples/ai389/train_utils.py>>>\nimport torch\n\n# BUG: dtype가 고정되어 혼합정밀에 불리\ndef get_dtype():\n    return torch.float16\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai389/train_utils.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["torch.bfloat16"]}, "code": "import torch\n\n\ndef get_dtype():\n    \"\"\"GPU가 bf16 지원 시 bfloat16, 아니면 float16 반환\"\"\"\n    if torch.cuda.is_available() and torch.cuda.is_bf16_supported():\n        return torch.bfloat16\n    return torch.float16\n"}]}}
{"instruction": "Propose a minimal JSON plan for code changes. Output PLAN JSON only.", "input": "[INTENT]\n입출력/런타임 안전성 개선\n\n[CONTEXT]\n<<<FILE examples/ai685/dataloader_main.py>>>\nloader = DataLoader(...); for b in loader: pass\n\n<<<END>>>\n<<<FILE examples/ai685/ddp_main.py>>>\ntrainer()\n\n<<<END>>>", "output": {"files": [{"path": "examples/ai685/dataloader_main.py", "reason": "Windows freeze 가능", "strategy": "regex", "tests": ["__main__ 가드"]}, {"path": "examples/ai685/ddp_main.py", "reason": "DDP 가드 없음", "strategy": "regex", "tests": ["__main__"]}], "notes": "spawn 보호"}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nDDP/torchrun 안전 가드\n\n[CONTEXT]\n<<<FILE examples/ai625/ddp_main.py>>>\nimport torch\n\n# BUG: torchrun에서 main 가드 없음\ntrainer()\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai625/ddp_main.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["__main__"]}, "code": "import torch\n\n\ndef main():\n    trainer()\n\n\nif __name__ == \"__main__\":\n    main()\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nLoRA target_modules 확장\n\n[CONTEXT]\n<<<FILE examples/ai584/lora_cfg.py>>>\nfrom peft import LoraConfig\n\nLC = LoraConfig(r=8, target_modules=[\n    \"q_proj\", \"v_proj\"  # BUG: 부족\n])\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai584/lora_cfg.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["k_proj", "o_proj"]}, "code": "from peft import LoraConfig\n\nLC = LoraConfig(r=8, target_modules=[\n    \"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"\n])\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nflash 미지원 시 sdpa 안전 경로\n\n[CONTEXT]\n<<<FILE examples/ai476/model_load.py>>>\nfrom transformers import AutoModelForCausalLM\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"Qwen/Qwen3-8B\",\n)\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai476/model_load.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["attn_implementation"]}, "code": "from transformers import AutoModelForCausalLM\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"Qwen/Qwen3-8B\",\n    attn_implementation=\"sdpa\",\n)\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nflash 미지원 시 sdpa 안전 경로\n\n[CONTEXT]\n<<<FILE examples/ai250/model_load.py>>>\nfrom transformers import AutoModelForCausalLM\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"Qwen/Qwen3-8B\",\n)\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai250/model_load.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["attn_implementation"]}, "code": "from transformers import AutoModelForCausalLM\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"Qwen/Qwen3-8B\",\n    attn_implementation=\"sdpa\",\n)\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n훈련 안정화: GA steps/로깅/평가/체크포인팅/스케줄러\n\n[CONTEXT]\n<<<FILE examples/ai462/training_args.py>>>\nfrom transformers import TrainingArguments\n\nargs = TrainingArguments(\n    output_dir=\"out\",\n    per_device_train_batch_size=1,\n    num_train_epochs=1,\n)\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai462/training_args.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["gradient_accumulation_steps"]}, "code": "from transformers import TrainingArguments\n\nargs = TrainingArguments(\n    output_dir=\"out\",\n    per_device_train_batch_size=1,\n    gradient_accumulation_steps=8,\n    num_train_epochs=1,\n    logging_steps=10,\n    eval_strategy=\"steps\",\n    eval_steps=50,\n    save_steps=50,\n    lr_scheduler_type=\"cosine\",\n    warmup_ratio=0.03,\n    fp16=False, bf16=True,\n    gradient_checkpointing=True,\n    max_grad_norm=1.0,\n)\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nLoRA target_modules 확장\n\n[CONTEXT]\n<<<FILE examples/ai286/lora_cfg.py>>>\nfrom peft import LoraConfig\n\nLC = LoraConfig(r=8, target_modules=[\n    \"q_proj\", \"v_proj\"  # BUG: 부족\n])\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai286/lora_cfg.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["k_proj", "o_proj"]}, "code": "from peft import LoraConfig\n\nLC = LoraConfig(r=8, target_modules=[\n    \"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"\n])\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nflash 미지원 시 sdpa 안전 경로\n\n[CONTEXT]\n<<<FILE examples/ai89/model_load.py>>>\nfrom transformers import AutoModelForCausalLM\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"Qwen/Qwen3-8B\",\n)\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai89/model_load.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["attn_implementation"]}, "code": "from transformers import AutoModelForCausalLM\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"Qwen/Qwen3-8B\",\n    attn_implementation=\"sdpa\",\n)\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n8bit 옵티마이저 + 코사인 스케줄러\n\n[CONTEXT]\n<<<FILE examples/ai252/optim.py>>>\nimport torch\nopt = torch.optim.Adam([], lr=3e-4)\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai252/optim.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["PagedAdamW8bit"]}, "code": "from transformers import get_scheduler\nfrom bitsandbytes.optim import PagedAdamW8bit\nopt = PagedAdamW8bit([], lr=2e-4)\nsched = get_scheduler(\"cosine\", optimizer=opt, num_warmup_steps=100, num_training_steps=1000)\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nWindows에서 DataLoader 프리즈 방지 + pin_memory/persistent_workers\n\n[CONTEXT]\n<<<FILE examples/ai331/dataloader_main.py>>>\nimport torch, torch.utils.data as tud\n\nclass Dummy(torch.utils.data.Dataset):\n    def __len__(self): return 8\n    def __getitem__(self, i): return i\n\n# BUG: 모듈 임포트만으로 워커가 스폰되어 Windows에서 프리즈\nloader = tud.DataLoader(Dummy(), batch_size=2, num_workers=4)\nfor b in loader:\n    pass\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai331/dataloader_main.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["if __name__ == \"__main__\":"]}, "code": "import torch, torch.utils.data as tud\n\nclass Dummy(torch.utils.data.Dataset):\n    def __len__(self): return 8\n    def __getitem__(self, i): return i\n\n\ndef main():\n    loader = tud.DataLoader(Dummy(), batch_size=2, num_workers=4, pin_memory=True, persistent_workers=True)\n    for _ in loader:\n        pass\n\n\nif __name__ == \"__main__\":\n    main()\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nGPU bf16 지원 시 dtype 선택 개선\n\n[CONTEXT]\n<<<FILE examples/ai275/train_utils.py>>>\nimport torch\n\n# BUG: dtype가 고정되어 혼합정밀에 불리\ndef get_dtype():\n    return torch.float16\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai275/train_utils.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["torch.bfloat16"]}, "code": "import torch\n\n\ndef get_dtype():\n    \"\"\"GPU가 bf16 지원 시 bfloat16, 아니면 float16 반환\"\"\"\n    if torch.cuda.is_available() and torch.cuda.is_bf16_supported():\n        return torch.bfloat16\n    return torch.float16\n"}]}}
{"instruction": "Propose a minimal JSON plan for code changes. Output PLAN JSON only.", "input": "[INTENT]\n학습 안정화(AMP+훈련 설정)\n\n[CONTEXT]\n<<<FILE examples/ai772/train_utils.py>>>\ndef get_dtype():\n    return torch.float16\n\n<<<END>>>\n<<<FILE examples/ai772/training_args.py>>>\nargs = TrainingArguments(output_dir='out', per_device_train_batch_size=1, num_train_epochs=1)\n\n<<<END>>>", "output": {"files": [{"path": "examples/ai772/train_utils.py", "reason": "dtype가 고정 float16", "strategy": "regex", "tests": ["bf16 또는 fp16 반환"]}, {"path": "examples/ai772/training_args.py", "reason": "로깅/평가/GA 설정 누락", "strategy": "regex", "tests": ["eval_strategy=='steps'"]}], "notes": "bf16 지원 시 우선"}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n학습 후 어댑터 저장 추가\n\n[CONTEXT]\n<<<FILE examples/ai447/save_after_train.py>>>\nfrom peft import PeftModel\n# ... 학습 후\n# BUG: 저장 누락\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai447/save_after_train.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["save_pretrained"]}, "code": "from peft import PeftModel\n# ... 학습 후\nmodel.save_pretrained(\"training/qlora-out/export\")\n"}]}}
{"instruction": "Propose a minimal JSON plan for code changes. Output PLAN JSON only.", "input": "[INTENT]\n학습 안정화(AMP+훈련 설정)\n\n[CONTEXT]\n<<<FILE examples/ai796/train_utils.py>>>\ndef get_dtype():\n    return torch.float16\n\n<<<END>>>\n<<<FILE examples/ai796/training_args.py>>>\nargs = TrainingArguments(output_dir='out', per_device_train_batch_size=1, num_train_epochs=1)\n\n<<<END>>>", "output": {"files": [{"path": "examples/ai796/train_utils.py", "reason": "dtype가 고정 float16", "strategy": "regex", "tests": ["bf16 또는 fp16 반환"]}, {"path": "examples/ai796/training_args.py", "reason": "로깅/평가/GA 설정 누락", "strategy": "regex", "tests": ["eval_strategy=='steps'"]}], "notes": "bf16 지원 시 우선"}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nflash 미지원 시 sdpa 안전 경로\n\n[CONTEXT]\n<<<FILE examples/ai496/model_load.py>>>\nfrom transformers import AutoModelForCausalLM\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"Qwen/Qwen3-8B\",\n)\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai496/model_load.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["attn_implementation"]}, "code": "from transformers import AutoModelForCausalLM\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"Qwen/Qwen3-8B\",\n    attn_implementation=\"sdpa\",\n)\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n체크포인트 재개 지원\n\n[CONTEXT]\n<<<FILE examples/ai552/trainer_resume.py>>>\nfrom transformers import Trainer\ntrainer = Trainer(... )\n# BUG: 재시작 미지원\ntrainer.train()\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai552/trainer_resume.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["resume_from_checkpoint"]}, "code": "from transformers import Trainer\ntrainer = Trainer(... )\ntrainer.train(resume_from_checkpoint=True)\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n재현성: 시드 함수 추가\n\n[CONTEXT]\n<<<FILE examples/ai631/seed_utils.py>>>\n# BUG: 시드 미설정\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai631/seed_utils.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["set_seed("]}, "code": "import os, random, numpy as np, torch\n\ndef set_seed(seed: int = 42):\n    random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nAMP 적용 + bf16 우선\n\n[CONTEXT]\n<<<FILE examples/ai277/train_step.py>>>\nimport torch\n\n# BUG: autocast 미사용\ndef train_step(model, batch):\n    x, y = batch\n    out = model(x)\n    loss = out.loss if hasattr(out, 'loss') else out.mean()\n    loss.backward()\n    return loss.item()\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai277/train_step.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["autocast"]}, "code": "import torch\n\n@torch.no_grad()\ndef _bf16_supported():\n    return torch.cuda.is_available() and torch.cuda.is_bf16_supported()\n\n\ndef train_step(model, batch):\n    x, y = batch\n    amp_dtype = torch.bfloat16 if _bf16_supported() else torch.float16\n    scaler = torch.cuda.amp.GradScaler(enabled=True)\n    with torch.cuda.amp.autocast(enabled=True, dtype=amp_dtype):\n        out = model(x)\n        loss = out.loss if hasattr(out, 'loss') else out.mean()\n    scaler.scale(loss).backward()\n    scaler.step(torch.optim.Adam(model.parameters(), lr=1e-4))\n    scaler.update()\n    model.zero_grad(set_to_none=True)\n    return float(loss.detach().item())\n"}]}}
{"instruction": "Propose a minimal JSON plan for code changes. Output PLAN JSON only.", "input": "[INTENT]\n입출력/런타임 안전성 개선\n\n[CONTEXT]\n<<<FILE examples/ai667/dataloader_main.py>>>\nloader = DataLoader(...); for b in loader: pass\n\n<<<END>>>\n<<<FILE examples/ai667/ddp_main.py>>>\ntrainer()\n\n<<<END>>>", "output": {"files": [{"path": "examples/ai667/dataloader_main.py", "reason": "Windows freeze 가능", "strategy": "regex", "tests": ["__main__ 가드"]}, {"path": "examples/ai667/ddp_main.py", "reason": "DDP 가드 없음", "strategy": "regex", "tests": ["__main__"]}], "notes": "spawn 보호"}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nGPU bf16 지원 시 dtype 선택 개선\n\n[CONTEXT]\n<<<FILE examples/ai535/train_utils.py>>>\nimport torch\n\n# BUG: dtype가 고정되어 혼합정밀에 불리\ndef get_dtype():\n    return torch.float16\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai535/train_utils.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["torch.bfloat16"]}, "code": "import torch\n\n\ndef get_dtype():\n    \"\"\"GPU가 bf16 지원 시 bfloat16, 아니면 float16 반환\"\"\"\n    if torch.cuda.is_available() and torch.cuda.is_bf16_supported():\n        return torch.bfloat16\n    return torch.float16\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nGPU bf16 지원 시 dtype 선택 개선\n\n[CONTEXT]\n<<<FILE examples/ai181/train_utils.py>>>\nimport torch\n\n# BUG: dtype가 고정되어 혼합정밀에 불리\ndef get_dtype():\n    return torch.float16\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai181/train_utils.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["torch.bfloat16"]}, "code": "import torch\n\n\ndef get_dtype():\n    \"\"\"GPU가 bf16 지원 시 bfloat16, 아니면 float16 반환\"\"\"\n    if torch.cuda.is_available() and torch.cuda.is_bf16_supported():\n        return torch.bfloat16\n    return torch.float16\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nGPU bf16 지원 시 dtype 선택 개선\n\n[CONTEXT]\n<<<FILE examples/ai446/train_utils.py>>>\nimport torch\n\n# BUG: dtype가 고정되어 혼합정밀에 불리\ndef get_dtype():\n    return torch.float16\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai446/train_utils.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["torch.bfloat16"]}, "code": "import torch\n\n\ndef get_dtype():\n    \"\"\"GPU가 bf16 지원 시 bfloat16, 아니면 float16 반환\"\"\"\n    if torch.cuda.is_available() and torch.cuda.is_bf16_supported():\n        return torch.bfloat16\n    return torch.float16\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n훈련 안정화: GA steps/로깅/평가/체크포인팅/스케줄러\n\n[CONTEXT]\n<<<FILE examples/ai157/training_args.py>>>\nfrom transformers import TrainingArguments\n\nargs = TrainingArguments(\n    output_dir=\"out\",\n    per_device_train_batch_size=1,\n    num_train_epochs=1,\n)\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai157/training_args.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["gradient_accumulation_steps"]}, "code": "from transformers import TrainingArguments\n\nargs = TrainingArguments(\n    output_dir=\"out\",\n    per_device_train_batch_size=1,\n    gradient_accumulation_steps=8,\n    num_train_epochs=1,\n    logging_steps=10,\n    eval_strategy=\"steps\",\n    eval_steps=50,\n    save_steps=50,\n    lr_scheduler_type=\"cosine\",\n    warmup_ratio=0.03,\n    fp16=False, bf16=True,\n    gradient_checkpointing=True,\n    max_grad_norm=1.0,\n)\n"}]}}
{"instruction": "Propose a minimal JSON plan for code changes. Output PLAN JSON only.", "input": "[INTENT]\n입출력/런타임 안전성 개선\n\n[CONTEXT]\n<<<FILE examples/ai725/dataloader_main.py>>>\nloader = DataLoader(...); for b in loader: pass\n\n<<<END>>>\n<<<FILE examples/ai725/ddp_main.py>>>\ntrainer()\n\n<<<END>>>", "output": {"files": [{"path": "examples/ai725/dataloader_main.py", "reason": "Windows freeze 가능", "strategy": "regex", "tests": ["__main__ 가드"]}, {"path": "examples/ai725/ddp_main.py", "reason": "DDP 가드 없음", "strategy": "regex", "tests": ["__main__"]}], "notes": "spawn 보호"}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nAMP 적용 + bf16 우선\n\n[CONTEXT]\n<<<FILE examples/ai471/train_step.py>>>\nimport torch\n\n# BUG: autocast 미사용\ndef train_step(model, batch):\n    x, y = batch\n    out = model(x)\n    loss = out.loss if hasattr(out, 'loss') else out.mean()\n    loss.backward()\n    return loss.item()\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai471/train_step.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["autocast"]}, "code": "import torch\n\n@torch.no_grad()\ndef _bf16_supported():\n    return torch.cuda.is_available() and torch.cuda.is_bf16_supported()\n\n\ndef train_step(model, batch):\n    x, y = batch\n    amp_dtype = torch.bfloat16 if _bf16_supported() else torch.float16\n    scaler = torch.cuda.amp.GradScaler(enabled=True)\n    with torch.cuda.amp.autocast(enabled=True, dtype=amp_dtype):\n        out = model(x)\n        loss = out.loss if hasattr(out, 'loss') else out.mean()\n    scaler.scale(loss).backward()\n    scaler.step(torch.optim.Adam(model.parameters(), lr=1e-4))\n    scaler.update()\n    model.zero_grad(set_to_none=True)\n    return float(loss.detach().item())\n"}]}}
{"instruction": "Propose a minimal JSON plan for code changes. Output PLAN JSON only.", "input": "[INTENT]\n입출력/런타임 안전성 개선\n\n[CONTEXT]\n<<<FILE examples/ai651/dataloader_main.py>>>\nloader = DataLoader(...); for b in loader: pass\n\n<<<END>>>\n<<<FILE examples/ai651/ddp_main.py>>>\ntrainer()\n\n<<<END>>>", "output": {"files": [{"path": "examples/ai651/dataloader_main.py", "reason": "Windows freeze 가능", "strategy": "regex", "tests": ["__main__ 가드"]}, {"path": "examples/ai651/ddp_main.py", "reason": "DDP 가드 없음", "strategy": "regex", "tests": ["__main__"]}], "notes": "spawn 보호"}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n재현성: 시드 함수 추가\n\n[CONTEXT]\n<<<FILE examples/ai371/seed_utils.py>>>\n# BUG: 시드 미설정\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai371/seed_utils.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["set_seed("]}, "code": "import os, random, numpy as np, torch\n\ndef set_seed(seed: int = 42):\n    random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nDDP/torchrun 안전 가드\n\n[CONTEXT]\n<<<FILE examples/ai444/ddp_main.py>>>\nimport torch\n\n# BUG: torchrun에서 main 가드 없음\ntrainer()\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai444/ddp_main.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["__main__"]}, "code": "import torch\n\n\ndef main():\n    trainer()\n\n\nif __name__ == \"__main__\":\n    main()\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nLoRA target_modules 확장\n\n[CONTEXT]\n<<<FILE examples/ai109/lora_cfg.py>>>\nfrom peft import LoraConfig\n\nLC = LoraConfig(r=8, target_modules=[\n    \"q_proj\", \"v_proj\"  # BUG: 부족\n])\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai109/lora_cfg.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["k_proj", "o_proj"]}, "code": "from peft import LoraConfig\n\nLC = LoraConfig(r=8, target_modules=[\n    \"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"\n])\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nWindows에서 DataLoader 프리즈 방지 + pin_memory/persistent_workers\n\n[CONTEXT]\n<<<FILE examples/ai502/dataloader_main.py>>>\nimport torch, torch.utils.data as tud\n\nclass Dummy(torch.utils.data.Dataset):\n    def __len__(self): return 8\n    def __getitem__(self, i): return i\n\n# BUG: 모듈 임포트만으로 워커가 스폰되어 Windows에서 프리즈\nloader = tud.DataLoader(Dummy(), batch_size=2, num_workers=4)\nfor b in loader:\n    pass\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai502/dataloader_main.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["if __name__ == \"__main__\":"]}, "code": "import torch, torch.utils.data as tud\n\nclass Dummy(torch.utils.data.Dataset):\n    def __len__(self): return 8\n    def __getitem__(self, i): return i\n\n\ndef main():\n    loader = tud.DataLoader(Dummy(), batch_size=2, num_workers=4, pin_memory=True, persistent_workers=True)\n    for _ in loader:\n        pass\n\n\nif __name__ == \"__main__\":\n    main()\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nLoRA target_modules 확장\n\n[CONTEXT]\n<<<FILE examples/ai431/lora_cfg.py>>>\nfrom peft import LoraConfig\n\nLC = LoraConfig(r=8, target_modules=[\n    \"q_proj\", \"v_proj\"  # BUG: 부족\n])\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai431/lora_cfg.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["k_proj", "o_proj"]}, "code": "from peft import LoraConfig\n\nLC = LoraConfig(r=8, target_modules=[\n    \"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"\n])\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nAMP 적용 + bf16 우선\n\n[CONTEXT]\n<<<FILE examples/ai13/train_step.py>>>\nimport torch\n\n# BUG: autocast 미사용\ndef train_step(model, batch):\n    x, y = batch\n    out = model(x)\n    loss = out.loss if hasattr(out, 'loss') else out.mean()\n    loss.backward()\n    return loss.item()\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai13/train_step.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["autocast"]}, "code": "import torch\n\n@torch.no_grad()\ndef _bf16_supported():\n    return torch.cuda.is_available() and torch.cuda.is_bf16_supported()\n\n\ndef train_step(model, batch):\n    x, y = batch\n    amp_dtype = torch.bfloat16 if _bf16_supported() else torch.float16\n    scaler = torch.cuda.amp.GradScaler(enabled=True)\n    with torch.cuda.amp.autocast(enabled=True, dtype=amp_dtype):\n        out = model(x)\n        loss = out.loss if hasattr(out, 'loss') else out.mean()\n    scaler.scale(loss).backward()\n    scaler.step(torch.optim.Adam(model.parameters(), lr=1e-4))\n    scaler.update()\n    model.zero_grad(set_to_none=True)\n    return float(loss.detach().item())\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nWindows에서 DataLoader 프리즈 방지 + pin_memory/persistent_workers\n\n[CONTEXT]\n<<<FILE examples/ai350/dataloader_main.py>>>\nimport torch, torch.utils.data as tud\n\nclass Dummy(torch.utils.data.Dataset):\n    def __len__(self): return 8\n    def __getitem__(self, i): return i\n\n# BUG: 모듈 임포트만으로 워커가 스폰되어 Windows에서 프리즈\nloader = tud.DataLoader(Dummy(), batch_size=2, num_workers=4)\nfor b in loader:\n    pass\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai350/dataloader_main.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["if __name__ == \"__main__\":"]}, "code": "import torch, torch.utils.data as tud\n\nclass Dummy(torch.utils.data.Dataset):\n    def __len__(self): return 8\n    def __getitem__(self, i): return i\n\n\ndef main():\n    loader = tud.DataLoader(Dummy(), batch_size=2, num_workers=4, pin_memory=True, persistent_workers=True)\n    for _ in loader:\n        pass\n\n\nif __name__ == \"__main__\":\n    main()\n"}]}}
{"instruction": "Propose a minimal JSON plan for code changes. Output PLAN JSON only.", "input": "[INTENT]\n학습 안정화(AMP+훈련 설정)\n\n[CONTEXT]\n<<<FILE examples/ai664/train_utils.py>>>\ndef get_dtype():\n    return torch.float16\n\n<<<END>>>\n<<<FILE examples/ai664/training_args.py>>>\nargs = TrainingArguments(output_dir='out', per_device_train_batch_size=1, num_train_epochs=1)\n\n<<<END>>>", "output": {"files": [{"path": "examples/ai664/train_utils.py", "reason": "dtype가 고정 float16", "strategy": "regex", "tests": ["bf16 또는 fp16 반환"]}, {"path": "examples/ai664/training_args.py", "reason": "로깅/평가/GA 설정 누락", "strategy": "regex", "tests": ["eval_strategy=='steps'"]}], "notes": "bf16 지원 시 우선"}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n재현성: 시드 함수 추가\n\n[CONTEXT]\n<<<FILE examples/ai519/seed_utils.py>>>\n# BUG: 시드 미설정\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai519/seed_utils.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["set_seed("]}, "code": "import os, random, numpy as np, torch\n\ndef set_seed(seed: int = 42):\n    random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n체크포인트 재개 지원\n\n[CONTEXT]\n<<<FILE examples/ai71/trainer_resume.py>>>\nfrom transformers import Trainer\ntrainer = Trainer(... )\n# BUG: 재시작 미지원\ntrainer.train()\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai71/trainer_resume.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["resume_from_checkpoint"]}, "code": "from transformers import Trainer\ntrainer = Trainer(... )\ntrainer.train(resume_from_checkpoint=True)\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nDDP/torchrun 안전 가드\n\n[CONTEXT]\n<<<FILE examples/ai380/ddp_main.py>>>\nimport torch\n\n# BUG: torchrun에서 main 가드 없음\ntrainer()\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai380/ddp_main.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["__main__"]}, "code": "import torch\n\n\ndef main():\n    trainer()\n\n\nif __name__ == \"__main__\":\n    main()\n"}]}}
{"instruction": "Propose a minimal JSON plan for code changes. Output PLAN JSON only.", "input": "[INTENT]\n입출력/런타임 안전성 개선\n\n[CONTEXT]\n<<<FILE examples/ai678/dataloader_main.py>>>\nloader = DataLoader(...); for b in loader: pass\n\n<<<END>>>\n<<<FILE examples/ai678/ddp_main.py>>>\ntrainer()\n\n<<<END>>>", "output": {"files": [{"path": "examples/ai678/dataloader_main.py", "reason": "Windows freeze 가능", "strategy": "regex", "tests": ["__main__ 가드"]}, {"path": "examples/ai678/ddp_main.py", "reason": "DDP 가드 없음", "strategy": "regex", "tests": ["__main__"]}], "notes": "spawn 보호"}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n필수 import 추가\n\n[CONTEXT]\n<<<FILE examples/ai439/imports.py>>>\n# BUG: numpy 미사용이지만 추후 사용 예정\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai439/imports.py", "loc": {"type": "anchor", "before": "\\A"}, "action": "insert_after", "once": true, "pre": {"must_not_contain": ["import numpy as np"]}, "code": "import numpy as np\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nWindows에서 DataLoader 프리즈 방지 + pin_memory/persistent_workers\n\n[CONTEXT]\n<<<FILE examples/ai546/dataloader_main.py>>>\nimport torch, torch.utils.data as tud\n\nclass Dummy(torch.utils.data.Dataset):\n    def __len__(self): return 8\n    def __getitem__(self, i): return i\n\n# BUG: 모듈 임포트만으로 워커가 스폰되어 Windows에서 프리즈\nloader = tud.DataLoader(Dummy(), batch_size=2, num_workers=4)\nfor b in loader:\n    pass\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai546/dataloader_main.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["if __name__ == \"__main__\":"]}, "code": "import torch, torch.utils.data as tud\n\nclass Dummy(torch.utils.data.Dataset):\n    def __len__(self): return 8\n    def __getitem__(self, i): return i\n\n\ndef main():\n    loader = tud.DataLoader(Dummy(), batch_size=2, num_workers=4, pin_memory=True, persistent_workers=True)\n    for _ in loader:\n        pass\n\n\nif __name__ == \"__main__\":\n    main()\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nWindows에서 DataLoader 프리즈 방지 + pin_memory/persistent_workers\n\n[CONTEXT]\n<<<FILE examples/ai356/dataloader_main.py>>>\nimport torch, torch.utils.data as tud\n\nclass Dummy(torch.utils.data.Dataset):\n    def __len__(self): return 8\n    def __getitem__(self, i): return i\n\n# BUG: 모듈 임포트만으로 워커가 스폰되어 Windows에서 프리즈\nloader = tud.DataLoader(Dummy(), batch_size=2, num_workers=4)\nfor b in loader:\n    pass\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai356/dataloader_main.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["if __name__ == \"__main__\":"]}, "code": "import torch, torch.utils.data as tud\n\nclass Dummy(torch.utils.data.Dataset):\n    def __len__(self): return 8\n    def __getitem__(self, i): return i\n\n\ndef main():\n    loader = tud.DataLoader(Dummy(), batch_size=2, num_workers=4, pin_memory=True, persistent_workers=True)\n    for _ in loader:\n        pass\n\n\nif __name__ == \"__main__\":\n    main()\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n재현성: 시드 함수 추가\n\n[CONTEXT]\n<<<FILE examples/ai43/seed_utils.py>>>\n# BUG: 시드 미설정\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai43/seed_utils.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["set_seed("]}, "code": "import os, random, numpy as np, torch\n\ndef set_seed(seed: int = 42):\n    random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n8bit 옵티마이저 + 코사인 스케줄러\n\n[CONTEXT]\n<<<FILE examples/ai580/optim.py>>>\nimport torch\nopt = torch.optim.Adam([], lr=3e-4)\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai580/optim.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["PagedAdamW8bit"]}, "code": "from transformers import get_scheduler\nfrom bitsandbytes.optim import PagedAdamW8bit\nopt = PagedAdamW8bit([], lr=2e-4)\nsched = get_scheduler(\"cosine\", optimizer=opt, num_warmup_steps=100, num_training_steps=1000)\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nLoRA target_modules 확장\n\n[CONTEXT]\n<<<FILE examples/ai124/lora_cfg.py>>>\nfrom peft import LoraConfig\n\nLC = LoraConfig(r=8, target_modules=[\n    \"q_proj\", \"v_proj\"  # BUG: 부족\n])\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai124/lora_cfg.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["k_proj", "o_proj"]}, "code": "from peft import LoraConfig\n\nLC = LoraConfig(r=8, target_modules=[\n    \"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"\n])\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nflash 미지원 시 sdpa 안전 경로\n\n[CONTEXT]\n<<<FILE examples/ai615/model_load.py>>>\nfrom transformers import AutoModelForCausalLM\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"Qwen/Qwen3-8B\",\n)\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai615/model_load.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["attn_implementation"]}, "code": "from transformers import AutoModelForCausalLM\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"Qwen/Qwen3-8B\",\n    attn_implementation=\"sdpa\",\n)\n"}]}}
{"instruction": "Propose a minimal JSON plan for code changes. Output PLAN JSON only.", "input": "[INTENT]\n학습 안정화(AMP+훈련 설정)\n\n[CONTEXT]\n<<<FILE examples/ai701/train_utils.py>>>\ndef get_dtype():\n    return torch.float16\n\n<<<END>>>\n<<<FILE examples/ai701/training_args.py>>>\nargs = TrainingArguments(output_dir='out', per_device_train_batch_size=1, num_train_epochs=1)\n\n<<<END>>>", "output": {"files": [{"path": "examples/ai701/train_utils.py", "reason": "dtype가 고정 float16", "strategy": "regex", "tests": ["bf16 또는 fp16 반환"]}, {"path": "examples/ai701/training_args.py", "reason": "로깅/평가/GA 설정 누락", "strategy": "regex", "tests": ["eval_strategy=='steps'"]}], "notes": "bf16 지원 시 우선"}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nLoRA target_modules 확장\n\n[CONTEXT]\n<<<FILE examples/ai309/lora_cfg.py>>>\nfrom peft import LoraConfig\n\nLC = LoraConfig(r=8, target_modules=[\n    \"q_proj\", \"v_proj\"  # BUG: 부족\n])\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai309/lora_cfg.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["k_proj", "o_proj"]}, "code": "from peft import LoraConfig\n\nLC = LoraConfig(r=8, target_modules=[\n    \"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"\n])\n"}]}}
{"instruction": "Propose a minimal JSON plan for code changes. Output PLAN JSON only.", "input": "[INTENT]\n입출력/런타임 안전성 개선\n\n[CONTEXT]\n<<<FILE examples/ai714/dataloader_main.py>>>\nloader = DataLoader(...); for b in loader: pass\n\n<<<END>>>\n<<<FILE examples/ai714/ddp_main.py>>>\ntrainer()\n\n<<<END>>>", "output": {"files": [{"path": "examples/ai714/dataloader_main.py", "reason": "Windows freeze 가능", "strategy": "regex", "tests": ["__main__ 가드"]}, {"path": "examples/ai714/ddp_main.py", "reason": "DDP 가드 없음", "strategy": "regex", "tests": ["__main__"]}], "notes": "spawn 보호"}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n필수 import 추가\n\n[CONTEXT]\n<<<FILE examples/ai105/imports.py>>>\n# BUG: numpy 미사용이지만 추후 사용 예정\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai105/imports.py", "loc": {"type": "anchor", "before": "\\A"}, "action": "insert_after", "once": true, "pre": {"must_not_contain": ["import numpy as np"]}, "code": "import numpy as np\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n훈련 안정화: GA steps/로깅/평가/체크포인팅/스케줄러\n\n[CONTEXT]\n<<<FILE examples/ai249/training_args.py>>>\nfrom transformers import TrainingArguments\n\nargs = TrainingArguments(\n    output_dir=\"out\",\n    per_device_train_batch_size=1,\n    num_train_epochs=1,\n)\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai249/training_args.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["gradient_accumulation_steps"]}, "code": "from transformers import TrainingArguments\n\nargs = TrainingArguments(\n    output_dir=\"out\",\n    per_device_train_batch_size=1,\n    gradient_accumulation_steps=8,\n    num_train_epochs=1,\n    logging_steps=10,\n    eval_strategy=\"steps\",\n    eval_steps=50,\n    save_steps=50,\n    lr_scheduler_type=\"cosine\",\n    warmup_ratio=0.03,\n    fp16=False, bf16=True,\n    gradient_checkpointing=True,\n    max_grad_norm=1.0,\n)\n"}]}}
{"instruction": "Propose a minimal JSON plan for code changes. Output PLAN JSON only.", "input": "[INTENT]\n입출력/런타임 안전성 개선\n\n[CONTEXT]\n<<<FILE examples/ai643/dataloader_main.py>>>\nloader = DataLoader(...); for b in loader: pass\n\n<<<END>>>\n<<<FILE examples/ai643/ddp_main.py>>>\ntrainer()\n\n<<<END>>>", "output": {"files": [{"path": "examples/ai643/dataloader_main.py", "reason": "Windows freeze 가능", "strategy": "regex", "tests": ["__main__ 가드"]}, {"path": "examples/ai643/ddp_main.py", "reason": "DDP 가드 없음", "strategy": "regex", "tests": ["__main__"]}], "notes": "spawn 보호"}}
{"instruction": "Propose a minimal JSON plan for code changes. Output PLAN JSON only.", "input": "[INTENT]\n입출력/런타임 안전성 개선\n\n[CONTEXT]\n<<<FILE examples/ai708/dataloader_main.py>>>\nloader = DataLoader(...); for b in loader: pass\n\n<<<END>>>\n<<<FILE examples/ai708/ddp_main.py>>>\ntrainer()\n\n<<<END>>>", "output": {"files": [{"path": "examples/ai708/dataloader_main.py", "reason": "Windows freeze 가능", "strategy": "regex", "tests": ["__main__ 가드"]}, {"path": "examples/ai708/ddp_main.py", "reason": "DDP 가드 없음", "strategy": "regex", "tests": ["__main__"]}], "notes": "spawn 보호"}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nAMP 적용 + bf16 우선\n\n[CONTEXT]\n<<<FILE examples/ai607/train_step.py>>>\nimport torch\n\n# BUG: autocast 미사용\ndef train_step(model, batch):\n    x, y = batch\n    out = model(x)\n    loss = out.loss if hasattr(out, 'loss') else out.mean()\n    loss.backward()\n    return loss.item()\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai607/train_step.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["autocast"]}, "code": "import torch\n\n@torch.no_grad()\ndef _bf16_supported():\n    return torch.cuda.is_available() and torch.cuda.is_bf16_supported()\n\n\ndef train_step(model, batch):\n    x, y = batch\n    amp_dtype = torch.bfloat16 if _bf16_supported() else torch.float16\n    scaler = torch.cuda.amp.GradScaler(enabled=True)\n    with torch.cuda.amp.autocast(enabled=True, dtype=amp_dtype):\n        out = model(x)\n        loss = out.loss if hasattr(out, 'loss') else out.mean()\n    scaler.scale(loss).backward()\n    scaler.step(torch.optim.Adam(model.parameters(), lr=1e-4))\n    scaler.update()\n    model.zero_grad(set_to_none=True)\n    return float(loss.detach().item())\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nGPU bf16 지원 시 dtype 선택 개선\n\n[CONTEXT]\n<<<FILE examples/ai78/train_utils.py>>>\nimport torch\n\n# BUG: dtype가 고정되어 혼합정밀에 불리\ndef get_dtype():\n    return torch.float16\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai78/train_utils.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["torch.bfloat16"]}, "code": "import torch\n\n\ndef get_dtype():\n    \"\"\"GPU가 bf16 지원 시 bfloat16, 아니면 float16 반환\"\"\"\n    if torch.cuda.is_available() and torch.cuda.is_bf16_supported():\n        return torch.bfloat16\n    return torch.float16\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nAMP 적용 + bf16 우선\n\n[CONTEXT]\n<<<FILE examples/ai204/train_step.py>>>\nimport torch\n\n# BUG: autocast 미사용\ndef train_step(model, batch):\n    x, y = batch\n    out = model(x)\n    loss = out.loss if hasattr(out, 'loss') else out.mean()\n    loss.backward()\n    return loss.item()\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai204/train_step.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["autocast"]}, "code": "import torch\n\n@torch.no_grad()\ndef _bf16_supported():\n    return torch.cuda.is_available() and torch.cuda.is_bf16_supported()\n\n\ndef train_step(model, batch):\n    x, y = batch\n    amp_dtype = torch.bfloat16 if _bf16_supported() else torch.float16\n    scaler = torch.cuda.amp.GradScaler(enabled=True)\n    with torch.cuda.amp.autocast(enabled=True, dtype=amp_dtype):\n        out = model(x)\n        loss = out.loss if hasattr(out, 'loss') else out.mean()\n    scaler.scale(loss).backward()\n    scaler.step(torch.optim.Adam(model.parameters(), lr=1e-4))\n    scaler.update()\n    model.zero_grad(set_to_none=True)\n    return float(loss.detach().item())\n"}]}}
{"instruction": "Propose a minimal JSON plan for code changes. Output PLAN JSON only.", "input": "[INTENT]\n학습 안정화(AMP+훈련 설정)\n\n[CONTEXT]\n<<<FILE examples/ai733/train_utils.py>>>\ndef get_dtype():\n    return torch.float16\n\n<<<END>>>\n<<<FILE examples/ai733/training_args.py>>>\nargs = TrainingArguments(output_dir='out', per_device_train_batch_size=1, num_train_epochs=1)\n\n<<<END>>>", "output": {"files": [{"path": "examples/ai733/train_utils.py", "reason": "dtype가 고정 float16", "strategy": "regex", "tests": ["bf16 또는 fp16 반환"]}, {"path": "examples/ai733/training_args.py", "reason": "로깅/평가/GA 설정 누락", "strategy": "regex", "tests": ["eval_strategy=='steps'"]}], "notes": "bf16 지원 시 우선"}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n8bit 옵티마이저 + 코사인 스케줄러\n\n[CONTEXT]\n<<<FILE examples/ai407/optim.py>>>\nimport torch\nopt = torch.optim.Adam([], lr=3e-4)\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai407/optim.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["PagedAdamW8bit"]}, "code": "from transformers import get_scheduler\nfrom bitsandbytes.optim import PagedAdamW8bit\nopt = PagedAdamW8bit([], lr=2e-4)\nsched = get_scheduler(\"cosine\", optimizer=opt, num_warmup_steps=100, num_training_steps=1000)\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n훈련 안정화: GA steps/로깅/평가/체크포인팅/스케줄러\n\n[CONTEXT]\n<<<FILE examples/ai385/training_args.py>>>\nfrom transformers import TrainingArguments\n\nargs = TrainingArguments(\n    output_dir=\"out\",\n    per_device_train_batch_size=1,\n    num_train_epochs=1,\n)\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai385/training_args.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["gradient_accumulation_steps"]}, "code": "from transformers import TrainingArguments\n\nargs = TrainingArguments(\n    output_dir=\"out\",\n    per_device_train_batch_size=1,\n    gradient_accumulation_steps=8,\n    num_train_epochs=1,\n    logging_steps=10,\n    eval_strategy=\"steps\",\n    eval_steps=50,\n    save_steps=50,\n    lr_scheduler_type=\"cosine\",\n    warmup_ratio=0.03,\n    fp16=False, bf16=True,\n    gradient_checkpointing=True,\n    max_grad_norm=1.0,\n)\n"}]}}
{"instruction": "Propose a minimal JSON plan for code changes. Output PLAN JSON only.", "input": "[INTENT]\n학습 안정화(AMP+훈련 설정)\n\n[CONTEXT]\n<<<FILE examples/ai694/train_utils.py>>>\ndef get_dtype():\n    return torch.float16\n\n<<<END>>>\n<<<FILE examples/ai694/training_args.py>>>\nargs = TrainingArguments(output_dir='out', per_device_train_batch_size=1, num_train_epochs=1)\n\n<<<END>>>", "output": {"files": [{"path": "examples/ai694/train_utils.py", "reason": "dtype가 고정 float16", "strategy": "regex", "tests": ["bf16 또는 fp16 반환"]}, {"path": "examples/ai694/training_args.py", "reason": "로깅/평가/GA 설정 누락", "strategy": "regex", "tests": ["eval_strategy=='steps'"]}], "notes": "bf16 지원 시 우선"}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n8bit 옵티마이저 + 코사인 스케줄러\n\n[CONTEXT]\n<<<FILE examples/ai590/optim.py>>>\nimport torch\nopt = torch.optim.Adam([], lr=3e-4)\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai590/optim.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["PagedAdamW8bit"]}, "code": "from transformers import get_scheduler\nfrom bitsandbytes.optim import PagedAdamW8bit\nopt = PagedAdamW8bit([], lr=2e-4)\nsched = get_scheduler(\"cosine\", optimizer=opt, num_warmup_steps=100, num_training_steps=1000)\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nGPU bf16 지원 시 dtype 선택 개선\n\n[CONTEXT]\n<<<FILE examples/ai320/train_utils.py>>>\nimport torch\n\n# BUG: dtype가 고정되어 혼합정밀에 불리\ndef get_dtype():\n    return torch.float16\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai320/train_utils.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["torch.bfloat16"]}, "code": "import torch\n\n\ndef get_dtype():\n    \"\"\"GPU가 bf16 지원 시 bfloat16, 아니면 float16 반환\"\"\"\n    if torch.cuda.is_available() and torch.cuda.is_bf16_supported():\n        return torch.bfloat16\n    return torch.float16\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n조기 종료 콜백 추가\n\n[CONTEXT]\n<<<FILE examples/ai27/callbacks.py>>>\nfrom transformers import TrainerCallback\n\nclass MyCB(TrainerCallback):\n    pass\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai27/callbacks.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["EarlyStoppingCallback"]}, "code": "from transformers import TrainerCallback, EarlyStoppingCallback\n\nclass MyCB(TrainerCallback):\n    pass\n\nCALLBACKS = [EarlyStoppingCallback(early_stopping_patience=2)]\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nLoRA target_modules 확장\n\n[CONTEXT]\n<<<FILE examples/ai593/lora_cfg.py>>>\nfrom peft import LoraConfig\n\nLC = LoraConfig(r=8, target_modules=[\n    \"q_proj\", \"v_proj\"  # BUG: 부족\n])\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai593/lora_cfg.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["k_proj", "o_proj"]}, "code": "from peft import LoraConfig\n\nLC = LoraConfig(r=8, target_modules=[\n    \"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"\n])\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n학습 후 어댑터 저장 추가\n\n[CONTEXT]\n<<<FILE examples/ai111/save_after_train.py>>>\nfrom peft import PeftModel\n# ... 학습 후\n# BUG: 저장 누락\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai111/save_after_train.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["save_pretrained"]}, "code": "from peft import PeftModel\n# ... 학습 후\nmodel.save_pretrained(\"training/qlora-out/export\")\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n조기 종료 콜백 추가\n\n[CONTEXT]\n<<<FILE examples/ai319/callbacks.py>>>\nfrom transformers import TrainerCallback\n\nclass MyCB(TrainerCallback):\n    pass\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai319/callbacks.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["EarlyStoppingCallback"]}, "code": "from transformers import TrainerCallback, EarlyStoppingCallback\n\nclass MyCB(TrainerCallback):\n    pass\n\nCALLBACKS = [EarlyStoppingCallback(early_stopping_patience=2)]\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nDDP/torchrun 안전 가드\n\n[CONTEXT]\n<<<FILE examples/ai168/ddp_main.py>>>\nimport torch\n\n# BUG: torchrun에서 main 가드 없음\ntrainer()\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai168/ddp_main.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["__main__"]}, "code": "import torch\n\n\ndef main():\n    trainer()\n\n\nif __name__ == \"__main__\":\n    main()\n"}]}}
{"instruction": "Propose a minimal JSON plan for code changes. Output PLAN JSON only.", "input": "[INTENT]\n학습 안정화(AMP+훈련 설정)\n\n[CONTEXT]\n<<<FILE examples/ai799/train_utils.py>>>\ndef get_dtype():\n    return torch.float16\n\n<<<END>>>\n<<<FILE examples/ai799/training_args.py>>>\nargs = TrainingArguments(output_dir='out', per_device_train_batch_size=1, num_train_epochs=1)\n\n<<<END>>>", "output": {"files": [{"path": "examples/ai799/train_utils.py", "reason": "dtype가 고정 float16", "strategy": "regex", "tests": ["bf16 또는 fp16 반환"]}, {"path": "examples/ai799/training_args.py", "reason": "로깅/평가/GA 설정 누락", "strategy": "regex", "tests": ["eval_strategy=='steps'"]}], "notes": "bf16 지원 시 우선"}}
{"instruction": "Propose a minimal JSON plan for code changes. Output PLAN JSON only.", "input": "[INTENT]\n입출력/런타임 안전성 개선\n\n[CONTEXT]\n<<<FILE examples/ai736/dataloader_main.py>>>\nloader = DataLoader(...); for b in loader: pass\n\n<<<END>>>\n<<<FILE examples/ai736/ddp_main.py>>>\ntrainer()\n\n<<<END>>>", "output": {"files": [{"path": "examples/ai736/dataloader_main.py", "reason": "Windows freeze 가능", "strategy": "regex", "tests": ["__main__ 가드"]}, {"path": "examples/ai736/ddp_main.py", "reason": "DDP 가드 없음", "strategy": "regex", "tests": ["__main__"]}], "notes": "spawn 보호"}}
{"instruction": "Propose a minimal JSON plan for code changes. Output PLAN JSON only.", "input": "[INTENT]\n입출력/런타임 안전성 개선\n\n[CONTEXT]\n<<<FILE examples/ai766/dataloader_main.py>>>\nloader = DataLoader(...); for b in loader: pass\n\n<<<END>>>\n<<<FILE examples/ai766/ddp_main.py>>>\ntrainer()\n\n<<<END>>>", "output": {"files": [{"path": "examples/ai766/dataloader_main.py", "reason": "Windows freeze 가능", "strategy": "regex", "tests": ["__main__ 가드"]}, {"path": "examples/ai766/ddp_main.py", "reason": "DDP 가드 없음", "strategy": "regex", "tests": ["__main__"]}], "notes": "spawn 보호"}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n재현성: 시드 함수 추가\n\n[CONTEXT]\n<<<FILE examples/ai622/seed_utils.py>>>\n# BUG: 시드 미설정\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai622/seed_utils.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["set_seed("]}, "code": "import os, random, numpy as np, torch\n\ndef set_seed(seed: int = 42):\n    random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n체크포인트 재개 지원\n\n[CONTEXT]\n<<<FILE examples/ai621/trainer_resume.py>>>\nfrom transformers import Trainer\ntrainer = Trainer(... )\n# BUG: 재시작 미지원\ntrainer.train()\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai621/trainer_resume.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["resume_from_checkpoint"]}, "code": "from transformers import Trainer\ntrainer = Trainer(... )\ntrainer.train(resume_from_checkpoint=True)\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nWindows에서 DataLoader 프리즈 방지 + pin_memory/persistent_workers\n\n[CONTEXT]\n<<<FILE examples/ai172/dataloader_main.py>>>\nimport torch, torch.utils.data as tud\n\nclass Dummy(torch.utils.data.Dataset):\n    def __len__(self): return 8\n    def __getitem__(self, i): return i\n\n# BUG: 모듈 임포트만으로 워커가 스폰되어 Windows에서 프리즈\nloader = tud.DataLoader(Dummy(), batch_size=2, num_workers=4)\nfor b in loader:\n    pass\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai172/dataloader_main.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["if __name__ == \"__main__\":"]}, "code": "import torch, torch.utils.data as tud\n\nclass Dummy(torch.utils.data.Dataset):\n    def __len__(self): return 8\n    def __getitem__(self, i): return i\n\n\ndef main():\n    loader = tud.DataLoader(Dummy(), batch_size=2, num_workers=4, pin_memory=True, persistent_workers=True)\n    for _ in loader:\n        pass\n\n\nif __name__ == \"__main__\":\n    main()\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n조기 종료 콜백 추가\n\n[CONTEXT]\n<<<FILE examples/ai378/callbacks.py>>>\nfrom transformers import TrainerCallback\n\nclass MyCB(TrainerCallback):\n    pass\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai378/callbacks.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["EarlyStoppingCallback"]}, "code": "from transformers import TrainerCallback, EarlyStoppingCallback\n\nclass MyCB(TrainerCallback):\n    pass\n\nCALLBACKS = [EarlyStoppingCallback(early_stopping_patience=2)]\n"}]}}
{"instruction": "Propose a minimal JSON plan for code changes. Output PLAN JSON only.", "input": "[INTENT]\n입출력/런타임 안전성 개선\n\n[CONTEXT]\n<<<FILE examples/ai786/dataloader_main.py>>>\nloader = DataLoader(...); for b in loader: pass\n\n<<<END>>>\n<<<FILE examples/ai786/ddp_main.py>>>\ntrainer()\n\n<<<END>>>", "output": {"files": [{"path": "examples/ai786/dataloader_main.py", "reason": "Windows freeze 가능", "strategy": "regex", "tests": ["__main__ 가드"]}, {"path": "examples/ai786/ddp_main.py", "reason": "DDP 가드 없음", "strategy": "regex", "tests": ["__main__"]}], "notes": "spawn 보호"}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n재현성: 시드 함수 추가\n\n[CONTEXT]\n<<<FILE examples/ai263/seed_utils.py>>>\n# BUG: 시드 미설정\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai263/seed_utils.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["set_seed("]}, "code": "import os, random, numpy as np, torch\n\ndef set_seed(seed: int = 42):\n    random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n"}]}}
{"instruction": "Propose a minimal JSON plan for code changes. Output PLAN JSON only.", "input": "[INTENT]\n입출력/런타임 안전성 개선\n\n[CONTEXT]\n<<<FILE examples/ai717/dataloader_main.py>>>\nloader = DataLoader(...); for b in loader: pass\n\n<<<END>>>\n<<<FILE examples/ai717/ddp_main.py>>>\ntrainer()\n\n<<<END>>>", "output": {"files": [{"path": "examples/ai717/dataloader_main.py", "reason": "Windows freeze 가능", "strategy": "regex", "tests": ["__main__ 가드"]}, {"path": "examples/ai717/ddp_main.py", "reason": "DDP 가드 없음", "strategy": "regex", "tests": ["__main__"]}], "notes": "spawn 보호"}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n필수 import 추가\n\n[CONTEXT]\n<<<FILE examples/ai477/imports.py>>>\n# BUG: numpy 미사용이지만 추후 사용 예정\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai477/imports.py", "loc": {"type": "anchor", "before": "\\A"}, "action": "insert_after", "once": true, "pre": {"must_not_contain": ["import numpy as np"]}, "code": "import numpy as np\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n학습 후 어댑터 저장 추가\n\n[CONTEXT]\n<<<FILE examples/ai224/save_after_train.py>>>\nfrom peft import PeftModel\n# ... 학습 후\n# BUG: 저장 누락\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai224/save_after_train.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["save_pretrained"]}, "code": "from peft import PeftModel\n# ... 학습 후\nmodel.save_pretrained(\"training/qlora-out/export\")\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\nWindows에서 DataLoader 프리즈 방지 + pin_memory/persistent_workers\n\n[CONTEXT]\n<<<FILE examples/ai578/dataloader_main.py>>>\nimport torch, torch.utils.data as tud\n\nclass Dummy(torch.utils.data.Dataset):\n    def __len__(self): return 8\n    def __getitem__(self, i): return i\n\n# BUG: 모듈 임포트만으로 워커가 스폰되어 Windows에서 프리즈\nloader = tud.DataLoader(Dummy(), batch_size=2, num_workers=4)\nfor b in loader:\n    pass\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai578/dataloader_main.py", "loc": {"type": "regex", "pattern": "(?s)\\\\A[\\\\s\\\\S]*\\\\Z"}, "action": "replace_range", "once": true, "pre": {"must_not_contain": ["if __name__ == \"__main__\":"]}, "code": "import torch, torch.utils.data as tud\n\nclass Dummy(torch.utils.data.Dataset):\n    def __len__(self): return 8\n    def __getitem__(self, i): return i\n\n\ndef main():\n    loader = tud.DataLoader(Dummy(), batch_size=2, num_workers=4, pin_memory=True, persistent_workers=True)\n    for _ in loader:\n        pass\n\n\nif __name__ == \"__main__\":\n    main()\n"}]}}
{"instruction": "Fix or improve ML/finetune code. Output PATCH JSON only.", "input": "[INTENT]\n필수 import 추가\n\n[CONTEXT]\n<<<FILE examples/ai118/imports.py>>>\n# BUG: numpy 미사용이지만 추후 사용 예정\n\n<<<END>>>", "output": {"version": "1", "edits": [{"path": "examples/ai118/imports.py", "loc": {"type": "anchor", "before": "\\A"}, "action": "insert_after", "once": true, "pre": {"must_not_contain": ["import numpy as np"]}, "code": "import numpy as np\n"}]}}
{"instruction": "Propose a minimal JSON plan for code changes. Output PLAN JSON only.", "input": "[INTENT]\n학습 안정화(AMP+훈련 설정)\n\n[CONTEXT]\n<<<FILE examples/ai686/train_utils.py>>>\ndef get_dtype():\n    return torch.float16\n\n<<<END>>>\n<<<FILE examples/ai686/training_args.py>>>\nargs = TrainingArguments(output_dir='out', per_device_train_batch_size=1, num_train_epochs=1)\n\n<<<END>>>", "output": {"files": [{"path": "examples/ai686/train_utils.py", "reason": "dtype가 고정 float16", "strategy": "regex", "tests": ["bf16 또는 fp16 반환"]}, {"path": "examples/ai686/training_args.py", "reason": "로깅/평가/GA 설정 누락", "strategy": "regex", "tests": ["eval_strategy=='steps'"]}], "notes": "bf16 지원 시 우선"}}
