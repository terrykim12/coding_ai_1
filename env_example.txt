# Qwen3-8B Local Coding AI 환경 설정
# 이 파일을 .env로 복사하고 실제 값으로 수정하세요

# 모델 설정 (Qwen3-8B로 변경)
MODEL_PATH=Qwen/Qwen3-8B
ADAPTER_PATH=training/qlora-out/adapter
DEVICE=cuda  # cuda 또는 cpu
TORCH_DTYPE=float16  # float16, bfloat16, float32

# CUDA 최적화 설정
CUDA_VISIBLE_DEVICES=0
CUDA_LAUNCH_BLOCKING=1
TORCH_CUDA_ARCH_LIST=8.9  # RTX 4070용 (Ada Lovelace)

# 서버 설정
HOST=127.0.0.1
PORT=8765
WORKERS=1

# 모델 생성 설정
MAX_LENGTH=2048
TEMPERATURE=0.7
TOP_P=0.9
DO_SAMPLE=True

# 컨텍스트 설정
MAX_CONTEXT_LENGTH=4096
MAX_FILES=50
MAX_TOKENS_PER_FILE=1000

# 테스트 설정
PYTEST_ARGS=-v --tb=short
COVERAGE_MIN=80

# 디버그 설정
DEBUG_PORT_START=5678
MAX_DEBUG_PROCESSES=5

# 학습 설정 (QLoRA)
LEARNING_RATE=2e-4
BATCH_SIZE=4
GRADIENT_ACCUMULATION_STEPS=4
MAX_STEPS=1000
WARMUP_STEPS=100
LOGGING_STEPS=10
SAVE_STEPS=100
EVAL_STEPS=100
SAVE_TOTAL_LIMIT=3

# LoRA 설정
LORA_R=16
LORA_ALPHA=32
LORA_DROPOUT=0.1

