# ğŸš€ CUDA ì„¤ì • ê°€ì´ë“œ - Qwen3-8B Local Coding AI

## ğŸ“‹ ì‹œìŠ¤í…œ ìš”êµ¬ì‚¬í•­

### í•˜ë“œì›¨ì–´
- **GPU**: NVIDIA GPU (RTX 4000 ì‹œë¦¬ì¦ˆ ì´ìƒ ê¶Œì¥)
- **VRAM**: ìµœì†Œ 8GB, ê¶Œì¥ 12GB ì´ìƒ
- **RAM**: ìµœì†Œ 16GB, ê¶Œì¥ 32GB ì´ìƒ

### ì†Œí”„íŠ¸ì›¨ì–´
- **OS**: Windows 10/11 (64ë¹„íŠ¸)
- **Python**: 3.8 - 3.11
- **CUDA**: 11.8 - 12.1

## ğŸ”§ CUDA ì„¤ì¹˜ ë‹¨ê³„

### 1. NVIDIA ë“œë¼ì´ë²„ ì„¤ì¹˜
```bash
# í˜„ì¬ ë“œë¼ì´ë²„ ë²„ì „ í™•ì¸
nvidia-smi

# ìµœì‹  ë“œë¼ì´ë²„ ë‹¤ìš´ë¡œë“œ (ê¶Œì¥)
# https://www.nvidia.com/Download/index.aspx
```

### 2. CUDA Toolkit ì„¤ì¹˜
```bash
# CUDA 12.1 ì„¤ì¹˜ (ê¶Œì¥)
# https://developer.nvidia.com/cuda-12-1-0-download-archive

# ì„¤ì¹˜ í›„ í™˜ê²½ ë³€ìˆ˜ í™•ì¸
echo $CUDA_HOME
echo $PATH
```

### 3. cuDNN ì„¤ì¹˜
```bash
# cuDNN 8.9.7 ì„¤ì¹˜ (CUDA 12.1ìš©)
# https://developer.nvidia.com/cudnn

# ì••ì¶• í•´ì œ í›„ CUDA í´ë”ì— ë³µì‚¬
# C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v12.1\
```

## ğŸ Python í™˜ê²½ ì„¤ì •

### 1. ê°€ìƒí™˜ê²½ ìƒì„±
```bash
# Conda ì‚¬ìš© (ê¶Œì¥)
conda create -n qwen-ai python=3.11
conda activate qwen-ai

# ë˜ëŠ” venv ì‚¬ìš©
python -m venv qwen-ai
qwen-ai\Scripts\activate
```

### 2. PyTorch ì„¤ì¹˜ (CUDA ì§€ì›)
```bash
# CUDA 12.1ìš© PyTorch ì„¤ì¹˜
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121

# ì„¤ì¹˜ í™•ì¸
python -c "import torch; print(torch.cuda.is_available()); print(torch.version.cuda)"
```

### 3. ê¸°íƒ€ ì˜ì¡´ì„± ì„¤ì¹˜
```bash
# ê¸°ë³¸ íŒ¨í‚¤ì§€ë“¤
pip install -r requirements.txt

# ë˜ëŠ” ë‹¨ê³„ë³„ ì„¤ì¹˜
pip install fastapi uvicorn pydantic
pip install transformers accelerate peft
pip install bitsandbytes scikit-learn
pip install pytest debugpy requests
```

## âš™ï¸ í™˜ê²½ ë³€ìˆ˜ ì„¤ì •

### 1. .env íŒŒì¼ ìƒì„±
```bash
# í”„ë¡œì íŠ¸ ë£¨íŠ¸ì— .env íŒŒì¼ ìƒì„±
cp env_example.txt .env
```

### 2. CUDA ê´€ë ¨ ì„¤ì •
```env
# CUDA ì„¤ì •
DEVICE=cuda
TORCH_DTYPE=float16
CUDA_VISIBLE_DEVICES=0
TORCH_CUDA_ARCH_LIST=8.9

# ëª¨ë¸ ì„¤ì •
MODEL_PATH=Qwen/Qwen2.5-7B-Instruct
ADAPTER_PATH=training/qlora-out/export

# ì„±ëŠ¥ ìµœì í™”
MAX_LENGTH=2048
TEMPERATURE=0.7
TOP_P=0.9
DO_SAMPLE=True
```

## ğŸ§ª CUDA ì„¤ì¹˜ í™•ì¸

### 1. ê¸°ë³¸ CUDA í™•ì¸
```bash
# CUDA ì»´íŒŒì¼ëŸ¬ í™•ì¸
nvcc --version

# GPU ì •ë³´ í™•ì¸
nvidia-smi

# PyTorch CUDA í™•ì¸
python -c "import torch; print(f'CUDA available: {torch.cuda.is_available()}'); print(f'CUDA version: {torch.version.cuda}'); print(f'GPU count: {torch.cuda.device_count()}')"
```

### 2. ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
```bash
# GPU ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰
python test_cuda_api.py

# ë˜ëŠ” ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸
python -c "
import torch
if torch.cuda.is_available():
    device = torch.device('cuda')
    x = torch.randn(1000, 1000, device=device)
    y = torch.randn(1000, 1000, device=device)
    z = torch.mm(x, y)
    print('GPU ì—°ì‚° ì„±ê³µ!')
    print(f'GPU: {torch.cuda.get_device_name()}')
    print(f'ë©”ëª¨ë¦¬: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f}GB')
else:
    print('CUDA ì‚¬ìš© ë¶ˆê°€')
"
```

## ğŸš€ ì„œë²„ ì‹¤í–‰

### 1. ì„œë²„ ì‹œì‘
```bash
# CUDA ìµœì í™”ëœ ì„œë²„ ì‹¤í–‰
python run_server.py --host 127.0.0.1 --port 8765

# ë˜ëŠ” ì§ì ‘ ì‹¤í–‰
python -m uvicorn server.app:app --host 127.0.0.1 --port 8765 --reload
```

### 2. CUDA ìƒíƒœ í™•ì¸
```bash
# ì„œë²„ ìƒíƒœ í™•ì¸ (CUDA ì •ë³´ í¬í•¨)
curl http://127.0.0.1:8765/health

# ë˜ëŠ” ë¸Œë¼ìš°ì €ì—ì„œ
# http://127.0.0.1:8765/health
```

## ğŸ” ë¬¸ì œ í•´ê²°

### 1. ì¼ë°˜ì ì¸ CUDA ì˜¤ë¥˜

#### CUDA out of memory
```bash
# GPU ë©”ëª¨ë¦¬ ì •ë¦¬
python -c "import torch; torch.cuda.empty_cache()"

# ëª¨ë¸ ì–‘ìí™” ì„¤ì • ì¡°ì •
# .env íŒŒì¼ì—ì„œ TORCH_DTYPE=float16 ì„¤ì •
```

#### CUDA driver version is insufficient
```bash
# NVIDIA ë“œë¼ì´ë²„ ì—…ë°ì´íŠ¸ í•„ìš”
# https://www.nvidia.com/Download/index.aspx
```

#### PyTorch CUDA ë²„ì „ ë¶ˆì¼ì¹˜
```bash
# ê¸°ì¡´ PyTorch ì œê±°
pip uninstall torch torchvision torchaudio

# ì˜¬ë°”ë¥¸ ë²„ì „ ì¬ì„¤ì¹˜
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121
```

### 2. ì„±ëŠ¥ ìµœì í™”

#### ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ìµœì í™”
```env
# .env íŒŒì¼ì—ì„œ
TORCH_DTYPE=float16  # 16ë¹„íŠ¸ ì •ë°€ë„
MAX_LENGTH=1024      # ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ ì œí•œ
```

#### GPU ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§
```bash
# ì‹¤ì‹œê°„ GPU ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§
watch -n 1 nvidia-smi

# ë˜ëŠ” Windowsì—ì„œ
# nvidia-smi -l 1
```

## ğŸ“Š ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬

### 1. CPU vs GPU ë¹„êµ
```bash
# CPU ëª¨ë“œ í…ŒìŠ¤íŠ¸
DEVICE=cpu python test_cuda_api.py

# GPU ëª¨ë“œ í…ŒìŠ¤íŠ¸
DEVICE=cuda python test_cuda_api.py
```

### 2. ì˜ˆìƒ ì„±ëŠ¥ í–¥ìƒ
- **CPU ëª¨ë“œ**: PLAN ìƒì„± 30-60ì´ˆ, PATCH ìƒì„± 60-120ì´ˆ
- **GPU ëª¨ë“œ**: PLAN ìƒì„± 5-15ì´ˆ, PATCH ìƒì„± 15-30ì´ˆ
- **ì„±ëŠ¥ í–¥ìƒ**: **3-5ë°° ë¹ ë¦„** ğŸš€

## ğŸ¯ ìµœì í™” íŒ

### 1. ëª¨ë¸ ì„ íƒ
- **Qwen2.5-7B**: ë¹ ë¥¸ ì‘ë‹µ, ì ì€ ë©”ëª¨ë¦¬ (ê¶Œì¥)
- **Qwen2.5-14B**: ë” ì •í™•í•œ ì‘ë‹µ, ë” ë§ì€ ë©”ëª¨ë¦¬
- **Qwen2.5-32B**: ìµœê³  í’ˆì§ˆ, ëŒ€ìš©ëŸ‰ GPU í•„ìš”

### 2. ë©”ëª¨ë¦¬ ìµœì í™”
- **4-bit ì–‘ìí™”**: ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ 75% ê°ì†Œ
- **Flash Attention 2**: ë©”ëª¨ë¦¬ íš¨ìœ¨ì„± í–¥ìƒ
- **Gradient Checkpointing**: í•™ìŠµ ì‹œ ë©”ëª¨ë¦¬ ì ˆì•½

### 3. ë°°ì¹˜ ì²˜ë¦¬
- ì—¬ëŸ¬ ìš”ì²­ì„ ë°°ì¹˜ë¡œ ì²˜ë¦¬í•˜ì—¬ GPU í™œìš©ë„ í–¥ìƒ
- ë¹„ë™ê¸° ì²˜ë¦¬ë¡œ ì‘ë‹µ ì‹œê°„ ë‹¨ì¶•

## ğŸ”— ìœ ìš©í•œ ë§í¬

- [NVIDIA CUDA ë‹¤ìš´ë¡œë“œ](https://developer.nvidia.com/cuda-downloads)
- [PyTorch CUDA ì„¤ì¹˜](https://pytorch.org/get-started/locally/)
- [CUDA ì•„í‚¤í…ì²˜ í˜¸í™˜ì„±](https://developer.nvidia.com/cuda-gpus)
- [Flash Attention 2](https://github.com/Dao-AILab/flash-attention)

## ğŸ“ ì²´í¬ë¦¬ìŠ¤íŠ¸

- [ ] NVIDIA ë“œë¼ì´ë²„ ì„¤ì¹˜
- [ ] CUDA Toolkit ì„¤ì¹˜
- [ ] cuDNN ì„¤ì¹˜
- [ ] Python ê°€ìƒí™˜ê²½ ìƒì„±
- [ ] PyTorch (CUDA) ì„¤ì¹˜
- [ ] ì˜ì¡´ì„± íŒ¨í‚¤ì§€ ì„¤ì¹˜
- [ ] í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
- [ ] CUDA ì„¤ì¹˜ í™•ì¸
- [ ] ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
- [ ] ì„œë²„ ì‹¤í–‰ ë° í…ŒìŠ¤íŠ¸

---

**ğŸ’¡ íŒ**: CUDA ì„¤ì •ì´ ì™„ë£Œë˜ë©´ `python test_cuda_api.py`ë¥¼ ì‹¤í–‰í•˜ì—¬ ì „ì²´ ì‹œìŠ¤í…œì„ í…ŒìŠ¤íŠ¸í•˜ì„¸ìš”!
