#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Qwen3-8B Local Coding AI - FastAPI ì„œë²„ (CUDA ìµœì í™”)
"""

import os
import logging
import uuid
import json
import atexit
from datetime import datetime
from typing import List, Dict, Any, Optional, Union
from fastapi import FastAPI, HTTPException, BackgroundTasks, Request
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse
from pydantic import BaseModel, Field
from dotenv import load_dotenv
from starlette.concurrency import run_in_threadpool
import anyio
import torch
from contextlib import asynccontextmanager

import re, os
from pathlib import Path
from .model import Model, load_model_once, get_model
from .context import build_context
from .patch_apply import apply_patch_json
from .test_runner import run_pytest, get_test_summary
from .debug_runtime import DebugRuntime
from .path_resolver import PathResolver
import re

# ë¡œê¹… ì„¤ì • ê°œì„ 
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler('app.log', encoding='utf-8')
    ]
)
logger = logging.getLogger(__name__)

# .env ë¡œë“œ
load_dotenv()

app = FastAPI(
    title="Qwen3-8B Local Coding AI",
    description="ë¡œì»¬ì—ì„œ ì‹¤í–‰ë˜ëŠ” AI ì½”ë”© ì–´ì‹œìŠ¤í„´íŠ¸",
    version="1.0.0"
)

@app.on_event("startup")
def _startup():
    global model, debug_runtime
    logger.info("ğŸš€ ì„œë²„ ì‹œì‘")
    if torch.cuda.is_available():
        logger.info(f"âœ… CUDA ì‚¬ìš© ê°€ëŠ¥: {torch.cuda.get_device_name()}")
        logger.info(f"   GPU ë©”ëª¨ë¦¬: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f}GB")
        logger.info(f"   CUDA ë²„ì „: {torch.version.cuda}")
    else:
        logger.warning("âš ï¸ CUDA ì‚¬ìš© ë¶ˆê°€, CPU ëª¨ë“œë¡œ ì‹¤í–‰")

    # ì‹œì‘ ì‹œ 1íšŒ ë¡œë“œ í›„ app.stateì— ë³´ê´€
    m, t, q = load_model_once()
    app.state.model = m
    app.state.tok = t
    app.state.quant = q
    model = Model()  # ê¸°ì¡´ ì½”ë“œ ì˜ì¡´ì„± í˜¸í™˜ ëª©ì  (get_device_info ë“±)
    debug_runtime = DebugRuntime()
    torch.backends.cuda.matmul.allow_tf32 = True
    logger.info("ğŸ‰ ì„œë²„ ì‹œì‘ ì™„ë£Œ!")

# CORS ì„¤ì •
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# JSON íŒŒì‹± ê°œì„ ì„ ìœ„í•œ ë¯¸ë“¤ì›¨ì–´
@app.middleware("http")
async def json_error_handler(request: Request, call_next):
    try:
        response = await call_next(request)
        return response
    except json.JSONDecodeError as e:
        logger.error(f"JSON íŒŒì‹± ì˜¤ë¥˜: {e}")
        return JSONResponse(
            status_code=400,
            content={"detail": f"JSON íŒŒì‹± ì‹¤íŒ¨: {str(e)}"}
        )
    except UnicodeDecodeError as e:
        logger.error(f"ì¸ì½”ë”© ì˜¤ë¥˜: {e}")
        return JSONResponse(
            status_code=400,
            content={"detail": f"ì¸ì½”ë”© ì˜¤ë¥˜: {str(e)}"}
        )

# DebugRuntime í´ë˜ìŠ¤ ìˆ˜ì •
class DebugRuntime:
    def __init__(self):
        self.resources = []
    
    def add_resource(self, resource):
        self.resources.append(resource)
    
    def cleanup_all(self):
        """ëª¨ë“  ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        try:
            for resource in self.resources:
                if hasattr(resource, 'close'):
                    resource.close()
                elif hasattr(resource, 'cleanup'):
                    resource.cleanup()
            self.resources.clear()
            logger.info("âœ… ëª¨ë“  ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì™„ë£Œ")
        except Exception as e:
            logger.error(f"âŒ ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì‹¤íŒ¨: {e}")

# ì „ì—­ ë³€ìˆ˜
WORKSPACE_ROOT = str(Path(__file__).resolve().parents[1])  # repo ë£¨íŠ¸ë¡œ ì¡°ì •
RESOLVER = PathResolver(WORKSPACE_ROOT)

# í•¨ìˆ˜ ìŠ¤ë‹ˆí« ì¶”ì¶œ
def extract_func_snippet(src: str, func: str, ctx_lines: int = 40):
    m = re.search(rf'(?m)^[ \t]*def[ \t]+{re.escape(func)}\s*\(.*\):', src)
    if not m: 
        return src[:2000]
    start = m.start()
    m2 = re.search(r'(?m)^(?=\S)', src[m.end():])
    end = m.end() + (m2.start() if m2 else len(src))
    pre = src.rfind("\n", 0, max(0, start-1))
    pre = src.rfind("\n", 0, max(0, pre-ctx_lines)) if pre != -1 else 0
    post = src.find("\n", end)
    post = src.find("\n", post+1 if post != -1 else end)
    return src[pre:post]
model: Optional[Model] = None
debug_runtime: Optional[DebugRuntime] = None

def _pre_match(text: str, pat: str, use_regex: bool) -> bool:
    """ì•ˆì „í•œ ë§¤ì¹­: ê¸°ë³¸ì€ ë¦¬í„°ëŸ´. regex ìš”ì²­ ì‹œ ì»´íŒŒì¼ ì‹¤íŒ¨í•˜ë©´ ë¦¬í„°ëŸ´ fallback."""
    if not use_regex:
        return pat in text
    try:
        return re.search(pat, text, re.DOTALL | re.MULTILINE) is not None
    except re.error:
        return pat in text

def _tighten_loc_for_func(edit: dict, func_name: str):
    """í•¨ìˆ˜ ë¸”ë¡ ì •ê·œì‹ìœ¼ë¡œ loc ìë™ ë³´ì •"""
    pat = rf"^def {re.escape(func_name)}\([^\)]*\):[\s\S]*?(?=^\S)"
    loc = edit.get("loc", {})
    if loc.get("type") != "regex":
        edit["loc"] = {"type": "regex", "pattern": pat}
    else:
        edit["loc"]["pattern"] = pat

def _sanitize_preconditions(patch: dict) -> tuple[dict, list[dict]]:
    """
    ìƒˆ ì½”ë“œì—ë§Œ ìˆëŠ” íŒ¨í„´ì´ must_containì— ìˆìœ¼ë©´ must_not_containë¡œ ì´ë™.
    pre.regexê°€ ì—†ìœ¼ë©´ ê¸°ë³¸ ë¦¬í„°ëŸ´ ë§¤ì¹­.
    """
    changes = []
    for e in patch.get("edits", []):
        path = e.get("path"); code = e.get("code", "")
        pre  = e.get("pre", {}) or {}
        mc   = list(pre.get("must_contain", []) or [])
        mnc  = list(pre.get("must_not_contain", []) or [])
        use_regex = bool(pre.get("regex", False))

        abs_path = RESOLVER.resolve(path) if path else None
        try:
            src = open(abs_path, "r", encoding="utf-8").read() if abs_path else ""
        except Exception:
            src = ""

        moved = []
        for pat in list(mc):
            in_src = _pre_match(src, pat, use_regex)
            in_new = _pre_match(code, pat, use_regex)
            # íŒŒì¼ì—ëŠ” ì—†ê³  ìƒˆ ì½”ë“œì—ëŠ” ìˆìœ¼ë©´ â†’ must_not_containìœ¼ë¡œ ì´ë™
            if (not in_src) and in_new:
                mc.remove(pat)
                if pat not in mnc:
                    mnc.append(pat)
                moved.append(pat)

        if moved:
            pre["must_contain"] = mc
            pre["must_not_contain"] = mnc
            e["pre"] = pre
            changes.append({"path": path, "moved": moved})
            
            # í•¨ìˆ˜ ë¸”ë¡ ì•µì»¤ ìë™-ì¡°ì´ê¸°
            for pat in moved:
                m = re.search(r"def\s+([A-Za-z_][A-Za-z0-9_]*)\s*\(", pat)
                if m:
                    _tighten_loc_for_func(e, m.group(1))
                    
    return patch, changes

# ìš”ì²­ ëª¨ë¸ ì •ì˜ (Pydantic ëª¨ë¸)
class PlanRequest(BaseModel):
    intent: str = Field(..., description="ê³„íšì˜ ì˜ë„")
    paths: List[str] = Field(..., description="íŒŒì¼ ê²½ë¡œë“¤")
    code_paste: Optional[str] = Field(None, description="ë¶™ì—¬ë„£ì€ ì½”ë“œ")

class FeedbackRequest(BaseModel):
    hint: Optional[str] = Field(None, description="íŒíŠ¸")
    reason: Optional[str] = Field(None, description="ì´ìœ ")

class PatchRequest(BaseModel):
    plan: Union[Dict[str, Any], str] = Field(..., description="í”Œëœ ë°ì´í„°")
    feedback: Optional[FeedbackRequest] = Field(None, description="í”¼ë“œë°±")

class PlanResponse(BaseModel):
    plan_id: str
    plan: Dict[str, Any]
    raw_response: str

class PatchResponse(BaseModel):
    patch_id: str
    patch: Dict[str, Any]
    raw_response: str

class ApplyRequest(BaseModel):
    patch: Dict[str, Any] = Field(..., description="ì ìš©í•  íŒ¨ì¹˜")
    allowed_paths: List[str] = Field(..., description="í—ˆìš©ëœ íŒŒì¼ ê²½ë¡œë“¤")
    dry_run: bool = Field(False, description="ì‹¤ì œ ì ìš© ì—¬ë¶€")

class ApplyResponse(BaseModel):
    applied: List[str]
    skipped: List[Dict[str, Any]] = Field(default_factory=list, description="ê±´ë„ˆë›´ íŒŒì¼ë“¤")
    failed: List[Dict[str, Any]] = Field(default_factory=list, description="ì‹¤íŒ¨í•œ íŒŒì¼ë“¤")
    dry_run: bool
    details: List[Dict[str, Any]] = Field(default_factory=list, description="ìƒì„¸ ì •ë³´")

class TestRequest(BaseModel):
    paths: Optional[List[str]] = Field(None, description="í…ŒìŠ¤íŠ¸í•  ê²½ë¡œë“¤")
    coverage: bool = Field(False, description="ì»¤ë²„ë¦¬ì§€ í¬í•¨ ì—¬ë¶€")

class TestResponse(BaseModel):
    summary: Dict[str, Any]
    output: str

class WorkflowRequest(BaseModel):
    intent: str = Field(..., description="ì‚¬ìš©ì ì˜ë„")
    paths: List[str] = Field(..., description="ë¶„ì„í•  íŒŒì¼ ê²½ë¡œë“¤")
    code_paste: str = Field("", description="ì½”ë“œ ìŠ¤ë‹ˆí«")
    auto_apply: bool = Field(False, description="ìë™ ì ìš© ì—¬ë¶€")
    auto_test: bool = Field(False, description="ìë™ í…ŒìŠ¤íŠ¸ ì—¬ë¶€")

class WorkflowResponse(BaseModel):
    workflow_id: str
    plan: Dict[str, Any]
    patch: Dict[str, Any]
    applied: bool
    test_results: Optional[Dict[str, Any]]

class DebugRequest(BaseModel):
    command: str = Field(..., description="ë””ë²„ê·¸ ëª…ë ¹ì–´")
    script_path: Optional[str] = Field(None, description="ìŠ¤í¬ë¦½íŠ¸ ê²½ë¡œ")
    port: Optional[int] = Field(None, description="ë””ë²„ê·¸ í¬íŠ¸")

class DebugResponse(BaseModel):
    success: bool
    message: str
    debug_port: Optional[int]
    processes: List[Dict[str, Any]]

# startup_event ì œê±° - lifespanì—ì„œ ì²˜ë¦¬

@app.on_event("shutdown")
async def shutdown_event():
    """ì„œë²„ ì¢…ë£Œ ì‹œ ì •ë¦¬"""
    global model, debug_runtime
    
    try:
        if model:
            del model
        if debug_runtime:
            debug_runtime.cleanup_all()
        
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        
        logger.info("âœ… ì„œë²„ ì •ë¦¬ ì™„ë£Œ")
        
    except Exception as e:
        logger.error(f"âŒ ì„œë²„ ì •ë¦¬ ì‹¤íŒ¨: {e}")

@app.get("/health")
async def health_check():
    """ì„œë²„ ìƒíƒœ í™•ì¸"""
    try:
        # ëª¨ë¸ ë¡œë“œ ìƒíƒœ ë° ì–‘ìí™” ëª¨ë“œ
        model_loaded = hasattr(app.state, "model")
        quant = getattr(app.state, "quant", "unknown")
        
        # ê¸°ë³¸ ìƒíƒœ ì •ë³´
        status = {
            "status": "healthy",
            "model_loaded": model_loaded,
            "quantization": quant,
            "use_4bit": quant == "4bit",
            "timestamp": datetime.now().isoformat()
        }
        
        # CUDA ì •ë³´ ì¶”ê°€ (ê¸°ì¡´ ë¡œì§ ìœ ì§€)
        if torch.cuda.is_available():
            status.update({
                "cuda_available": True,
                "gpu_memory_total": f"{torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f}GB",
                "gpu_memory_allocated": f"{torch.cuda.memory_allocated(0) / 1024**3:.1f}GB",
                "gpu_memory_cached": f"{torch.cuda.memory_reserved(0) / 1024**3:.1f}GB",
                "cuda_version": torch.version.cuda
            })
        else:
            status["cuda_available"] = False
        
        # ëª¨ë¸ ë””ë°”ì´ìŠ¤ ì •ë³´
        if model:
            try:
                device_info = model.get_device_info()
                status.update(device_info)
            except Exception:
                pass
        
        return status
    except Exception as e:
        logger.error(f"í—¬ìŠ¤ ì²´í¬ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=f"ì„œë²„ ìƒíƒœ ë¶ˆì•ˆì •: {str(e)}")

@app.post("/plan", response_model=PlanResponse)
async def create_plan(request: PlanRequest):
    try:
        logger.info(f"í”Œëœ ìš”ì²­ ë°›ìŒ: intent={request.intent}, paths={request.paths}")
        
        # context ë³€ìˆ˜ ì´ˆê¸°í™”
        context = ""
        
        # ì»¨í…ìŠ¤íŠ¸ ë¹Œë“œ
        try:
            context = await run_in_threadpool(build_context, request.paths)
            logger.info(f"ì»¨í…ìŠ¤íŠ¸ ë¹Œë“œ ì„±ê³µ: {len(context)} ë¬¸ì")
        except Exception as e:
            logger.warning(f"ì»¨í…ìŠ¤íŠ¸ ë¹Œë“œ ì‹¤íŒ¨: {e}")
            context = f"ì»¨í…ìŠ¤íŠ¸ ë¹Œë“œ ì‹¤íŒ¨: {str(e)}"
        
        # í”Œëœ ìƒì„±
        plan_data = {
            "intent": request.intent,
            "context": context,
            "code_paste": request.code_paste or "",
            "paths": request.paths,
            "timestamp": str(datetime.now())
        }
        
        # ì‹¤ì œ í”Œëœ ìƒì„± (ì—¬ê¸°ì— AI ëª¨ë¸ í˜¸ì¶œ ë¡œì§ êµ¬í˜„)
        plan_result = await generate_plan_with_ai(plan_data)
        
        return PlanResponse(
            plan_id=f"plan_{uuid.uuid4().hex[:8]}",
            plan=plan_result,
            raw_response="Generated with 4-bit quantization"
        )
        
    except Exception as e:
        logger.error(f"í”Œëœ ìƒì„± ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=f"í”Œëœ ìƒì„± ì‹¤íŒ¨: {str(e)}")

@app.post("/patch", response_model=PatchResponse)
async def create_patch(request: PatchRequest):
    try:
        logger.info("íŒ¨ì¹˜ ìš”ì²­ ë°›ìŒ")
        
        # planì´ ë¬¸ìì—´ì¸ ê²½ìš° JSONìœ¼ë¡œ íŒŒì‹±
        if isinstance(request.plan, str):
            try:
                plan_data = json.loads(request.plan)
            except json.JSONDecodeError as e:
                raise HTTPException(status_code=400, detail=f"í”Œëœ JSON íŒŒì‹± ì‹¤íŒ¨: {str(e)}")
        else:
            plan_data = request.plan
        
        # íŒ¨ì¹˜ ìƒì„± ë¡œì§
        patch_result = await generate_patch_with_ai(plan_data, request.feedback)
        
        return {
            "patch": patch_result,
            "status": "success"
        }
        
    except Exception as e:
        logger.error(f"íŒ¨ì¹˜ ìƒì„± ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=f"íŒ¨ì¹˜ ìƒì„± ì‹¤íŒ¨: {str(e)}")

@app.post("/patch_smart")
async def create_smart_patch(request: PatchRequest):
    try:
        logger.info("ìŠ¤ë§ˆíŠ¸ íŒ¨ì¹˜ ìš”ì²­ ë°›ìŒ")
        
        # plan ë°ì´í„° ì²˜ë¦¬
        if isinstance(request.plan, str):
            try:
                plan_data = json.loads(request.plan)
            except json.JSONDecodeError as e:
                logger.error(f"JSON íŒŒì‹± ì‹¤íŒ¨: {e}")
                raise HTTPException(status_code=400, detail=f"JSON íŒŒì‹± ì‹¤íŒ¨: {str(e)}")
        else:
            plan_data = request.plan
        
        # ìŠ¤ë§ˆíŠ¸ íŒ¨ì¹˜ ìƒì„±
        patch_result = await generate_smart_patch(plan_data, request.feedback)
        
        # ì„±ê³µ ë¡œê·¸ ê¸°ë¡
        try:
            os.makedirs("training/success_logs", exist_ok=True)
            rec = {
                "when": datetime.now().isoformat(),
                "role": "patch_success",
                "plan": plan_data,
                "feedback": request.feedback.dict() if request.feedback else {},
                "patch": patch_result
            }
            with open(f"training/success_logs/patch_{datetime.now():%Y%m%d}.jsonl", "a", encoding="utf-8") as f:
                f.write(json.dumps(rec, ensure_ascii=False) + "\n")
            logger.info("âœ… ì„±ê³µ ë¡œê·¸ ê¸°ë¡ ì™„ë£Œ")
        except Exception as e:
            logger.warning(f"âš ï¸ ì„±ê³µ ë¡œê·¸ ê¸°ë¡ ì‹¤íŒ¨: {e}")
        
        # feedbackì˜ hintê°€ "Return ONLY the items of the edits array"ì¸ ê²½ìš°
        if (request.feedback and 
            request.feedback.hint and 
            "Return ONLY the items of the edits array" in request.feedback.hint):
            
            # edits ë°°ì—´ë§Œ ë°˜í™˜
            if isinstance(patch_result, dict) and "edits" in patch_result:
                return patch_result["edits"]
            
        return patch_result
        
    except Exception as e:
        logger.error(f"ìŠ¤ë§ˆíŠ¸ íŒ¨ì¹˜ ìƒì„± ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=f"ìŠ¤ë§ˆíŠ¸ íŒ¨ì¹˜ ìƒì„± ì‹¤íŒ¨: {str(e)}")

@app.post("/apply", response_model=ApplyResponse)
async def apply_patch(request: ApplyRequest):
    """íŒ¨ì¹˜ ì ìš©"""
    try:
        # íŒ¨ì¹˜ ì ìš©
        result = apply_patch_json(
            patch=request.patch,
            allowed_paths=request.allowed_paths,
            dry_run=request.dry_run
        )
        
        return ApplyResponse(
            applied=result.get("applied", []),
            skipped=result.get("skipped", []),
            failed=result.get("failed", []),
            dry_run=request.dry_run,
            details=result.get("details", [])
        )
        
    except Exception as e:
        logger.error(f"íŒ¨ì¹˜ ì ìš© ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=f"íŒ¨ì¹˜ ì ìš© ì‹¤íŒ¨: {str(e)}")

@app.post("/test", response_model=TestResponse)
async def run_tests(request: TestRequest):
    """í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
    try:
        # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
        if request.coverage:
            test_path = (request.paths or ["."])[0] if request.paths else "."
            output = run_pytest(test_path, coverage=True)
        else:
            test_path = (request.paths or ["."])[0] if request.paths else "."
            output = run_pytest(test_path)
        
        # í…ŒìŠ¤íŠ¸ ìš”ì•½
        summary = get_test_summary(output)
        
        return TestResponse(summary=summary, output=output.output)
        
    except Exception as e:
        logger.error(f"í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=f"í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì‹¤íŒ¨: {str(e)}")

@app.post("/workflow", response_model=WorkflowResponse)
async def run_workflow(request: WorkflowRequest):
    """ì „ì²´ ì›Œí¬í”Œë¡œìš° ì‹¤í–‰"""
    if not model:
        raise HTTPException(status_code=503, detail="ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•ŠìŒ")
    
    try:
        workflow_id = str(uuid.uuid4())
        logger.info(f"ì›Œí¬í”Œë¡œìš° ì‹œì‘: {workflow_id}")
        
        # 1. ì»¨í…ìŠ¤íŠ¸ ë¹Œë“œ
        context = await run_in_threadpool(build_context, request.paths)
        
        # 2. ê³„íš ìƒì„± (íƒ€ì„ì•„ì›ƒ ì ìš©)
        with anyio.move_on_after(30) as cs:
            plan_result = await run_in_threadpool(
                model.plan,
                context,
                request.intent,
                request.code_paste
            )
        
        if cs.cancel_called:
            raise TimeoutError("PLAN generation timeout")
        
        # 3. íŒ¨ì¹˜ ìƒì„± (íƒ€ì„ì•„ì›ƒ ì ìš©)
        with anyio.move_on_after(90) as cs:   # 90ì´ˆë¡œ ì¦ê°€
            patch_result = await run_in_threadpool(model.patch, plan_result)
        
        if cs.cancel_called:
            raise TimeoutError("PATCH generation timeout")
        
        # 4. íŒ¨ì¹˜ ì ìš© (ì„ íƒì‚¬í•­)
        applied = False
        if request.auto_apply:
            apply_result = apply_patch_json(
                patch=patch_result,
                allowed_paths=request.paths,
                dry_run=False
            )
            applied = len(apply_result.get("applied", [])) > 0
        
        # 5. í…ŒìŠ¤íŠ¸ ì‹¤í–‰ (ì„ íƒì‚¬í•­)
        test_results = None
        if request.auto_test:
            try:
                test_path = request.paths[0] if request.paths else "."
                output = run_pytest(test_path)
                test_results = get_test_summary(output)
            except Exception as e:
                logger.warning(f"í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
        
        return WorkflowResponse(
            workflow_id=workflow_id,
            plan=plan_result,
            patch=patch_result,
            applied=applied,
            test_results=test_results
        )
        
    except TimeoutError as te:
        logger.error(f"ì›Œí¬í”Œë¡œìš° íƒ€ì„ì•„ì›ƒ: {te}")
        raise HTTPException(status_code=504, detail=str(te))
    except Exception as e:
        logger.error(f"ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=f"ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ ì‹¤íŒ¨: {str(e)}")

@app.post("/debug", response_model=DebugResponse)
async def debug_control(request: DebugRequest):
    """ë””ë²„ê·¸ ëŸ°íƒ€ì„ ì œì–´"""
    if not debug_runtime:
        raise HTTPException(status_code=503, detail="ë””ë²„ê·¸ ëŸ°íƒ€ì„ì´ ì´ˆê¸°í™”ë˜ì§€ ì•ŠìŒ")
    
    try:
        if request.command == "start":
            if not request.script_path:
                raise HTTPException(status_code=400, detail="ìŠ¤í¬ë¦½íŠ¸ ê²½ë¡œê°€ í•„ìš”í•©ë‹ˆë‹¤")
            
            debug_port = debug_runtime.run_with_debugpy(
                script_path=request.script_path,
                port=request.port
            )
            
            return DebugResponse(
                success=True,
                message=f"ë””ë²„ê·¸ í”„ë¡œì„¸ìŠ¤ ì‹œì‘ë¨ (í¬íŠ¸: {debug_port})",
                debug_port=debug_port,
                processes=debug_runtime.list_processes()
            )
            
        elif request.command == "stop":
            if not request.port:
                raise HTTPException(status_code=400, detail="í¬íŠ¸ ë²ˆí˜¸ê°€ í•„ìš”í•©ë‹ˆë‹¤")
            
            debug_runtime.stop_process(request.port)
            
            return DebugResponse(
                success=True,
                message=f"í¬íŠ¸ {request.port}ì˜ í”„ë¡œì„¸ìŠ¤ ì¤‘ì§€ë¨",
                debug_port=None,
                processes=debug_runtime.list_processes()
            )
            
        elif request.command == "list":
            return DebugResponse(
                success=True,
                message="ë””ë²„ê·¸ í”„ë¡œì„¸ìŠ¤ ëª©ë¡",
                debug_port=None,
                processes=debug_runtime.list_processes()
            )
            
        else:
            raise HTTPException(status_code=400, detail=f"ì•Œ ìˆ˜ ì—†ëŠ” ëª…ë ¹ì–´: {request.command}")
            
    except Exception as e:
        logger.error(f"ë””ë²„ê·¸ ì œì–´ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=f"ë””ë²„ê·¸ ì œì–´ ì‹¤íŒ¨: {str(e)}")

@app.get("/")
async def root():
    """ë£¨íŠ¸ ì—”ë“œí¬ì¸íŠ¸"""
    return {
        "message": "Qwen3-8B Local Coding AI ì„œë²„",
        "version": "1.0.0",
        "endpoints": [
            "/health - ì„œë²„ ìƒíƒœ í™•ì¸",
            "/plan - ì½”ë“œ ìˆ˜ì • ê³„íš ìƒì„±",
            "/patch - ì½”ë“œ íŒ¨ì¹˜ ìƒì„±",
            "/apply - íŒ¨ì¹˜ ì ìš©",
            "/test - í…ŒìŠ¤íŠ¸ ì‹¤í–‰",
            "/workflow - ì „ì²´ ì›Œí¬í”Œë¡œìš° ì‹¤í–‰",
            "/debug - ë””ë²„ê·¸ ëŸ°íƒ€ì„ ì œì–´"
        ]
    }

# AI ìƒì„± í•¨ìˆ˜ë“¤
import os, json, logging
from server.model import get_model

log = logging.getLogger(__name__)
USE_DUMMY = os.getenv("USE_DUMMY_AI", "0") == "1"

async def generate_plan_with_ai(plan_data: dict) -> dict:
    """AIë¥¼ ì‚¬ìš©í•˜ì—¬ í”Œëœ ìƒì„±"""
    if USE_DUMMY:
        # ì„ì‹œ ë”ë¯¸ê°€ í•„ìš”í•˜ë©´ ë‚¨ê²¨ë‘ë˜ ê¸°ë³¸ê°’ì€ ë¹„í™œì„±í™”
        return {"files":[{"path": plan_data["paths"][0], "reason": plan_data["intent"], "strategy":"anchor"}], "notes":""}

    # ì•± ìƒíƒœì˜ ë‹¨ì¼ ëª¨ë¸/í† í¬ë‚˜ì´ì € ì‚¬ìš©
    m = app.state.model
    tok = app.state.tok
    # ì…ë ¥ ì»·(1500ì) + ì»¨í…ìŠ¤íŠ¸ ì œê±° â†’ í”„ë¦¬í•„ ì‹œê°„ ë‹¨ì¶•
    code = (plan_data.get("code_paste") or "")[:1500]
    sys = "You must output STRICT JSON plan with keys 'files' and 'notes'."
    prompt = f"{sys}\n\n[CODE]\n{code}\n"
    inputs = tok(prompt, return_tensors="pt", add_special_tokens=False, truncation=True, max_length=1024).to(m.device)
    from transformers import StoppingCriteriaList
    class _Wall:
        def __init__(self, s=12):
            import time
            self.t0=time.monotonic(); self.S=s; self._time=time
        def __call__(self, input_ids, scores=None, **kw):
            return (self._time.monotonic()-self.t0) >= self.S
    gen_kw = dict(
        max_new_tokens=24,
        do_sample=False,
        temperature=0.0,
        top_p=1.0,
        use_cache=True,
        repetition_penalty=1.05,
        max_time=12,
        stopping_criteria=StoppingCriteriaList([_Wall(12)]),
        eos_token_id=tok.eos_token_id,
        pad_token_id=tok.eos_token_id,
    )
    out = m.generate(**inputs, **gen_kw)
    text = tok.decode(out[0], skip_special_tokens=True)
    # ê°„ëµ íŒŒì„œ: ìµœì†Œ ìŠ¤í‚¤ë§ˆ ë³´ì •
    try:
        import json as _json
        obj = _json.loads(text)
        if not isinstance(obj, dict):
            raise ValueError
        if "files" not in obj:
            obj["files"] = [{"path": plan_data["paths"][0] if plan_data.get("paths") else "", "reason": plan_data["intent"], "strategy":"regex", "tests": []}]
        if "notes" not in obj:
            obj["notes"] = ""
        return obj
    except Exception:
        return {"files":[{"path": plan_data["paths"][0] if plan_data.get("paths") else "", "reason": plan_data["intent"], "strategy":"regex", "tests": []}], "notes":""}
    if isinstance(plan, str):
        plan = json.loads(plan)
    return plan

async def generate_patch_with_ai(plan_data: dict, feedback: Optional[FeedbackRequest]) -> dict:
    """AIë¥¼ ì‚¬ìš©í•˜ì—¬ íŒ¨ì¹˜ ìƒì„±"""
    if USE_DUMMY:
        return {"version":"1", "edits":[]}

    m = get_model()
    patch = m.patch(plan=plan_data, feedback=feedback, max_new_tokens=256, budget_s=90)
    if isinstance(patch, str):
        patch = json.loads(patch)
    return patch

async def generate_smart_patch(plan_data: dict, feedback: Optional[FeedbackRequest]) -> dict:
    """ìŠ¤ë§ˆíŠ¸ íŒ¨ì¹˜ ìƒì„±"""
    if USE_DUMMY:
        return {"version":"1", "edits":[]}

    m = get_model()
    patch = m.patch(plan=plan_data, feedback=feedback, max_new_tokens=256, budget_s=90)
    if isinstance(patch, str):
        patch = json.loads(patch)
    return patch

# ì „ì—­ ì˜ˆì™¸ í•¸ë“¤ëŸ¬
@app.exception_handler(Exception)
async def global_exception_handler(request: Request, exc: Exception):
    logger.error(f"ì „ì—­ ì˜ˆì™¸ ë°œìƒ: {exc}", exc_info=True)
    return JSONResponse(
        status_code=500,
        content={"detail": f"ì„œë²„ ë‚´ë¶€ ì˜¤ë¥˜: {str(exc)}"}
    )

# ë˜ëŠ” ê¸°ì¡´ ë°©ì‹ì„ ì‚¬ìš©í•œë‹¤ë©´
@app.on_event("shutdown")
async def shutdown_event():
    try:
        if hasattr(debug_runtime, 'cleanup_all'):
            debug_runtime.cleanup_all()
        logger.info("âœ… ì„œë²„ ì •ë¦¬ ì™„ë£Œ")
    except Exception as e:
        logger.error(f"âŒ ì„œë²„ ì •ë¦¬ ì‹¤íŒ¨: {e}")

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="127.0.0.1", port=8765)

