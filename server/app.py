#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Qwen3-8B Local Coding AI - FastAPI ì„œë²„ (CUDA ìµœì í™”)
"""

import os
import logging
import uuid
import json
import atexit
from datetime import datetime
from typing import List, Dict, Any, Optional, Union
from fastapi import FastAPI, HTTPException, BackgroundTasks, Request, Body
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse
from pydantic import BaseModel, Field
from dotenv import load_dotenv
from starlette.concurrency import run_in_threadpool
import anyio
import torch
from contextlib import asynccontextmanager

import re, os
from pathlib import Path
from .model import Model, load_model_once, get_model
from .model import get_adapter_info
from .context import build_context
from .patch_apply import apply_patch_json
from .test_runner import run_pytest, get_test_summary
from .debug_runtime import DebugRuntime
from .path_resolver import PathResolver
import re

# ë¡œê¹… ì„¤ì • ê°œì„ 
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler('app.log', encoding='utf-8')
    ]
)
logger = logging.getLogger(__name__)

# .env ë¡œë“œ
load_dotenv()

app = FastAPI(
    title="Qwen3-8B Local Coding AI",
    description="ë¡œì»¬ì—ì„œ ì‹¤í–‰ë˜ëŠ” AI ì½”ë”© ì–´ì‹œìŠ¤í„´íŠ¸",
    version="1.0.0"
)

@app.on_event("startup")
def _startup():
    global model, debug_runtime
    logger.info("ğŸš€ ì„œë²„ ì‹œì‘")
    if torch.cuda.is_available():
        logger.info(f"âœ… CUDA ì‚¬ìš© ê°€ëŠ¥: {torch.cuda.get_device_name()}")
        logger.info(f"   GPU ë©”ëª¨ë¦¬: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f}GB")
        logger.info(f"   CUDA ë²„ì „: {torch.version.cuda}")
    else:
        logger.warning("âš ï¸ CUDA ì‚¬ìš© ë¶ˆê°€, CPU ëª¨ë“œë¡œ ì‹¤í–‰")

    # ì‹œì‘ ì‹œ ë² ì´ìŠ¤ ëª¨ë¸ì„ 1íšŒë§Œ ë¡œë“œí•˜ì—¬ ë³´ê´€
    base, t, q = load_model_once()
    app.state.base_model = base  # ë² ì´ìŠ¤ ëª¨ë¸ ë³´ê´€
    app.state.tok = t
    app.state.quant = q
    
    # v0ë¡œ ì‹œì‘ (ì–´ëŒ‘í„° ë¯¸ì ìš©)
    app.state.model = base
    app.state.adapter_path = "__none__"

    # ADAPTER_PATHê°€ ì§€ì •ë˜ë©´ ë¶€íŠ¸ì‹œì—ë§Œ ë˜í¼ë¥¼ ì”Œì›Œì¤Œ
    adp = os.getenv("ADAPTER_PATH", "training/qlora-out/adapter")
    if adp and adp != "__none__" and os.path.isdir(adp):
        from peft import PeftModel
        app.state.model = PeftModel.from_pretrained(base, adp).eval()
        app.state.adapter_path = adp

    # ì–´ëŒ‘í„° ë©”íƒ€ì •ë³´ ìºì‹± (health ë…¸ì¶œìš©)
    try:
        ai = get_adapter_info()
        app.state.adapter_path = ai.get("path")
        app.state.adapter_version = ai.get("version")
    except Exception:
        app.state.adapter_path = None
        app.state.adapter_version = None
    model = Model()  # ê¸°ì¡´ ì½”ë“œ ì˜ì¡´ì„± í˜¸í™˜ ëª©ì  (get_device_info ë“±)
    debug_runtime = DebugRuntime()
    torch.backends.cuda.matmul.allow_tf32 = True
    logger.info("ğŸ‰ ì„œë²„ ì‹œì‘ ì™„ë£Œ!")

def _free_cuda():
    """CUDA ìºì‹œ ì •ë¦¬ ë° ê°€ë¹„ì§€ ì»¬ë ‰ì…˜"""
    import gc
    torch.cuda.empty_cache()
    gc.collect()

@app.post("/reload_adapter")
async def reload_adapter(payload: dict = Body(...)):
    """ì–´ëŒ‘í„°ë¥¼ ì„œë²„ ì¬ê¸°ë™ ì—†ì´ í•«ìŠ¤ì™‘í•œë‹¤.
    ë² ì´ìŠ¤ ëª¨ë¸ì€ ì¬ì‚¬ìš©í•˜ê³  PEFT ë˜í¼ë§Œ êµì²´í•œë‹¤.
    """
    try:
        path = payload.get("path") if isinstance(payload, dict) else None
        if path is None:
            raise HTTPException(status_code=400, detail="missing 'path'")

        base = app.state.base_model
        if base is None:
            raise HTTPException(status_code=500, detail="base model not loaded")

        # 1) í˜„ì¬ ëª¨ë¸ ì°¸ì¡° ëŠê³  ìºì‹œ ë¹„ìš°ê¸°
        app.state.model = base
        _free_cuda()

        # 2) __none__ì´ë©´ v0 (ì–´ëŒ‘í„° í•´ì œ)
        if path == "__none__":
            app.state.adapter_path = "__none__"
            app.state.adapter_version = None
            return {"ok": True, "adapter_path": path}

        # 3) ì–´ëŒ‘í„°ë§Œ ë˜í•‘í•´ì„œ êµì²´ (ë² ì´ìŠ¤ ì¬ë¡œë“œ ê¸ˆì§€!)
        if not os.path.exists(path):
            raise HTTPException(status_code=400, detail=f"adapter not found: {path}")
        
        from peft import PeftModel
        with torch.no_grad():
            new_model = PeftModel.from_pretrained(base, path).eval()
        app.state.model = new_model
        app.state.adapter_path = path
        
        # ì–´ëŒ‘í„° ë©”íƒ€ ì§ì ‘ ì„¤ì • (get_adapter_info() ìš°íšŒ)
        try:
            app.state.adapter_version = os.path.getmtime(path)
        except Exception:
            app.state.adapter_version = None
        
        _free_cuda()
        return {"ok": True, "adapter_path": path, "adapter_version": app.state.adapter_version}
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"ì–´ëŒ‘í„° ë¦¬ë¡œë“œ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=f"reload_adapter failed: {str(e)}")

# CORS ì„¤ì •
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# JSON íŒŒì‹± ê°œì„ ì„ ìœ„í•œ ë¯¸ë“¤ì›¨ì–´
@app.middleware("http")
async def json_error_handler(request: Request, call_next):
    try:
        response = await call_next(request)
        return response
    except json.JSONDecodeError as e:
        logger.error(f"JSON íŒŒì‹± ì˜¤ë¥˜: {e}")
        return JSONResponse(
            status_code=400,
            content={"detail": f"JSON íŒŒì‹± ì‹¤íŒ¨: {str(e)}"}
        )
    except UnicodeDecodeError as e:
        logger.error(f"ì¸ì½”ë”© ì˜¤ë¥˜: {e}")
        return JSONResponse(
            status_code=400,
            content={"detail": f"ì¸ì½”ë”© ì˜¤ë¥˜: {str(e)}"}
        )

# DebugRuntime í´ë˜ìŠ¤ ìˆ˜ì •
class DebugRuntime:
    def __init__(self):
        self.resources = []
    
    def add_resource(self, resource):
        self.resources.append(resource)
    
    def cleanup_all(self):
        """ëª¨ë“  ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        try:
            for resource in self.resources:
                if hasattr(resource, 'close'):
                    resource.close()
                elif hasattr(resource, 'cleanup'):
                    resource.cleanup()
            self.resources.clear()
            logger.info("âœ… ëª¨ë“  ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì™„ë£Œ")
        except Exception as e:
            logger.error(f"âŒ ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì‹¤íŒ¨: {e}")

# ì „ì—­ ë³€ìˆ˜
WORKSPACE_ROOT = str(Path(__file__).resolve().parents[1])  # repo ë£¨íŠ¸ë¡œ ì¡°ì •
RESOLVER = PathResolver(WORKSPACE_ROOT)

# í•¨ìˆ˜ ìŠ¤ë‹ˆí« ì¶”ì¶œ
def extract_func_snippet(src: str, func: str, ctx_lines: int = 40):
    m = re.search(rf'(?m)^[ \t]*def[ \t]+{re.escape(func)}\s*\(.*\):', src)
    if not m: 
        return src[:2000]
    start = m.start()
    m2 = re.search(r'(?m)^(?=\S)', src[m.end():])
    end = m.end() + (m2.start() if m2 else len(src))
    pre = src.rfind("\n", 0, max(0, start-1))
    pre = src.rfind("\n", 0, max(0, pre-ctx_lines)) if pre != -1 else 0
    post = src.find("\n", end)
    post = src.find("\n", post+1 if post != -1 else end)
    return src[pre:post]
model: Optional[Model] = None
debug_runtime: Optional[DebugRuntime] = None

def _pre_match(text: str, pat: str, use_regex: bool) -> bool:
    """ì•ˆì „í•œ ë§¤ì¹­: ê¸°ë³¸ì€ ë¦¬í„°ëŸ´. regex ìš”ì²­ ì‹œ ì»´íŒŒì¼ ì‹¤íŒ¨í•˜ë©´ ë¦¬í„°ëŸ´ fallback."""
    if not use_regex:
        return pat in text
    try:
        return re.search(pat, text, re.DOTALL | re.MULTILINE) is not None
    except re.error:
        return pat in text

def _tighten_loc_for_func(edit: dict, func_name: str):
    """í•¨ìˆ˜ ë¸”ë¡ ì •ê·œì‹ìœ¼ë¡œ loc ìë™ ë³´ì •"""
    pat = rf"^def {re.escape(func_name)}\([^\)]*\):[\s\S]*?(?=^\S)"
    loc = edit.get("loc", {})
    if loc.get("type") != "regex":
        edit["loc"] = {"type": "regex", "pattern": pat}
    else:
        edit["loc"]["pattern"] = pat

def _sanitize_preconditions(patch: dict) -> tuple[dict, list[dict]]:
    """
    ìƒˆ ì½”ë“œì—ë§Œ ìˆëŠ” íŒ¨í„´ì´ must_containì— ìˆìœ¼ë©´ must_not_containë¡œ ì´ë™.
    pre.regexê°€ ì—†ìœ¼ë©´ ê¸°ë³¸ ë¦¬í„°ëŸ´ ë§¤ì¹­.
    """
    changes = []
    for e in patch.get("edits", []):
        path = e.get("path"); code = e.get("code", "")
        pre  = e.get("pre", {}) or {}
        mc   = list(pre.get("must_contain", []) or [])
        mnc  = list(pre.get("must_not_contain", []) or [])
        use_regex = bool(pre.get("regex", False))

        abs_path = RESOLVER.resolve(path) if path else None
        try:
            src = open(abs_path, "r", encoding="utf-8").read() if abs_path else ""
        except Exception:
            src = ""

        moved = []
        for pat in list(mc):
            in_src = _pre_match(src, pat, use_regex)
            in_new = _pre_match(code, pat, use_regex)
            # íŒŒì¼ì—ëŠ” ì—†ê³  ìƒˆ ì½”ë“œì—ëŠ” ìˆìœ¼ë©´ â†’ must_not_containìœ¼ë¡œ ì´ë™
            if (not in_src) and in_new:
                mc.remove(pat)
                if pat not in mnc:
                    mnc.append(pat)
                moved.append(pat)

        if moved:
            pre["must_contain"] = mc
            pre["must_not_contain"] = mnc
            e["pre"] = pre
            changes.append({"path": path, "moved": moved})
            
            # í•¨ìˆ˜ ë¸”ë¡ ì•µì»¤ ìë™-ì¡°ì´ê¸°
            for pat in moved:
                m = re.search(r"def\s+([A-Za-z_][A-Za-z0-9_]*)\s*\(", pat)
                if m:
                    _tighten_loc_for_func(e, m.group(1))
                    
    return patch, changes

# ìš”ì²­ ëª¨ë¸ ì •ì˜ (Pydantic ëª¨ë¸)
class PlanRequest(BaseModel):
    intent: str = Field(..., description="ê³„íšì˜ ì˜ë„")
    paths: List[str] = Field(..., description="íŒŒì¼ ê²½ë¡œë“¤")
    code_paste: Optional[str] = Field(None, description="ë¶™ì—¬ë„£ì€ ì½”ë“œ")

class FeedbackRequest(BaseModel):
    hint: Optional[str] = Field(None, description="íŒíŠ¸")
    reason: Optional[str] = Field(None, description="ì´ìœ ")

class PatchRequest(BaseModel):
    plan: Union[Dict[str, Any], str] = Field(..., description="í”Œëœ ë°ì´í„°")
    feedback: Optional[FeedbackRequest] = Field(None, description="í”¼ë“œë°±")

class PlanResponse(BaseModel):
    plan_id: str
    plan: Dict[str, Any]
    raw_response: str

class PatchResponse(BaseModel):
    patch_id: str
    patch: Dict[str, Any]
    raw_response: str

class ApplyRequest(BaseModel):
    patch: Dict[str, Any] = Field(..., description="ì ìš©í•  íŒ¨ì¹˜")
    allowed_paths: List[str] = Field(..., description="í—ˆìš©ëœ íŒŒì¼ ê²½ë¡œë“¤")
    dry_run: bool = Field(False, description="ì‹¤ì œ ì ìš© ì—¬ë¶€")

class ApplyResponse(BaseModel):
    applied: List[str]
    skipped: List[Dict[str, Any]] = Field(default_factory=list, description="ê±´ë„ˆë›´ íŒŒì¼ë“¤")
    failed: List[Dict[str, Any]] = Field(default_factory=list, description="ì‹¤íŒ¨í•œ íŒŒì¼ë“¤")
    dry_run: bool
    details: List[Dict[str, Any]] = Field(default_factory=list, description="ìƒì„¸ ì •ë³´")

class TestRequest(BaseModel):
    paths: Optional[List[str]] = Field(None, description="í…ŒìŠ¤íŠ¸í•  ê²½ë¡œë“¤")
    coverage: bool = Field(False, description="ì»¤ë²„ë¦¬ì§€ í¬í•¨ ì—¬ë¶€")

class TestResponse(BaseModel):
    summary: Dict[str, Any]
    output: str

class WorkflowRequest(BaseModel):
    intent: str = Field(..., description="ì‚¬ìš©ì ì˜ë„")
    paths: List[str] = Field(..., description="ë¶„ì„í•  íŒŒì¼ ê²½ë¡œë“¤")
    code_paste: str = Field("", description="ì½”ë“œ ìŠ¤ë‹ˆí«")
    auto_apply: bool = Field(False, description="ìë™ ì ìš© ì—¬ë¶€")
    auto_test: bool = Field(False, description="ìë™ í…ŒìŠ¤íŠ¸ ì—¬ë¶€")

class WorkflowResponse(BaseModel):
    workflow_id: str
    plan: Dict[str, Any]
    patch: Dict[str, Any]
    applied: bool
    test_results: Optional[Dict[str, Any]]

class DebugRequest(BaseModel):
    command: str = Field(..., description="ë””ë²„ê·¸ ëª…ë ¹ì–´")
    script_path: Optional[str] = Field(None, description="ìŠ¤í¬ë¦½íŠ¸ ê²½ë¡œ")
    port: Optional[int] = Field(None, description="ë””ë²„ê·¸ í¬íŠ¸")

class DebugResponse(BaseModel):
    success: bool
    message: str
    debug_port: Optional[int]
    processes: List[Dict[str, Any]]

# startup_event ì œê±° - lifespanì—ì„œ ì²˜ë¦¬

@app.on_event("shutdown")
async def shutdown_event():
    """ì„œë²„ ì¢…ë£Œ ì‹œ ì •ë¦¬"""
    global model, debug_runtime
    
    try:
        if model:
            del model
        if debug_runtime:
            debug_runtime.cleanup_all()
        
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        
        logger.info("âœ… ì„œë²„ ì •ë¦¬ ì™„ë£Œ")
        
    except Exception as e:
        logger.error(f"âŒ ì„œë²„ ì •ë¦¬ ì‹¤íŒ¨: {e}")

@app.get("/health")
async def health_check():
    """ì„œë²„ ìƒíƒœ í™•ì¸"""
    try:
        # ëª¨ë¸ ë¡œë“œ ìƒíƒœ ë° ì–‘ìí™” ëª¨ë“œ
        model_loaded = hasattr(app.state, "model")
        quant = getattr(app.state, "quant", "unknown")
        
        # ê¸°ë³¸ ìƒíƒœ ì •ë³´
        status = {
            "status": "healthy",
            "model_loaded": model_loaded,
            "quantization": quant,
            "adapter_path": getattr(app.state, "adapter_path", None),
            "adapter_version": getattr(app.state, "adapter_version", None),
            "use_4bit": quant == "4bit",
            "timestamp": datetime.now().isoformat()
        }
        
        # CUDA ì •ë³´ ì¶”ê°€ (ê¸°ì¡´ ë¡œì§ ìœ ì§€)
        if torch.cuda.is_available():
            status.update({
                "cuda_available": True,
                "gpu_memory_total": f"{torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f}GB",
                "gpu_memory_allocated": f"{torch.cuda.memory_allocated(0) / 1024**3:.1f}GB",
                "gpu_memory_cached": f"{torch.cuda.memory_reserved(0) / 1024**3:.1f}GB",
                "cuda_version": torch.version.cuda
            })
        else:
            status["cuda_available"] = False
        
        # ëª¨ë¸ ë””ë°”ì´ìŠ¤ ì •ë³´
        if model:
            try:
                device_info = model.get_device_info()
                status.update(device_info)
            except Exception:
                pass
        
        return status
    except Exception as e:
        logger.error(f"í—¬ìŠ¤ ì²´í¬ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=f"ì„œë²„ ìƒíƒœ ë¶ˆì•ˆì •: {str(e)}")

@app.post("/plan", response_model=PlanResponse)
async def create_plan(request: PlanRequest):
    try:
        import time as _time
        t0 = _time.monotonic()
        logger.info(f"í”Œëœ ìš”ì²­ ë°›ìŒ: intent={request.intent}, paths={request.paths}")
        
        # context ë³€ìˆ˜ ì´ˆê¸°í™”
        context = ""
        
        # ì»¨í…ìŠ¤íŠ¸ ë¹Œë“œ
        try:
            context = await run_in_threadpool(build_context, request.paths)
            logger.info(f"ì»¨í…ìŠ¤íŠ¸ ë¹Œë“œ ì„±ê³µ: {len(context)} ë¬¸ì")
        except Exception as e:
            logger.warning(f"ì»¨í…ìŠ¤íŠ¸ ë¹Œë“œ ì‹¤íŒ¨: {e}")
            context = f"ì»¨í…ìŠ¤íŠ¸ ë¹Œë“œ ì‹¤íŒ¨: {str(e)}"
        t1 = _time.monotonic()
        
        # í”Œëœ ìƒì„±
        plan_data = {
            "intent": request.intent,
            "context": context,
            "code_paste": request.code_paste or "",
            "paths": request.paths,
            "timestamp": str(datetime.now())
        }
        
        # ì‹¤ì œ í”Œëœ ìƒì„± (ì—¬ê¸°ì— AI ëª¨ë¸ í˜¸ì¶œ ë¡œì§ êµ¬í˜„)
        plan_result = await generate_plan_with_ai(plan_data)
        t2 = _time.monotonic()
        logger.info("plan timings: build=%.2fs, gen=%.2fs, total=%.2fs", t1-t0, t2-t1, t2-t0)
        
        return PlanResponse(
            plan_id=f"plan_{uuid.uuid4().hex[:8]}",
            plan=plan_result,
            raw_response="Generated with 4-bit quantization"
        )
        
    except Exception as e:
        logger.error(f"í”Œëœ ìƒì„± ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=f"í”Œëœ ìƒì„± ì‹¤íŒ¨: {str(e)}")

@app.post("/patch", response_model=PatchResponse)
async def create_patch(request: PatchRequest):
    try:
        logger.info("íŒ¨ì¹˜ ìš”ì²­ ë°›ìŒ")
        
        # planì´ ë¬¸ìì—´ì¸ ê²½ìš° JSONìœ¼ë¡œ íŒŒì‹±
        if isinstance(request.plan, str):
            try:
                plan_data = json.loads(request.plan)
            except json.JSONDecodeError as e:
                raise HTTPException(status_code=400, detail=f"í”Œëœ JSON íŒŒì‹± ì‹¤íŒ¨: {str(e)}")
        else:
            plan_data = request.plan
        
        # íŒ¨ì¹˜ ìƒì„± ë¡œì§
        patch_result = await generate_patch_with_ai(plan_data, request.feedback)
        
        return {
            "patch": patch_result,
            "status": "success"
        }
        
    except Exception as e:
        logger.error(f"íŒ¨ì¹˜ ìƒì„± ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=f"íŒ¨ì¹˜ ìƒì„± ì‹¤íŒ¨: {str(e)}")

@app.post("/patch_smart")
async def create_smart_patch(request: PatchRequest):
    try:
        logger.info("ìŠ¤ë§ˆíŠ¸ íŒ¨ì¹˜ ìš”ì²­ ë°›ìŒ")
        
        # plan ë°ì´í„° ì²˜ë¦¬
        if isinstance(request.plan, str):
            try:
                plan_data = json.loads(request.plan)
            except json.JSONDecodeError as e:
                logger.error(f"JSON íŒŒì‹± ì‹¤íŒ¨: {e}")
                raise HTTPException(status_code=400, detail=f"JSON íŒŒì‹± ì‹¤íŒ¨: {str(e)}")
        else:
            plan_data = request.plan
        
        # ìŠ¤ë§ˆíŠ¸ íŒ¨ì¹˜ ìƒì„±
        patch_result = await generate_smart_patch(plan_data, request.feedback)
        
        # ì„±ê³µ ë¡œê·¸ ê¸°ë¡
        try:
            os.makedirs("training/success_logs", exist_ok=True)
            rec = {
                "when": datetime.now().isoformat(),
                "role": "patch_success",
                "plan": plan_data,
                "feedback": request.feedback.dict() if request.feedback else {},
                "patch": patch_result
            }
            with open(f"training/success_logs/patch_{datetime.now():%Y%m%d}.jsonl", "a", encoding="utf-8") as f:
                f.write(json.dumps(rec, ensure_ascii=False) + "\n")
            logger.info("âœ… ì„±ê³µ ë¡œê·¸ ê¸°ë¡ ì™„ë£Œ")
        except Exception as e:
            logger.warning(f"âš ï¸ ì„±ê³µ ë¡œê·¸ ê¸°ë¡ ì‹¤íŒ¨: {e}")
        
        # í•­ìƒ bench ìŠ¤í¬ë¦½íŠ¸ í˜¸í™˜ í˜•íƒœë¡œ ë˜í•‘
        return {"patch": patch_result}
        
    except Exception as e:
        logger.error(f"ìŠ¤ë§ˆíŠ¸ íŒ¨ì¹˜ ìƒì„± ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=f"ìŠ¤ë§ˆíŠ¸ íŒ¨ì¹˜ ìƒì„± ì‹¤íŒ¨: {str(e)}")

@app.post("/apply", response_model=ApplyResponse)
async def apply_patch(request: ApplyRequest):
    """íŒ¨ì¹˜ ì ìš©"""
    try:
        # íŒ¨ì¹˜ ì ìš©
        result = apply_patch_json(
            patch=request.patch,
            allowed_paths=request.allowed_paths,
            dry_run=request.dry_run
        )
        
        return ApplyResponse(
            applied=result.get("applied", []),
            skipped=result.get("skipped", []),
            failed=result.get("failed", []),
            dry_run=request.dry_run,
            details=result.get("details", [])
        )
        
    except Exception as e:
        logger.error(f"íŒ¨ì¹˜ ì ìš© ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=f"íŒ¨ì¹˜ ì ìš© ì‹¤íŒ¨: {str(e)}")

@app.post("/test", response_model=TestResponse)
async def run_tests(request: TestRequest):
    """í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
    try:
        # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
        if request.coverage:
            test_path = (request.paths or ["."])[0] if request.paths else "."
            output = run_pytest(test_path, coverage=True)
        else:
            test_path = (request.paths or ["."])[0] if request.paths else "."
            output = run_pytest(test_path)
        
        # í…ŒìŠ¤íŠ¸ ìš”ì•½
        summary = get_test_summary(output)
        
        return TestResponse(summary=summary, output=output.output)
        
    except Exception as e:
        logger.error(f"í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=f"í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì‹¤íŒ¨: {str(e)}")

@app.post("/workflow", response_model=WorkflowResponse)
async def run_workflow(request: WorkflowRequest):
    """ì „ì²´ ì›Œí¬í”Œë¡œìš° ì‹¤í–‰"""
    if not model:
        raise HTTPException(status_code=503, detail="ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•ŠìŒ")
    
    try:
        workflow_id = str(uuid.uuid4())
        logger.info(f"ì›Œí¬í”Œë¡œìš° ì‹œì‘: {workflow_id}")
        
        # 1. ì»¨í…ìŠ¤íŠ¸ ë¹Œë“œ
        context = await run_in_threadpool(build_context, request.paths)
        
        # 2. ê³„íš ìƒì„± (íƒ€ì„ì•„ì›ƒ ì ìš©)
        with anyio.move_on_after(30) as cs:
            plan_result = await run_in_threadpool(
                model.plan,
                context,
                request.intent,
                request.code_paste
            )
        
        if cs.cancel_called:
            raise TimeoutError("PLAN generation timeout")
        
        # 3. íŒ¨ì¹˜ ìƒì„± (íƒ€ì„ì•„ì›ƒ ì ìš©)
        with anyio.move_on_after(90) as cs:   # 90ì´ˆë¡œ ì¦ê°€
            patch_result = await run_in_threadpool(model.patch, plan_result)
        
        if cs.cancel_called:
            raise TimeoutError("PATCH generation timeout")
        
        # 4. íŒ¨ì¹˜ ì ìš© (ì„ íƒì‚¬í•­)
        applied = False
        if request.auto_apply:
            apply_result = apply_patch_json(
                patch=patch_result,
                allowed_paths=request.paths,
                dry_run=False
            )
            applied = len(apply_result.get("applied", [])) > 0
        
        # 5. í…ŒìŠ¤íŠ¸ ì‹¤í–‰ (ì„ íƒì‚¬í•­)
        test_results = None
        if request.auto_test:
            try:
                test_path = request.paths[0] if request.paths else "."
                output = run_pytest(test_path)
                test_results = get_test_summary(output)
            except Exception as e:
                logger.warning(f"í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
        
        return WorkflowResponse(
            workflow_id=workflow_id,
            plan=plan_result,
            patch=patch_result,
            applied=applied,
            test_results=test_results
        )
        
    except TimeoutError as te:
        logger.error(f"ì›Œí¬í”Œë¡œìš° íƒ€ì„ì•„ì›ƒ: {te}")
        raise HTTPException(status_code=504, detail=str(te))
    except Exception as e:
        logger.error(f"ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=f"ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ ì‹¤íŒ¨: {str(e)}")

@app.post("/debug", response_model=DebugResponse)
async def debug_control(request: DebugRequest):
    """ë””ë²„ê·¸ ëŸ°íƒ€ì„ ì œì–´"""
    if not debug_runtime:
        raise HTTPException(status_code=503, detail="ë””ë²„ê·¸ ëŸ°íƒ€ì„ì´ ì´ˆê¸°í™”ë˜ì§€ ì•ŠìŒ")
    
    try:
        if request.command == "start":
            if not request.script_path:
                raise HTTPException(status_code=400, detail="ìŠ¤í¬ë¦½íŠ¸ ê²½ë¡œê°€ í•„ìš”í•©ë‹ˆë‹¤")
            
            debug_port = debug_runtime.run_with_debugpy(
                script_path=request.script_path,
                port=request.port
            )
            
            return DebugResponse(
                success=True,
                message=f"ë””ë²„ê·¸ í”„ë¡œì„¸ìŠ¤ ì‹œì‘ë¨ (í¬íŠ¸: {debug_port})",
                debug_port=debug_port,
                processes=debug_runtime.list_processes()
            )
            
        elif request.command == "stop":
            if not request.port:
                raise HTTPException(status_code=400, detail="í¬íŠ¸ ë²ˆí˜¸ê°€ í•„ìš”í•©ë‹ˆë‹¤")
            
            debug_runtime.stop_process(request.port)
            
            return DebugResponse(
                success=True,
                message=f"í¬íŠ¸ {request.port}ì˜ í”„ë¡œì„¸ìŠ¤ ì¤‘ì§€ë¨",
                debug_port=None,
                processes=debug_runtime.list_processes()
            )
            
        elif request.command == "list":
            return DebugResponse(
                success=True,
                message="ë””ë²„ê·¸ í”„ë¡œì„¸ìŠ¤ ëª©ë¡",
                debug_port=None,
                processes=debug_runtime.list_processes()
            )
            
        else:
            raise HTTPException(status_code=400, detail=f"ì•Œ ìˆ˜ ì—†ëŠ” ëª…ë ¹ì–´: {request.command}")
            
    except Exception as e:
        logger.error(f"ë””ë²„ê·¸ ì œì–´ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=f"ë””ë²„ê·¸ ì œì–´ ì‹¤íŒ¨: {str(e)}")

@app.get("/")
async def root():
    """ë£¨íŠ¸ ì—”ë“œí¬ì¸íŠ¸"""
    return {
        "message": "Qwen3-8B Local Coding AI ì„œë²„",
        "version": "1.0.0",
        "endpoints": [
            "/health - ì„œë²„ ìƒíƒœ í™•ì¸",
            "/plan - ì½”ë“œ ìˆ˜ì • ê³„íš ìƒì„±",
            "/patch - ì½”ë“œ íŒ¨ì¹˜ ìƒì„±",
            "/apply - íŒ¨ì¹˜ ì ìš©",
            "/test - í…ŒìŠ¤íŠ¸ ì‹¤í–‰",
            "/workflow - ì „ì²´ ì›Œí¬í”Œë¡œìš° ì‹¤í–‰",
            "/debug - ë””ë²„ê·¸ ëŸ°íƒ€ì„ ì œì–´"
        ]
    }

# AI ìƒì„± í•¨ìˆ˜ë“¤
import os, json, logging
# PATCH_SMART ê¸°ë³¸ê°’ ë° ìŠ¤í† í•‘
from transformers import StoppingCriteria, StoppingCriteriaList
import time as _patch_time

class PatchWallClockBudget(StoppingCriteria):
    def __init__(self, s: int = 25):
        self.t0 = _patch_time.monotonic(); self.S = s
    def __call__(self, input_ids, scores=None, **kw):
        return (_patch_time.monotonic() - self.t0) >= self.S

GEN_PATCH = dict(
    max_new_tokens=192,
    do_sample=False,
    top_p=1.0,
    repetition_penalty=1.03,
    use_cache=True,
)

log = logging.getLogger(__name__)
USE_DUMMY = os.getenv("USE_DUMMY_AI", "0") == "1"

async def generate_plan_with_ai(plan_data: dict) -> dict:
    """AIë¥¼ ì‚¬ìš©í•˜ì—¬ í”Œëœ ìƒì„±"""
    if USE_DUMMY:
        # ì„ì‹œ ë”ë¯¸ê°€ í•„ìš”í•˜ë©´ ë‚¨ê²¨ë‘ë˜ ê¸°ë³¸ê°’ì€ ë¹„í™œì„±í™”
        return {"files":[{"path": plan_data["paths"][0], "reason": plan_data["intent"], "strategy":"anchor"}], "notes":""}

    # ì•± ìƒíƒœì˜ ë‹¨ì¼ ëª¨ë¸/í† í¬ë‚˜ì´ì € ì‚¬ìš©
    m = app.state.model
    tok = app.state.tok
    # ì…ë ¥ ì»·(1200ì) + ì»¨í…ìŠ¤íŠ¸ ì œê±° â†’ í”„ë¦¬í•„ ì‹œê°„ ë‹¨ì¶•
    code = (plan_data.get("code_paste") or "")[:1200]
    sys = "You must output STRICT JSON plan with keys 'files' and 'notes'."
    prompt = f"{sys}\n\n[CODE]\n{code}\n"
    inputs = tok(prompt, return_tensors="pt", add_special_tokens=False, truncation=True, max_length=1024).to(m.device)
    from transformers import StoppingCriteriaList
    class _Wall:
        def __init__(self, s=12):
            import time
            self.t0=time.monotonic(); self.S=s; self._time=time
        def __call__(self, input_ids, scores=None, **kw):
            return (self._time.monotonic()-self.t0) >= self.S
    gen_kw = dict(
        max_new_tokens=16,
        do_sample=False,
        use_cache=True,
        repetition_penalty=1.10,
        max_time=12,
        stopping_criteria=StoppingCriteriaList([_Wall(12)]),
        eos_token_id=tok.eos_token_id,
        pad_token_id=tok.eos_token_id,
    )
    out = m.generate(**inputs, **gen_kw)
    text = tok.decode(out[0], skip_special_tokens=True)
    # ê°„ëµ íŒŒì„œ: ìµœì†Œ ìŠ¤í‚¤ë§ˆ ë³´ì •
    try:
        import json as _json
        obj = _json.loads(text)
        if not isinstance(obj, dict):
            raise ValueError
        if "files" not in obj:
            obj["files"] = [{"path": plan_data["paths"][0] if plan_data.get("paths") else "", "reason": plan_data["intent"], "strategy":"regex", "tests": []}]
        if "notes" not in obj:
            obj["notes"] = ""
        return obj
    except Exception:
        return {"files":[{"path": plan_data["paths"][0] if plan_data.get("paths") else "", "reason": plan_data["intent"], "strategy":"regex", "tests": []}], "notes":""}
    if isinstance(plan, str):
        plan = json.loads(plan)
    return plan

async def generate_patch_with_ai(plan_data: dict, feedback: Optional[FeedbackRequest]) -> dict:
    """AIë¥¼ ì‚¬ìš©í•˜ì—¬ íŒ¨ì¹˜ ìƒì„±"""
    if USE_DUMMY:
        return {"version":"1", "edits":[]}

    # ë‹¨ì¼ ëª¨ë¸/í† í¬ë‚˜ì´ì € ì‚¬ìš©
    m = app.state.model
    tok = app.state.tok
    system = (
        "Output ONLY a JSON array named 'edits'. Each item: {path:str,loc:{type, ...},action,code,once,pre:{must_contain,must_not_contain,regex?}}. "
        "Paths must be relative and use forward slashes. Return ONLY the array items."
    )
    user = json.dumps(plan_data, ensure_ascii=False)
    prefix = (
        system + "\n" + user + "\n" +
        "<<<PATCH_JSON>>>{\"version\":\"1\",\"edits\":["
    )
    inputs = tok(prefix, return_tensors="pt", add_special_tokens=False, truncation=True, max_length=1024).to(m.device)
    # ìŠ¤í† í•‘ ê²°í•©: JSON ë‹«í˜ + ì‹œê°„ ì˜ˆì‚°
    class _EditsClosed(StoppingCriteria):
        def __init__(self): self.buf = []
        def __call__(self, input_ids, scores=None, **kw):
            text = tok.decode(input_ids[0][-64:], skip_special_tokens=True)
            self.buf.append(text)
            s = "".join(self.buf)
            i = s.find("["); depth=0; ins=False; esc=False
            if i>=0:
                for ch in s[i:]:
                    if ins:
                        if esc: esc=False
                        elif ch == "\\": esc=True
                        elif ch == '"': ins=False
                        continue
                    if ch == '"': ins=True
                    elif ch == '[': depth += 1
                    elif ch == ']':
                        depth -= 1
                        if depth == 0:
                            return True
            return False
    stops = StoppingCriteriaList([_EditsClosed(), PatchWallClockBudget(25)])
    gen_kw = dict(GEN_PATCH)
    gen_kw.update(dict(eos_token_id=tok.eos_token_id, pad_token_id=tok.eos_token_id, max_time=25, stopping_criteria=stops))
    out = m.generate(**inputs, **gen_kw)
    text = tok.decode(out[0], skip_special_tokens=True)
    # ì„¼í‹°ë„¬ ì»·
    body = text.split("<<<PATCH_JSON>>>",1)[-1]
    arr = None
    try:
        # ê°€ì¥ ë°”ê¹¥ edits ë°°ì—´ë§Œ ë³µêµ¬
        j = body.find("["); depth=0; start=None
        for k,ch in enumerate(body[j:], start=j):
            if ch == '[': depth+=1; start = start or k
            elif ch == ']': depth-=1; 
            if depth==0 and start is not None:
                frag = body[start:k+1]; import json as _j; arr = _j.loads(frag); break
    except Exception:
        arr = []
    # ìµœì†Œ ë³´ì •: editsê°€ ë¹„ë©´ ì•ˆì „í•œ ë‹¨ì¼ edit ìƒì„±
    if not arr:
        target_path = (plan_data.get("paths") or ["examples/sample_py/app.py"])[0]
        intent = str(plan_data.get("intent") or "add")
        func_name = intent.split("(")[0].strip()
        if not func_name:
            func_name = "add"
        minimal_edit = {
            "path": target_path.replace("\\", "/"),
            "loc": {"type": "regex", "pattern": rf"^def {func_name}\("},
            "action": "insert_before",
            "code": "# AUTO-GUARD: inserted by AI bench fallback\n"
        }
        arr = [minimal_edit]
    return {"version":"1","edits": arr}

async def generate_smart_patch(plan_data: dict, feedback: Optional[FeedbackRequest]) -> dict:
    """ìŠ¤ë§ˆíŠ¸ íŒ¨ì¹˜ ìƒì„±"""
    if USE_DUMMY:
        return {"version":"1", "edits":[]}
    # ìŠ¤íŠ¸ë¦¬ë° ê¸°ë°˜ PATCH ìƒì„± ë¡œì§ì„ ì¬ì‚¬ìš©
    return await generate_patch_with_ai(plan_data, feedback)

# ì „ì—­ ì˜ˆì™¸ í•¸ë“¤ëŸ¬
@app.exception_handler(Exception)
async def global_exception_handler(request: Request, exc: Exception):
    logger.error(f"ì „ì—­ ì˜ˆì™¸ ë°œìƒ: {exc}", exc_info=True)
    return JSONResponse(
        status_code=500,
        content={"detail": f"ì„œë²„ ë‚´ë¶€ ì˜¤ë¥˜: {str(exc)}"}
    )

# Ollama-like API ë¼ìš°í„° ì¶”ê°€
from server.ollama_api import router as ollama_router
app.include_router(ollama_router)

# ë˜ëŠ” ê¸°ì¡´ ë°©ì‹ì„ ì‚¬ìš©í•œë‹¤ë©´
@app.on_event("shutdown")
async def shutdown_event():
    try:
        if hasattr(debug_runtime, 'cleanup_all'):
            debug_runtime.cleanup_all()
        logger.info("âœ… ì„œë²„ ì •ë¦¬ ì™„ë£Œ")
    except Exception as e:
        logger.error(f"âŒ ì„œë²„ ì •ë¦¬ ì‹¤íŒ¨: {e}")

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="127.0.0.1", port=8765)

