#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Qwen3-8B Local Coding AI CUDA í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸
GPU ê°€ì† ì„±ëŠ¥ì„ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤
"""

import requests
import json
import time
import torch
from typing import Dict, Any

def print_cuda_info():
    """CUDA í™˜ê²½ ì •ë³´ ì¶œë ¥"""
    print("ğŸš€ CUDA í™˜ê²½ ì •ë³´")
    print("=" * 50)
    
    if torch.cuda.is_available():
        print(f"âœ… CUDA ì‚¬ìš© ê°€ëŠ¥")
        print(f"   GPU: {torch.cuda.get_device_name()}")
        print(f"   GPU ë©”ëª¨ë¦¬: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f}GB")
        print(f"   CUDA ë²„ì „: {torch.version.cuda}")
        print(f"   PyTorch ë²„ì „: {torch.__version__}")
        
        # GPU ë©”ëª¨ë¦¬ ìƒíƒœ
        print(f"   í˜„ì¬ í• ë‹¹ëœ ë©”ëª¨ë¦¬: {torch.cuda.memory_allocated(0) / 1024**3:.1f}GB")
        print(f"   í˜„ì¬ ìºì‹œëœ ë©”ëª¨ë¦¬: {torch.cuda.memory_reserved(0) / 1024**3:.1f}GB")
        
    else:
        print("âŒ CUDA ì‚¬ìš© ë¶ˆê°€")
        print("   CPU ëª¨ë“œë¡œ ì‹¤í–‰ë©ë‹ˆë‹¤")
    
    print()

def test_server_health(base_url: str) -> bool:
    """ì„œë²„ ìƒíƒœ í™•ì¸"""
    print("1ï¸âƒ£ ì„œë²„ ìƒíƒœ í™•ì¸...")
    try:
        response = requests.get(f"{base_url}/health", timeout=10)
        if response.status_code == 200:
            data = response.json()
            print(f"âœ… ì„œë²„ ìƒíƒœ: {data.get('status', 'unknown')}")
            print(f"   ëª¨ë¸ ë¡œë“œ: {data.get('model_loaded', 'unknown')}")
            
            # CUDA ì •ë³´ í‘œì‹œ
            if data.get('cuda_available'):
                print(f"   GPU: {data.get('gpu_name', 'unknown')}")
                print(f"   GPU ë©”ëª¨ë¦¬: {data.get('gpu_memory_total', 'unknown')}")
                print(f"   í• ë‹¹ëœ ë©”ëª¨ë¦¬: {data.get('gpu_memory_allocated', 'unknown')}")
                print(f"   ìºì‹œëœ ë©”ëª¨ë¦¬: {data.get('gpu_memory_cached', 'unknown')}")
                print(f"   CUDA ë²„ì „: {data.get('cuda_version', 'unknown')}")
            else:
                print("   CUDA ì‚¬ìš© ë¶ˆê°€")
            
            return True
        else:
            print(f"âŒ ì„œë²„ ì‘ë‹µ ì˜¤ë¥˜: {response.status_code}")
            return False
    except Exception as e:
        print(f"âŒ ì„œë²„ ì—°ê²° ì‹¤íŒ¨: {e}")
        return False

def test_plan_generation(base_url: str) -> Dict[str, Any]:
    """PLAN ë‹¨ê³„ í…ŒìŠ¤íŠ¸ (ì„±ëŠ¥ ì¸¡ì •)"""
    print("\n2ï¸âƒ£ PLAN ë‹¨ê³„ í…ŒìŠ¤íŠ¸ (ì„±ëŠ¥ ì¸¡ì •)...")
    
    plan_data = {
        "intent": "Add input validation to add() function",
        "paths": ["examples/sample_py"],
        "code_paste": "def add(a, b): return a + b"
    }
    
    try:
        start_time = time.time()
        
        response = requests.post(
            f"{base_url}/plan",
            json=plan_data,
            headers={"Content-Type": "application/json"},
            timeout=120  # 2ë¶„ íƒ€ì„ì•„ì›ƒ
        )
        
        end_time = time.time()
        duration = end_time - start_time
        
        if response.status_code == 200:
            plan_result = response.json()
            print(f"âœ… PLAN ìƒì„± ì„±ê³µ!")
            print(f"   ì‘ë‹µ ì‹œê°„: {duration:.2f}ì´ˆ")
            print(f"   ê³„íš ID: {plan_result.get('plan_id', 'N/A')}")
            
            # GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í™•ì¸
            try:
                health_response = requests.get(f"{base_url}/health")
                if health_response.status_code == 200:
                    health_data = health_response.json()
                    if health_data.get('cuda_available'):
                        print(f"   GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {health_data.get('gpu_memory_allocated', 'unknown')}")
            except:
                pass
            
            return plan_result
        else:
            print(f"âŒ PLAN ìƒì„± ì‹¤íŒ¨: {response.status_code}")
            print(f"   ì˜¤ë¥˜: {response.text}")
            return {}
            
    except Exception as e:
        print(f"âŒ PLAN í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        return {}

def test_patch_generation(base_url: str, plan: Dict[str, Any]) -> Dict[str, Any]:
    """PATCH ë‹¨ê³„ í…ŒìŠ¤íŠ¸ (ì„±ëŠ¥ ì¸¡ì •)"""
    if not plan:
        print("âŒ PLANì´ ì—†ì–´ì„œ PATCH í…ŒìŠ¤íŠ¸ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤")
        return {}
    
    print("\n3ï¸âƒ£ PATCH ë‹¨ê³„ í…ŒìŠ¤íŠ¸ (ì„±ëŠ¥ ì¸¡ì •)...")
    
    patch_data = {"plan": plan.get('plan', {})}
    
    try:
        start_time = time.time()
        
        response = requests.post(
            f"{base_url}/patch",
            json=patch_data,
            headers={"Content-Type": "application/json"},
            timeout=180  # 3ë¶„ íƒ€ì„ì•„ì›ƒ
        )
        
        end_time = time.time()
        duration = end_time - start_time
        
        if response.status_code == 200:
            patch_result = response.json()
            print(f"âœ… PATCH ìƒì„± ì„±ê³µ!")
            print(f"   ì‘ë‹µ ì‹œê°„: {duration:.2f}ì´ˆ")
            print(f"   íŒ¨ì¹˜ ID: {patch_result.get('patch_id', 'N/A')}")
            
            # GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í™•ì¸
            try:
                health_response = requests.get(f"{base_url}/health")
                if health_response.status_code == 200:
                    health_data = health_response.json()
                    if health_data.get('cuda_available'):
                        print(f"   GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {health_data.get('gpu_memory_allocated', 'unknown')}")
            except:
                pass
            
            return patch_result
        else:
            print(f"âŒ PATCH ìƒì„± ì‹¤íŒ¨: {response.status_code}")
            print(f"   ì˜¤ë¥˜: {response.text}")
            return {}
            
    except Exception as e:
        print(f"âŒ PATCH í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        return {}

def test_workflow(base_url: str) -> Dict[str, Any]:
    """ì „ì²´ ì›Œí¬í”Œë¡œìš° í…ŒìŠ¤íŠ¸ (ì„±ëŠ¥ ì¸¡ì •)"""
    print("\n4ï¸âƒ£ ì „ì²´ ì›Œí¬í”Œë¡œìš° í…ŒìŠ¤íŠ¸ (ì„±ëŠ¥ ì¸¡ì •)...")
    
    workflow_data = {
        "intent": "Fix the bug in calculate_average function",
        "paths": ["examples/sample_py"],
        "code_paste": "def calculate_average(numbers): return sum(numbers) / len(numbers)",
        "auto_apply": False,  # ì•ˆì „ì„ ìœ„í•´ ìë™ ì ìš© ë¹„í™œì„±í™”
        "auto_test": False
    }
    
    try:
        start_time = time.time()
        
        response = requests.post(
            f"{base_url}/workflow",
            json=workflow_data,
            headers={"Content-Type": "application/json"},
            timeout=300  # 5ë¶„ íƒ€ì„ì•„ì›ƒ
        )
        
        end_time = time.time()
        duration = end_time - start_time
        
        if response.status_code == 200:
            workflow_result = response.json()
            print(f"âœ… ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ ì„±ê³µ!")
            print(f"   ì´ ì‹¤í–‰ ì‹œê°„: {duration:.2f}ì´ˆ")
            print(f"   ì›Œí¬í”Œë¡œìš° ID: {workflow_result.get('workflow_id', 'N/A')}")
            
            return workflow_result
        else:
            print(f"âŒ ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ ì‹¤íŒ¨: {response.status_code}")
            print(f"   ì˜¤ë¥˜: {response.text}")
            return {}
            
    except Exception as e:
        print(f"âŒ ì›Œí¬í”Œë¡œìš° í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        return {}

def benchmark_gpu_performance():
    """GPU ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬"""
    print("\n5ï¸âƒ£ GPU ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬...")
    
    if not torch.cuda.is_available():
        print("âŒ CUDAë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        return
    
    try:
        # ê°„ë‹¨í•œ í…ì„œ ì—°ì‚°ìœ¼ë¡œ GPU ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
        device = torch.device("cuda")
        
        # ë©”ëª¨ë¦¬ í• ë‹¹ í…ŒìŠ¤íŠ¸
        start_time = time.time()
        x = torch.randn(1000, 1000, device=device)
        y = torch.randn(1000, 1000, device=device)
        allocation_time = time.time() - start_time
        
        # í–‰ë ¬ ê³±ì…ˆ í…ŒìŠ¤íŠ¸
        start_time = time.time()
        z = torch.mm(x, y)
        torch.cuda.synchronize()  # GPU ì—°ì‚° ì™„ë£Œ ëŒ€ê¸°
        computation_time = time.time() - start_time
        
        # ë©”ëª¨ë¦¬ ì •ë¦¬
        del x, y, z
        torch.cuda.empty_cache()
        
        print(f"âœ… GPU ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
        print(f"   ë©”ëª¨ë¦¬ í• ë‹¹ ì‹œê°„: {allocation_time:.4f}ì´ˆ")
        print(f"   1000x1000 í–‰ë ¬ ê³±ì…ˆ: {computation_time:.4f}ì´ˆ")
        
    except Exception as e:
        print(f"âŒ GPU ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")

def main():
    """ë©”ì¸ í…ŒìŠ¤íŠ¸ í•¨ìˆ˜"""
    base_url = "http://127.0.0.1:8765"
    
    print("ğŸš€ Qwen3-8B Local Coding AI CUDA í…ŒìŠ¤íŠ¸ ì‹œì‘")
    print("=" * 60)
    
    # 1. CUDA í™˜ê²½ ì •ë³´
    print_cuda_info()
    
    # 2. GPU ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬
    benchmark_gpu_performance()
    
    # 3. ì„œë²„ ìƒíƒœ í™•ì¸
    if not test_server_health(base_url):
        print("âŒ ì„œë²„ê°€ ì‹¤í–‰ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë¨¼ì € ì„œë²„ë¥¼ ì‹œì‘í•˜ì„¸ìš”.")
        return
    
    # 4. PLAN ë‹¨ê³„ í…ŒìŠ¤íŠ¸
    plan_result = test_plan_generation(base_url)
    
    # 5. PATCH ë‹¨ê³„ í…ŒìŠ¤íŠ¸
    patch_result = test_patch_generation(base_url, plan_result)
    
    # 6. ì „ì²´ ì›Œí¬í”Œë¡œìš° í…ŒìŠ¤íŠ¸
    workflow_result = test_workflow(base_url)
    
    # 7. ìµœì¢… ê²°ê³¼ ìš”ì•½
    print("\nğŸ‰ CUDA í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
    print("=" * 60)
    
    if torch.cuda.is_available():
        print("âœ… CUDA ê°€ì†ì´ í™œì„±í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤")
        print(f"   GPU: {torch.cuda.get_device_name()}")
        print(f"   ì‚¬ìš© ê°€ëŠ¥í•œ ë©”ëª¨ë¦¬: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f}GB")
    else:
        print("âš ï¸ CUDAë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤ (CPU ëª¨ë“œ)")
    
    print("\nğŸ“Š í…ŒìŠ¤íŠ¸ ê²°ê³¼:")
    if plan_result:
        print("   âœ… PLAN ë‹¨ê³„: ì„±ê³µ")
    else:
        print("   âŒ PLAN ë‹¨ê³„: ì‹¤íŒ¨")
    
    if patch_result:
        print("   âœ… PATCH ë‹¨ê³„: ì„±ê³µ")
    else:
        print("   âŒ PATCH ë‹¨ê³„: ì‹¤íŒ¨")
    
    if workflow_result:
        print("   âœ… ì›Œí¬í”Œë¡œìš°: ì„±ê³µ")
    else:
        print("   âŒ ì›Œí¬í”Œë¡œìš°: ì‹¤íŒ¨")

if __name__ == "__main__":
    main()
